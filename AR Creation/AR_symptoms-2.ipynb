{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "d6P4uZga2OjM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 261
        },
        "outputId": "8b96011e-f7b7-474b-88d5-c53697c84784"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Networkx not available. Network visualizations will be skipped.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "usage: colab_kernel_launcher.py [-h] [--sample_fraction SAMPLE_FRACTION]\n",
            "                                [--min_support MIN_SUPPORT]\n",
            "                                [--min_confidence MIN_CONFIDENCE]\n",
            "                                [--skip_visualizations] [--skip_report]\n",
            "                                [--read_only]\n",
            "colab_kernel_launcher.py: error: unrecognized arguments: -f /root/.local/share/jupyter/runtime/kernel-866c2b57-1ad3-4b3b-8354-79c1ab321679.json\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "2",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py:3561: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ]
        }
      ],
      "source": [
        "# Main function remains largely the same, but with updated imports and function calls\n",
        "def main(sample_fraction=0.1, min_support=0.0006, min_confidence=0.5, visualize=True, generate_report=True):\n",
        "    \"\"\"\n",
        "    Main function to run the full pipeline.\n",
        "\n",
        "    Args:\n",
        "        sample_fraction (float): Fraction of data to sample\n",
        "        min_support (float): Minimum support threshold for FP-Growth\n",
        "        min_confidence (float): Minimum confidence threshold for rules\n",
        "        visualize (bool): Whether to create visualizations\n",
        "        generate_report (bool): Whether to generate HTML report\n",
        "\n",
        "    Returns:\n",
        "        tuple: (frequent_itemsets, rules, procedure_rules)\n",
        "    \"\"\"\n",
        "    # 1. Load data\n",
        "    patients, admissions, diagnoses, d_icd_diagnoses, d_icd_procedures, procedures, symptoms = load_data(sample_fraction)\n",
        "\n",
        "    # 2. Preprocess data\n",
        "    transactions_base, diagnoses_with_desc = preprocess_data(patients, admissions, diagnoses, d_icd_diagnoses, symptoms)\n",
        "\n",
        "    # 3. Check if the transaction_matrix.csv already exists\n",
        "    if os.path.exists('output/transaction_matrix.csv'):\n",
        "        print(\"Loading existing transaction matrix...\")\n",
        "        transactions_matrix = pd.read_csv('output/transaction_matrix.csv', index_col=0)\n",
        "\n",
        "        # Create human-readable versions if they don't exist yet\n",
        "        if not os.path.exists('output/detailed_transaction_matrix.csv'):\n",
        "            print(\"Creating human-readable transaction matrices...\")\n",
        "            create_readable_transaction_matrix(transactions_matrix)\n",
        "            create_detailed_transaction_matrix(transactions_matrix, transactions_base, diagnoses_with_desc, procedures)\n",
        "    else:\n",
        "        # 4. Engineer features\n",
        "        transactions_matrix = engineer_features(transactions_base, procedures, d_icd_procedures, diagnoses_with_desc, symptoms)\n",
        "\n",
        "    # 5. Mine association rules\n",
        "    frequent_itemsets, rules, procedure_rules, diagnosis_to_procedure_rules = mine_association_rules(transactions_matrix, min_support, min_confidence)\n",
        "\n",
        "    # 6. Create visualizations if requested and available\n",
        "    if visualize and visualization_available:\n",
        "        print(\"\\nCreating visualizations...\")\n",
        "\n",
        "        # Visualize feature distribution in transaction matrix\n",
        "        visualize_feature_distribution(transactions_matrix, save_path='output/feature_distribution.png')\n",
        "\n",
        "        # Visualize rule metrics\n",
        "        if not rules.empty:\n",
        "            visualize_rule_metrics(rules, save_path='output/rule_metrics.png')\n",
        "            visualize_rules_summary(rules, save_path='output/rules_summary.png')\n",
        "\n",
        "        # Visualize procedure rules network\n",
        "        if not procedure_rules.empty:\n",
        "            visualize_rules_network(procedure_rules, max_rules=50, min_lift=1.0,\n",
        "                                   save_path='output/procedure_rules_network.png')\n",
        "\n",
        "    # 7. Generate HTML report if requested\n",
        "    if generate_report and visualization_available:\n",
        "        print(\"\\nGenerating HTML report...\")\n",
        "        report_path = create_html_report(transactions_matrix, rules, procedure_rules)\n",
        "        if report_path:\n",
        "            print(f\"HTML report generated at: {report_path}\")\n",
        "\n",
        "    return frequent_itemsets, rules, procedure_rules\n",
        "\n",
        "# Update main imports at the top\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from mlxtend.frequent_patterns import fpgrowth, association_rules  # Changed from apriori to fpgrowth\n",
        "import matplotlib.pyplot as plt\n",
        "import ast\n",
        "import os\n",
        "import pickle\n",
        "# Import visualization functions\n",
        "try:\n",
        "    import networkx as nx\n",
        "    from visualization import (\n",
        "        visualize_rules_network,\n",
        "        visualize_rule_metrics,\n",
        "        visualize_feature_distribution,\n",
        "        visualize_rules_summary,\n",
        "        create_html_report\n",
        "    )\n",
        "    visualization_available = True\n",
        "except ImportError:\n",
        "    print(\"Networkx not available. Network visualizations will be skipped.\")\n",
        "    visualization_available = False\n",
        "\n",
        "    # Define empty visualization functions to avoid errors\n",
        "    def visualize_rules_network(*args, **kwargs):\n",
        "        print(\"Networkx not available. Skipping network visualization.\")\n",
        "\n",
        "    def visualize_rule_metrics(*args, **kwargs):\n",
        "        print(\"Visualization functions not available. Skipping rule metrics visualization.\")\n",
        "\n",
        "    def visualize_feature_distribution(*args, **kwargs):\n",
        "        print(\"Visualization functions not available. Skipping feature distribution visualization.\")\n",
        "\n",
        "    def visualize_rules_summary(*args, **kwargs):\n",
        "        print(\"Visualization functions not available. Skipping rules summary visualization.\")\n",
        "\n",
        "    def create_html_report(*args, **kwargs):\n",
        "        print(\"Visualization functions not available. Skipping HTML report generation.\")\n",
        "        return None\n",
        "def sample_data(data, fraction=0.1, seed=42):\n",
        "    if data is None or data.empty:\n",
        "        return data\n",
        "\n",
        "    # For small datasets, use at least 1000 rows or the original size, whichever is smaller\n",
        "    min_rows = min(10000, len(data))\n",
        "\n",
        "    # Calculate how many rows to sample (at least min_rows)\n",
        "    sample_size = max(min_rows, int(len(data) * fraction))\n",
        "\n",
        "    print(\"Sample size of the data:\", sample_size)\n",
        "\n",
        "    # Sample the data with a fixed random seed for reproducibility\n",
        "    return data.sample(n=sample_size, random_state=seed)\n",
        "\n",
        "def load_data(sample_fraction=0.1):\n",
        "    patients_path = 'patients.csv'\n",
        "    admissions_path = 'admissions.csv'\n",
        "    diagnoses_path = 'diagnoses_icd.csv'\n",
        "    d_icd_diagnoses_path = 'd_icd_diagnoses.csv'\n",
        "    d_icd_procedures_path = 'd_icd_procedures.csv'\n",
        "    procedures_path = 'procedures_icd.csv'\n",
        "    symptoms_path = 'Notes.csv'\n",
        "\n",
        "    print(f\"Loading data with sampling fraction: {sample_fraction}\")\n",
        "\n",
        "    patients = pd.read_csv(patients_path)\n",
        "    admissions = pd.read_csv(admissions_path)\n",
        "    diagnoses = pd.read_csv(diagnoses_path)\n",
        "    d_icd_diagnoses = pd.read_csv(d_icd_diagnoses_path)\n",
        "    d_icd_procedures = pd.read_csv(d_icd_procedures_path)\n",
        "    procedures = pd.read_csv(procedures_path)\n",
        "    symptoms = pd.read_csv(symptoms_path)\n",
        "\n",
        "    print(\"Original dataset shapes:\")\n",
        "    print(f\"Patients dataset shape: {patients.shape}\")\n",
        "    print(f\"Admissions dataset shape: {admissions.shape}\")\n",
        "    print(f\"Diagnoses dataset shape: {diagnoses.shape}\")\n",
        "    print(f\"d_icd_diagnoses dataset shape: {d_icd_diagnoses.shape}\")\n",
        "    print(f\"d_icd_procedures dataset shape: {d_icd_procedures.shape}\")\n",
        "    print(f\"Procedures dataset shape: {procedures.shape}\")\n",
        "    print(f\"Procedures dataset shape: {symptoms.shape}\")\n",
        "\n",
        "    # Sample the main data tables that contain patient-level information\n",
        "    # We don't sample the reference tables (d_icd_*)\n",
        "    if sample_fraction < 1.0:\n",
        "        # First, sample patients\n",
        "        admissions_sampled = sample_data(admissions, fraction=sample_fraction)\n",
        "\n",
        "        # Then filter other tables to only include the sampled patients\n",
        "        sampled_subject_ids = set(admissions_sampled['subject_id'])\n",
        "        sampled_hadm_ids = set(admissions_sampled['hadm_id'])\n",
        "        patients= patients[patients['subject_id'].isin(sampled_subject_ids)]\n",
        "        diagnoses = diagnoses[diagnoses['hadm_id'].isin(sampled_hadm_ids)]\n",
        "        procedures = procedures[procedures['hadm_id'].isin(sampled_hadm_ids)]\n",
        "        symptoms = symptoms[symptoms['hadm_id'].isin(sampled_hadm_ids)]\n",
        "\n",
        "        admissions = admissions_sampled\n",
        "\n",
        "        print(\"\\nSampled dataset shapes:\")\n",
        "        print(f\"Patients dataset shape: {patients.shape}\")\n",
        "        print(f\"Admissions dataset shape: {admissions.shape}\")\n",
        "        print(f\"Diagnoses dataset shape: {diagnoses.shape}\")\n",
        "        print(f\"Procedures dataset shape: {procedures.shape}\")\n",
        "        print(f\"Symptoms dataset shape: {symptoms.shape}\")\n",
        "\n",
        "    print(\"\\nPatients data preview: \\n\", patients.head())\n",
        "    print(\"Admissions data preview: \\n\", admissions.head())\n",
        "    print(\"Diagnoses data preview: \\n\", diagnoses.head())\n",
        "    print(\"d_icd_diagnoses data preview: \\n\", d_icd_diagnoses.head())\n",
        "    print(\"d_icd_procedures data preview: \\n\", d_icd_procedures.head())\n",
        "    print(\"Procedures data preview: \\n\", procedures.head())\n",
        "    print(\"Symptoms data preview: \\n\", symptoms.head())\n",
        "    # Check for missing values\n",
        "    print(\"Patients missing values: \", patients.isnull().sum())\n",
        "    print(\"Admissions missing values: \", admissions.isnull().sum())\n",
        "    print(\"Diagnoses missing values: \", diagnoses.isnull().sum())\n",
        "\n",
        "    return patients, admissions, diagnoses, d_icd_diagnoses, d_icd_procedures, procedures, symptoms\n",
        "def create_readable_rules(rules_df, feature_mappings=None):\n",
        "    \"\"\"\n",
        "    Create a human-readable version of the rules.\n",
        "\n",
        "    Args:\n",
        "        rules_df (DataFrame): The rules DataFrame from association_rules function\n",
        "        feature_mappings (dict, optional): Mappings of feature codes to descriptions\n",
        "\n",
        "    Returns:\n",
        "        DataFrame: A DataFrame with human-readable rules\n",
        "    \"\"\"\n",
        "    print(\"Creating human-readable rules...\")\n",
        "\n",
        "    # Create a copy to avoid modifying the original\n",
        "    readable_rules = rules_df.copy()\n",
        "\n",
        "    # Function to format a single itemset to a readable string\n",
        "    def format_itemset(itemset, feature_mappings=None):\n",
        "        if isinstance(itemset, str):\n",
        "            # Parse string representation if needed\n",
        "            try:\n",
        "                itemset = ast.literal_eval(itemset)\n",
        "            except:\n",
        "                return itemset\n",
        "\n",
        "        items = []\n",
        "        for item in itemset:\n",
        "            if feature_mappings is not None:\n",
        "                # Try to map to a readable description\n",
        "                if item.startswith('Procedure_'):\n",
        "                    # Extract the code from the feature name\n",
        "                    proc_name = item.replace('Procedure_', '')\n",
        "                    # Get description if available\n",
        "                    if 'procedure_mapping' in feature_mappings:\n",
        "                        for code, desc in feature_mappings['procedure_mapping'].items():\n",
        "                            if desc == proc_name:\n",
        "                                items.append(f\"Procedure: {desc}\")\n",
        "                                break\n",
        "                        else:\n",
        "                            items.append(f\"Procedure: {proc_name}\")\n",
        "                    else:\n",
        "                        items.append(f\"Procedure: {proc_name}\")\n",
        "                elif item.startswith('Diagnosis_'):\n",
        "                    # Extract the code from the feature name\n",
        "                    diag_name = item.replace('Diagnosis_', '')\n",
        "                    # Get description if available\n",
        "                    if 'diagnosis_mapping' in feature_mappings:\n",
        "                        for code, desc in feature_mappings['diagnosis_mapping'].items():\n",
        "                            if desc == diag_name:\n",
        "                                items.append(f\"Diagnosis: {desc}\")\n",
        "                                break\n",
        "                        else:\n",
        "                            items.append(f\"Diagnosis: {diag_name}\")\n",
        "                    else:\n",
        "                        items.append(f\"Diagnosis: {diag_name}\")\n",
        "                elif item.startswith('Gender_'):\n",
        "                    items.append(f\"Gender: {item.replace('Gender_', '')}\")\n",
        "                elif item.startswith('Age_'):\n",
        "                    items.append(f\"Age Category: {item.replace('Age_', '')}\")\n",
        "                else:\n",
        "                    items.append(item)\n",
        "            else:\n",
        "                # Just clean up the feature name a bit\n",
        "                if item.startswith('Procedure_'):\n",
        "                    items.append(f\"Procedure: {item.replace('Procedure_', '')}\")\n",
        "                elif item.startswith('Diagnosis_'):\n",
        "                    items.append(f\"Diagnosis: {item.replace('Diagnosis_', '')}\")\n",
        "                elif item.startswith('Gender_'):\n",
        "                    items.append(f\"Gender: {item.replace('Gender_', '')}\")\n",
        "                elif item.startswith('Age_'):\n",
        "                    items.append(f\"Age Category: {item.replace('Age_', '')}\")\n",
        "                else:\n",
        "                    items.append(item)\n",
        "\n",
        "        return items\n",
        "\n",
        "    # Format antecedents and consequents\n",
        "    readable_rules['readable_antecedents'] = readable_rules['antecedents'].apply(\n",
        "        lambda x: format_itemset(x, feature_mappings)\n",
        "    )\n",
        "\n",
        "    readable_rules['readable_consequents'] = readable_rules['consequents'].apply(\n",
        "        lambda x: format_itemset(x, feature_mappings)\n",
        "    )\n",
        "\n",
        "    # Create rule strings\n",
        "    readable_rules['rule_string'] = readable_rules.apply(\n",
        "        lambda x: f\"{x['readable_antecedents']} => {x['readable_consequents']} \"\n",
        "                 f\"(Support: {x['support']:.3f}, Confidence: {x['confidence']:.3f}, Lift: {x['lift']:.3f})\",\n",
        "        axis=1\n",
        "    )\n",
        "\n",
        "    # Save to CSV\n",
        "    os.makedirs('output', exist_ok=True)\n",
        "    readable_rules[['rule_string', 'support', 'confidence', 'lift', 'readable_antecedents', 'readable_consequents']].to_csv('output/readable_rules.csv', index=False)\n",
        "\n",
        "    print(f\"Saved {len(readable_rules)} human-readable rules\")\n",
        "\n",
        "    return readable_rules\n",
        "def preprocess_data(patients, admissions, diagnoses, d_icd_diagnoses, symptoms):\n",
        "    def categorize_age(age):\n",
        "        if age < 18:\n",
        "            return 'Child'\n",
        "        elif age < 30:\n",
        "            return 'Young_Adult'\n",
        "        elif age < 50:\n",
        "            return 'Adult'\n",
        "        elif age < 70:\n",
        "            return 'Middle_Aged'\n",
        "        else:\n",
        "            return 'Elderly'\n",
        "\n",
        "    print(\"Starting preprocessing...\")\n",
        "    print(f\"Initial shapes - Patients: {patients.shape}, Admissions: {admissions.shape}, Diagnoses: {diagnoses.shape}\")\n",
        "\n",
        "    # 1. Handle missing values in patients\n",
        "    # No action needed as only dod is missing which is expected\n",
        "\n",
        "    #Handle missing values in admissions\n",
        "    essential_columns = ['hadm_id', 'subject_id']\n",
        "    admissions_subset = admissions[essential_columns].copy()\n",
        "\n",
        "    #admissions_subset[\"discharge_location\"] = admissions_subset[\"discharge_location\"].fillna(\"Unknown\")\n",
        "\n",
        "    #print(f\"Missing values in discharge_location after handling: {admissions_subset['discharge_location'].isna().sum()}\")\n",
        "\n",
        "    patients[\"age_category\"] = patients[\"anchor_age\"].apply(categorize_age)\n",
        "\n",
        "    #Merge diagnoses with description\n",
        "    diagnoses_with_desc= pd.merge(diagnoses, d_icd_diagnoses, how='left', left_on=[\"icd_code\", \"icd_version\"], right_on=[\"icd_code\", \"icd_version\"])\n",
        "    missing_desc = diagnoses_with_desc[diagnoses_with_desc[\"long_title\"].isnull()]\n",
        "\n",
        "    if len(missing_desc) > 0:\n",
        "        print(f\"WARNING: {len(missing_desc)} diagnosis codes have no description in the dictionary\")\n",
        "        #fill missing descriptions with code itself\n",
        "        diagnoses_with_desc[\"long_title\"] =  diagnoses_with_desc[\"long_title\"].fillna(\"Unlabeled_\" + diagnoses_with_desc[\"icd_code\"].astype(str))\n",
        "\n",
        "    #Merde patients with admissions (only essential columns)\n",
        "\n",
        "    patient_admissions = pd.merge(admissions_subset, patients[[\"subject_id\", \"anchor_age\", \"gender\", \"age_category\"]], how='left', on=\"subject_id\")\n",
        "\n",
        "    #get primary diagnosis\n",
        "\n",
        "    primary_diagnosis = diagnoses_with_desc[diagnoses_with_desc[\"seq_num\"] == 1].copy()\n",
        "\n",
        "    #create base transaction dataset\n",
        "    transactions_base = pd.merge(patient_admissions, primary_diagnosis[[\"subject_id\", \"hadm_id\", \"icd_code\", \"long_title\"]], how='inner', on=[\"subject_id\", \"hadm_id\"])\n",
        "\n",
        "    transactions_base = transactions_base.rename(columns={\"long_title\": \"primary_diagnosis\", \"icd_code\": \"primary_diagnosis_code\"})\n",
        "\n",
        "    #checking for missing values\n",
        "    missing_values = transactions_base.isnull().sum()\n",
        "\n",
        "    if missing_values.sum() > 0:\n",
        "        print(\"WARNING: Missing values found in transactions_base\")\n",
        "        print(missing_values[missing_values > 0])\n",
        "    transactions_with_symptoms = pd.merge(transactions_base, symptoms,\n",
        "                                          how='left', on='hadm_id')  # or 'subject_id' based on data\n",
        "\n",
        "    if 'Symptoms' in transactions_with_symptoms.columns:\n",
        "      transactions_with_symptoms['Symptoms'] = transactions_with_symptoms['Symptoms'].fillna(\"No_Symptom\")\n",
        "      transactions_base['Symptoms'] = transactions_with_symptoms['Symptoms']\n",
        "    else:\n",
        "      transactions_base['Symptoms'] = \"No_Symptom\"\n",
        "\n",
        "    print(f\"Created base transaction dataset with {len(transactions_base)} rows and {transactions_base.shape[1]} columns\")\n",
        "    print(f\"Columns in transactions_base: {transactions_base.columns.tolist()}\")\n",
        "    print(f\"Transactions base preview: \\n{transactions_base.head()}\")\n",
        "\n",
        "    return transactions_base, diagnoses_with_desc\n",
        "\n",
        "def create_readable_transaction_matrix(transactions_matrix):\n",
        "    \"\"\"\n",
        "    Converts a one-hot encoded transaction matrix to a human-readable format.\n",
        "\n",
        "    Args:\n",
        "        transactions_matrix (DataFrame): The one-hot encoded transaction matrix\n",
        "\n",
        "    Returns:\n",
        "        DataFrame: A DataFrame where each row contains the hadm_id and a list of active features\n",
        "    \"\"\"\n",
        "    print(\"Creating human-readable transaction matrix...\")\n",
        "\n",
        "    # Create a new DataFrame to store the results\n",
        "    readable_matrix = pd.DataFrame(index=transactions_matrix.index)\n",
        "    readable_matrix['hadm_id'] = readable_matrix.index\n",
        "    readable_matrix['active_features'] = ''\n",
        "\n",
        "    # For each row, collect the names of the columns where the value is 1\n",
        "    for idx in transactions_matrix.index:\n",
        "        # Get boolean series where True indicates a 1 in the original matrix\n",
        "        active_cols = transactions_matrix.loc[idx] == 1\n",
        "\n",
        "        # Get the names of active columns\n",
        "        active_features = active_cols.index[active_cols].tolist()\n",
        "\n",
        "        # Store in the new DataFrame\n",
        "        readable_matrix.loc[idx, 'active_features'] = str(active_features)\n",
        "\n",
        "    # Split the features by category for better readability\n",
        "    readable_matrix['demographics'] = readable_matrix['active_features'].apply(\n",
        "        lambda x: [f for f in eval(x) if f.startswith(('Gender_', 'Age_')) or f == 'anchor_age']\n",
        "    )\n",
        "\n",
        "    readable_matrix['procedures'] = readable_matrix['active_features'].apply(\n",
        "        lambda x: [f for f in eval(x) if f.startswith('Procedure_')]\n",
        "    )\n",
        "\n",
        "    readable_matrix['diagnoses'] = readable_matrix['active_features'].apply(\n",
        "        lambda x: [f for f in eval(x) if f.startswith('Diagnosis_')]\n",
        "    )\n",
        "\n",
        "    # Save to CSV\n",
        "    os.makedirs('output', exist_ok=True)\n",
        "    readable_matrix.to_csv('output/readable_transaction_matrix.csv', index=False)\n",
        "\n",
        "    print(f\"Saved human-readable transaction matrix with {len(readable_matrix)} rows\")\n",
        "\n",
        "    return readable_matrix\n",
        "\n",
        "def create_detailed_transaction_matrix(transactions_matrix, transactions_base, diagnoses_with_desc, procedures_with_desc):\n",
        "    \"\"\"\n",
        "    Creates a detailed, human-readable transaction matrix with decoded feature descriptions.\n",
        "\n",
        "    Args:\n",
        "        transactions_matrix (DataFrame): The one-hot encoded transaction matrix\n",
        "        transactions_base (DataFrame): The base transactions dataframe with raw data\n",
        "        diagnoses_with_desc (DataFrame): The diagnoses dataframe with descriptions\n",
        "        procedures_with_desc (DataFrame): The procedures dataframe with descriptions\n",
        "\n",
        "    Returns:\n",
        "        DataFrame: A detailed, human-readable transaction matrix\n",
        "    \"\"\"\n",
        "    print(\"Creating detailed human-readable transaction matrix...\")\n",
        "\n",
        "    # Create a mapping of hadm_id to patient info\n",
        "    patient_info = transactions_base[['hadm_id', 'subject_id', 'anchor_age', 'gender', 'age_category', 'primary_diagnosis', 'primary_diagnosis_code']].drop_duplicates()\n",
        "    patient_info_dict = patient_info.set_index('hadm_id').to_dict('index')\n",
        "\n",
        "    # Create a new dataframe\n",
        "    detailed_matrix = pd.DataFrame(index=transactions_matrix.index)\n",
        "    detailed_matrix['hadm_id'] = detailed_matrix.index\n",
        "\n",
        "    # Add patient demographic information\n",
        "    detailed_matrix['subject_id'] = detailed_matrix['hadm_id'].map(lambda x: patient_info_dict.get(x, {}).get('subject_id', 'Unknown'))\n",
        "    detailed_matrix['age'] = detailed_matrix['hadm_id'].map(lambda x: patient_info_dict.get(x, {}).get('anchor_age', 'Unknown'))\n",
        "    detailed_matrix['gender'] = detailed_matrix['hadm_id'].map(lambda x: patient_info_dict.get(x, {}).get('gender', 'Unknown'))\n",
        "    detailed_matrix['age_category'] = detailed_matrix['hadm_id'].map(lambda x: patient_info_dict.get(x, {}).get('age_category', 'Unknown'))\n",
        "    detailed_matrix['primary_diagnosis'] = detailed_matrix['hadm_id'].map(lambda x: patient_info_dict.get(x, {}).get('primary_diagnosis', 'Unknown'))\n",
        "    detailed_matrix['primary_diagnosis_code'] = detailed_matrix['hadm_id'].map(lambda x: patient_info_dict.get(x, {}).get('primary_diagnosis_code', 'Unknown'))\n",
        "\n",
        "    # Create column for active procedures\n",
        "    # Get procedure columns from transaction matrix\n",
        "    procedure_columns = [col for col in transactions_matrix.columns if col.startswith('Procedure_')]\n",
        "\n",
        "    # For each admission, find which procedures are active (value = 1)\n",
        "    def get_active_procedures(hadm_id):\n",
        "        # Check if hadm_id exists in transactions_matrix\n",
        "        if hadm_id not in transactions_matrix.index:\n",
        "            return []\n",
        "\n",
        "        # Get active procedures\n",
        "        row = transactions_matrix.loc[hadm_id]\n",
        "        active_procs = [col for col in procedure_columns if row[col] == 1]\n",
        "\n",
        "        # Extract procedure names from column names\n",
        "        proc_names = [col.replace('Procedure_', '') for col in active_procs]\n",
        "\n",
        "        return proc_names\n",
        "\n",
        "    # Map procedure ICD codes to descriptions if available\n",
        "    detailed_matrix['active_procedures'] = detailed_matrix['hadm_id'].apply(get_active_procedures)\n",
        "\n",
        "    # Get procedure descriptions from procedures_with_desc\n",
        "    if procedures_with_desc is not None and not procedures_with_desc.empty:\n",
        "        proc_desc_dict = dict(zip(\n",
        "            procedures_with_desc['long_title'],\n",
        "            procedures_with_desc['long_title']\n",
        "        ))\n",
        "\n",
        "        def format_procedures(proc_list):\n",
        "            if not proc_list:\n",
        "                return []\n",
        "            return [f\"{proc}\" for proc in proc_list]\n",
        "\n",
        "        detailed_matrix['active_procedures'] = detailed_matrix['active_procedures'].apply(format_procedures)\n",
        "\n",
        "    # Get diagnosis columns\n",
        "    diagnosis_columns = [col for col in transactions_matrix.columns if col.startswith('Diagnosis_')]\n",
        "\n",
        "    # For each admission, find which diagnoses are active\n",
        "    def get_active_diagnoses(hadm_id):\n",
        "        # Check if hadm_id exists in transactions_matrix\n",
        "        if hadm_id not in transactions_matrix.index:\n",
        "            return []\n",
        "\n",
        "        # Get active diagnoses\n",
        "        row = transactions_matrix.loc[hadm_id]\n",
        "        active_diags = [col for col in diagnosis_columns if row[col] == 1]\n",
        "\n",
        "        # Extract diagnosis names from column names\n",
        "        diag_names = [col.replace('Diagnosis_', '') for col in active_diags]\n",
        "\n",
        "        return diag_names\n",
        "\n",
        "    detailed_matrix['active_diagnoses'] = detailed_matrix['hadm_id'].apply(get_active_diagnoses)\n",
        "\n",
        "    # Count the number of active features in each category\n",
        "    detailed_matrix['procedure_count'] = detailed_matrix['active_procedures'].apply(len)\n",
        "    detailed_matrix['diagnosis_count'] = detailed_matrix['active_diagnoses'].apply(len)\n",
        "    detailed_matrix['total_feature_count'] = detailed_matrix['procedure_count'] + detailed_matrix['diagnosis_count']\n",
        "\n",
        "    # Save to CSV\n",
        "    os.makedirs('output', exist_ok=True)\n",
        "    detailed_matrix.to_csv('output/detailed_transaction_matrix.csv', index=False)\n",
        "\n",
        "    print(f\"Saved detailed human-readable transaction matrix with {len(detailed_matrix)} rows\")\n",
        "\n",
        "    return detailed_matrix\n",
        "\n",
        "def engineer_features(transactions_base, procedures, d_icd_procedures, diagnoses_with_desc, symptoms):\n",
        "    print(\"Starting feature engineering...\")\n",
        "\n",
        "    try:\n",
        "        max_features_count = int(os.environ.get(\"MAX_FEATURES_COUNT\", 1000))\n",
        "        print(f\"Max features count set to {max_features_count}\")\n",
        "    except (ValueError, TypeError):\n",
        "        print(\"Invalid MAX_FEATURES_COUNT environment variable. Using default value of 1000.\")\n",
        "        max_features_count = 1000\n",
        "\n",
        "    # 1. Merge procedures with descriptions\n",
        "    procedures_with_desc = pd.merge(procedures, d_icd_procedures, how='left', left_on=[\"icd_code\", \"icd_version\"], right_on=[\"icd_code\", \"icd_version\"])\n",
        "    missing_proc_desc = procedures_with_desc[procedures_with_desc[\"long_title\"].isnull()]\n",
        "    if len(missing_proc_desc) > 0:\n",
        "        print(f\"WARNING: {len(missing_proc_desc)} procedure codes have no description in the dictionary\")\n",
        "        procedures_with_desc[\"long_title\"] = procedures_with_desc[\"long_title\"].fillna(\"Unlabeled_\" + procedures_with_desc[\"icd_code\"].astype(str))\n",
        "\n",
        "    # Create procedure presence feature\n",
        "    procedure_counts = procedures_with_desc[\"long_title\"].value_counts()\n",
        "    min_procedure_freq = 2\n",
        "\n",
        "    common_procedures = procedure_counts[procedure_counts >= min_procedure_freq].index.tolist()\n",
        "    print(f\"Using {len(common_procedures)} common procedures for feature engineering out of {len(procedure_counts)} total procedures\")\n",
        "\n",
        "    procedures_filtered = procedures_with_desc[procedures_with_desc[\"long_title\"].isin(common_procedures)]\n",
        "\n",
        "    print(f\"Filtered procedures dataset shape: {procedures_filtered.shape}\")\n",
        "    print(f\"Filtered procedures dataset preview: \\n{procedures_filtered.head()}\")\n",
        "\n",
        "    # Checking if procedures are more than the frequency threshold\n",
        "    if len(procedures_filtered) == 0:\n",
        "        print(\"WARNING: No procedures match the frequency threshold. Reducing threshold.\")\n",
        "        min_procedure_freq = 10\n",
        "        common_procedures = procedure_counts[procedure_counts >= min_procedure_freq].index.tolist()\n",
        "        procedures_filtered = procedures_with_desc[procedures_with_desc['long_title'].isin(common_procedures)]\n",
        "        print(f\"Using {len(common_procedures)} procedures with reduced threshold\")\n",
        "\n",
        "    # Create mapping of procedure codes to descriptions for readability\n",
        "    procedure_mapping = procedures_with_desc[['icd_code', 'long_title']].drop_duplicates().set_index('icd_code')['long_title'].to_dict()\n",
        "\n",
        "    if len(procedures_filtered) > 0:\n",
        "        procedures_pivot = pd.get_dummies(procedures_filtered[[\"hadm_id\", \"long_title\"]], columns=[\"long_title\"], prefix=\"Procedure\", prefix_sep=\"_\")\n",
        "        procedures_by_admission= procedures_pivot.groupby(\"hadm_id\").max()\n",
        "        print(f\"Created procedures with admissons with shape: {procedures_by_admission.shape}\")\n",
        "    else:\n",
        "        print(\"WARNING: No procedures found after filtering. Skipping procedure feature engineering.\")\n",
        "        procedures_by_admission = pd.DataFrame(index = transactions_base[\"hadm_id\"].unique())\n",
        "\n",
        "    # Create demographic features - REMOVING anchor_age as requested\n",
        "    demographic_cols = [\"hadm_id\", \"gender\", \"age_category\"]  # Removed anchor_age\n",
        "\n",
        "    demographic_features = pd.get_dummies(transactions_base[demographic_cols], columns=[\"gender\", \"age_category\"], prefix=[\"Gender\", \"Age\"], prefix_sep=\"_\")\n",
        "\n",
        "    if \"hospital_expire_flag\" in demographic_features.columns:\n",
        "        demographic_features[\"Expired_In_Hospital\"] = demographic_features[\"hospital_expire_flag\"]\n",
        "        demographic_features = demographic_features.drop(\"hospital_expire_flag\", axis=1)\n",
        "\n",
        "    demographics_by_admission = demographic_features.groupby(\"hadm_id\").first()\n",
        "    print(f\"Created demographic features with shape: {demographics_by_admission.shape}\")\n",
        "\n",
        "    print(f\"Demographic features preview: \\n{demographics_by_admission.head()}\")\n",
        "        # === symptoms ===\n",
        "    print(\"Processing symptom features...\")\n",
        "    if 'hadm_id' not in symptoms.columns:\n",
        "        raise ValueError(\"The symptoms dataframe must contain a 'hadm_id' column to join with transactions_base.\")\n",
        "\n",
        "    if 'Symptoms' not in symptoms.columns:\n",
        "        raise ValueError(\"The symptoms dataframe must contain a 'symptom_text' column with symptom labels.\")\n",
        "\n",
        "    #deleting dublicates\n",
        "    symptoms = symptoms[['hadm_id', 'Symptoms']].dropna().drop_duplicates()\n",
        "\n",
        "    # One-hot encode: every unique symptom_text → new binary\n",
        "    symptom_dummies = pd.get_dummies(symptoms['Symptoms'], prefix='Symptom')\n",
        "    symptoms_encoded = pd.concat([symptoms[['hadm_id']].reset_index(drop=True), symptom_dummies.reset_index(drop=True)], axis=1)\n",
        "    symptoms_by_admission = symptoms_encoded.groupby('hadm_id').max()\n",
        "\n",
        "    print(f\"Created symptom features with shape: {symptoms_by_admission.shape}\")\n",
        "    print(f\"Symptoms preview:\\n{symptoms_by_admission.head()}\")\n",
        "\n",
        "\n",
        "    # Merge all features into one dataset\n",
        "    all_features = pd.DataFrame(index = transactions_base[\"hadm_id\"].unique())\n",
        "\n",
        "    # List to keep track of dataframes with potential join issues\n",
        "    empty_dfs = []\n",
        "    for name, df in [(\"Procedures\", procedures_by_admission), (\"Demographics\", demographics_by_admission), (\"Symptoms\", symptoms_by_admission)]:\n",
        "        if df.empty:\n",
        "            print(f\"WARNING: {name} dataframe is empty.\")\n",
        "            empty_dfs.append(name)\n",
        "            continue\n",
        "        before_rows = len(all_features)\n",
        "        all_features = all_features.join(df, how='left')\n",
        "        after_rows = len(all_features)\n",
        "\n",
        "        if before_rows != after_rows:\n",
        "            print(f\"WARNING: {name} dataframe caused a join issue. Rows before: {before_rows}, Rows after: {after_rows}\")\n",
        "\n",
        "    if empty_dfs:\n",
        "        print(f\"WARNING: The following dataframes were empty and not included in the final dataset: {', '.join(empty_dfs)}\")\n",
        "\n",
        "    # Fill missing values with 0\n",
        "    all_features = all_features.fillna(0)\n",
        "\n",
        "    # 8. Create diagnosis outcome features\n",
        "    diagnosis_counts = transactions_base['primary_diagnosis'].value_counts()\n",
        "\n",
        "    # Create mapping of diagnosis codes to descriptions for readability\n",
        "    diagnosis_mapping = transactions_base[['primary_diagnosis_code', 'primary_diagnosis']].drop_duplicates().set_index('primary_diagnosis_code')['primary_diagnosis'].to_dict()\n",
        "\n",
        "    # We want at least 10 diagnoses, but respect max_features budget\n",
        "    max_procedure_features = max_features_count // 2  # Reserve half for procedures, half for diagnoses\n",
        "    diagnosis_feature_limit = max(10, max_features_count - len(demographics_by_admission.columns) -\n",
        "                            min(max_procedure_features,\n",
        "                                len(procedures_by_admission.columns) if hasattr(procedures_by_admission, 'columns') else 0))\n",
        "\n",
        "    # Simply take the top N most frequent diagnoses\n",
        "    common_diagnoses = diagnosis_counts.nlargest(min(diagnosis_feature_limit, len(diagnosis_counts))).index.tolist()\n",
        "\n",
        "    print(f\"Using {len(common_diagnoses)} most common diagnoses out of {len(diagnosis_counts)} total\")\n",
        "    diagnoses_filtered = transactions_base[transactions_base['primary_diagnosis'].isin(common_diagnoses)]\n",
        "    print(f\"Filtered diagnoses dataset shape: {diagnoses_filtered.shape}\")\n",
        "    print(f\"Filtered diagnoses dataset preview: \\n{diagnoses_filtered.head()}\")\n",
        "\n",
        "    # Check if we have diagnoses left after filtering\n",
        "    if len(diagnoses_filtered) == 0:\n",
        "        print(\"ERROR: No diagnoses meet the frequency threshold. Unable to create meaningful rules.\")\n",
        "        print(\"Please check your data or reduce the threshold further.\")\n",
        "        # Return a minimal dataframe to avoid errors\n",
        "        return pd.DataFrame(columns=['no_features_available'])\n",
        "\n",
        "    diagnosis_pivot = pd.get_dummies(diagnoses_filtered[['hadm_id', 'primary_diagnosis']],\n",
        "                                    columns=['primary_diagnosis'],\n",
        "                                    prefix='Diagnosis',\n",
        "                                    prefix_sep='_')\n",
        "    diagnosis_by_admission = diagnosis_pivot.groupby('hadm_id').max()\n",
        "\n",
        "    print(f\"Created diagnosis features with shape: {diagnosis_by_admission.shape}\")\n",
        "    print(f\"Diagnosis features preview: \\n{diagnosis_by_admission.head()}\")\n",
        "\n",
        "    transactions_matrix = all_features.join(diagnosis_by_admission, how='inner')\n",
        "\n",
        "    # Printing transaction matrix shape\n",
        "    print(\"TRANSACTION MATRIX\")\n",
        "    print(f\"Transaction matrix shape after joining features and outcomes: {transactions_matrix.shape}\")\n",
        "    print(f\"Transaction matrix preview: \\n{transactions_matrix.head()}\")\n",
        "    if transactions_matrix.empty:\n",
        "        print(\"ERROR: Empty transaction matrix after joining features and outcomes.\")\n",
        "        print(\"Please check that hadm_ids are consistent across your datasets.\")\n",
        "        return pd.DataFrame(columns=['empty_transactions_matrix'])\n",
        "    if (transactions_matrix.nunique() > 2).all():\n",
        "        print(\"WARNING: No binary features found in transaction matrix. Check your data transformations.\")\n",
        "\n",
        "    # Check for excessive NaN values\n",
        "    nan_percentage = transactions_matrix.isna().mean().mean() * 100\n",
        "    if nan_percentage > 0:\n",
        "        print(f\"WARNING: Transaction matrix contains {nan_percentage:.2f}% NaN values\")\n",
        "        transactions_matrix = transactions_matrix.fillna(0)\n",
        "\n",
        "    print(f\"Final transaction matrix: {transactions_matrix.shape[0]} rows and {transactions_matrix.shape[1]} columns\")\n",
        "    print(f\"Features include {len(demographics_by_admission.columns)} demographic features, \"\n",
        "         f\"{len(procedures_by_admission.columns) if hasattr(procedures_by_admission, 'columns') else 0} procedure features, \"\n",
        "         f\"and {len(diagnosis_by_admission.columns) if hasattr(diagnosis_by_admission, 'columns') else 0} diagnosis outcomes\")\n",
        "\n",
        "    # Save the transaction matrix\n",
        "    os.makedirs('output', exist_ok=True)\n",
        "    transactions_matrix.to_csv('output/transaction_matrix.csv')\n",
        "\n",
        "    # Save mappings for later use in readability\n",
        "    mappings = {\n",
        "        'procedure_mapping': procedure_mapping,\n",
        "        'diagnosis_mapping': diagnosis_mapping\n",
        "    }\n",
        "\n",
        "    with open('output/feature_mappings.pkl', 'wb') as f:\n",
        "        pickle.dump(mappings, f)\n",
        "\n",
        "    # Create and save human-readable versions of the transaction matrix\n",
        "    readable_matrix = create_readable_transaction_matrix(transactions_matrix)\n",
        "    detailed_matrix = create_detailed_transaction_matrix(transactions_matrix, transactions_base, diagnoses_with_desc, procedures_with_desc)\n",
        "\n",
        "    # For debug purposes, also save feature counts\n",
        "    feature_counts = pd.Series({\n",
        "        'demographic_features': len(demographics_by_admission.columns),\n",
        "        'procedure_features': len(procedures_by_admission.columns) if hasattr(procedures_by_admission, 'columns') else 0,\n",
        "        'diagnosis_features': len(diagnosis_by_admission.columns) if hasattr(diagnosis_by_admission, 'columns') else 0,\n",
        "        'total_features': transactions_matrix.shape[1],\n",
        "        'total_transactions': transactions_matrix.shape[0]\n",
        "    })\n",
        "    feature_counts.to_csv('output/feature_counts.csv')\n",
        "\n",
        "    return transactions_matrix\n",
        "\n",
        "\n",
        "def filter_diagnosis_to_procedure_demographic_rules(rules_df, transactions_matrix):\n",
        "    \"\"\"\n",
        "    Filter rules where antecedents (LHS) are diagnoses and consequents (RHS) are\n",
        "    procedures or demographic features.\n",
        "\n",
        "    Args:\n",
        "        rules_df (DataFrame): The complete set of association rules\n",
        "        transactions_matrix (DataFrame): The transaction matrix to identify feature types\n",
        "\n",
        "    Returns:\n",
        "        DataFrame: Filtered rules\n",
        "    \"\"\"\n",
        "    if rules_df.empty:\n",
        "        print(\"No rules to filter.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    # Identify feature types from the transaction matrix\n",
        "    diagnosis_cols = [col for col in transactions_matrix.columns if col.startswith('Diagnosis_')]\n",
        "    procedure_cols = [col for col in transactions_matrix.columns if col.startswith('Procedure_')]\n",
        "    # Modify demographic cols to exclude anchor_age\n",
        "    demographic_cols = [col for col in transactions_matrix.columns\n",
        "                       if col.startswith(('Gender_', 'Age_'))]  # Removed anchor_age\n",
        "\n",
        "    # Filter rules where:\n",
        "    # 1. Antecedents (LHS) contain only diagnosis features\n",
        "    # 2. Consequents (RHS) contain only procedure or demographic features\n",
        "    filtered_rules = rules_df[rules_df.apply(\n",
        "        lambda row: (\n",
        "            # Check that all antecedents are diagnoses\n",
        "            all(item in diagnosis_cols for item in row['antecedents'])\n",
        "            and\n",
        "            # Check that all consequents are either procedures or demographics\n",
        "            all(item in procedure_cols or item in demographic_cols for item in row['consequents'])\n",
        "        ),\n",
        "        axis=1\n",
        "    )]\n",
        "\n",
        "    # Check if we found any matching rules\n",
        "    if filtered_rules.empty:\n",
        "        print(\"No rules matching the Diagnosis → Procedure/Demographic pattern.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    print(f\"Found {len(filtered_rules)} rules where diagnoses predict procedures or demographics.\")\n",
        "\n",
        "    # Sort by lift for most interesting rules first\n",
        "    filtered_rules = filtered_rules.sort_values('lift', ascending=False)\n",
        "\n",
        "    # Save the filtered rules\n",
        "    os.makedirs('output', exist_ok=True)\n",
        "    filtered_rules.to_csv('output/diagnosis_to_proc_demo_rules.csv', index=False)\n",
        "\n",
        "    # Create a human-readable version\n",
        "    try:\n",
        "        # Load the feature mappings if available\n",
        "        if os.path.exists('output/feature_mappings.pkl'):\n",
        "            with open('output/feature_mappings.pkl', 'rb') as f:\n",
        "                feature_mappings = pickle.load(f)\n",
        "\n",
        "            # Create a human-readable version of the filtered rules\n",
        "            readable_filtered_rules = create_readable_rules(filtered_rules, feature_mappings)\n",
        "            readable_filtered_rules.to_csv('output/readable_diagnosis_to_proc_demo_rules.csv', index=False)\n",
        "    except Exception as e:\n",
        "        print(f\"WARNING: Could not create readable filtered rules: {str(e)}\")\n",
        "\n",
        "    return filtered_rules\n",
        "\n",
        "def mine_association_rules(transactions_matrix, min_support=0.0006, min_confidence=0.5):\n",
        "    print(\"Starting association rule mining...\")\n",
        "\n",
        "    # Convert the DataFrame to a one-hot encoded format\n",
        "    transactions_matrix_bool = transactions_matrix.astype(bool)\n",
        "\n",
        "    min_support_floor = min_support/10\n",
        "    min_confidence_floor = min_confidence/2\n",
        "\n",
        "    # Check if we should use a sample\n",
        "    sample_size = 0\n",
        "    try:\n",
        "        sample_size = int(os.environ.get('SAMPLE_SIZE', '0'))\n",
        "    except (ValueError, TypeError):\n",
        "        sample_size = 0\n",
        "\n",
        "    if sample_size > 0 and sample_size < transactions_matrix.shape[0]:\n",
        "        print(f\"Using a sample of {sample_size} transactions\")\n",
        "        transactions_matrix_bool = transactions_matrix_bool.sample(sample_size)\n",
        "\n",
        "    # 1. Find frequent itemsets with adaptive support threshold using FP-Growth\n",
        "    # Try to find a reasonable number of itemsets\n",
        "    frequent_itemsets = pd.DataFrame()\n",
        "\n",
        "    try:\n",
        "        # Using fpgrowth instead of apriori\n",
        "        frequent_itemsets = fpgrowth(transactions_matrix_bool,\n",
        "                                   min_support=min_support,\n",
        "                                   use_colnames=True,\n",
        "                                   max_len=4)  # Limit to combinations of at most 4 items\n",
        "\n",
        "        # If we found too few itemsets, try with a lower threshold\n",
        "        if len(frequent_itemsets) < 10:\n",
        "            old_support = min_support\n",
        "            min_support = max(min_support_floor, min_support / 2)\n",
        "            print(f\"Found too few itemsets ({len(frequent_itemsets)}). Reducing support from {old_support} to {min_support}\")\n",
        "\n",
        "            frequent_itemsets = fpgrowth(transactions_matrix_bool,\n",
        "                                       min_support=min_support,\n",
        "                                       use_colnames=True,\n",
        "                                       max_len=4)\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR in FP-Growth algorithm: {str(e)}\")\n",
        "        print(\"Trying with a smaller dataset...\")\n",
        "\n",
        "        # Sample the data if it's too large\n",
        "        if transactions_matrix.shape[0] > 10000:\n",
        "            sample_size = min(10000, int(transactions_matrix.shape[0] * 0.5))\n",
        "            transactions_sample = transactions_matrix.sample(sample_size)\n",
        "            try:\n",
        "                frequent_itemsets = fpgrowth(transactions_sample.astype(bool),\n",
        "                                           min_support=min_support,\n",
        "                                           use_colnames=True,\n",
        "                                           max_len=3)\n",
        "                print(f\"Successfully ran FP-Growth on a sample of {sample_size} transactions\")\n",
        "            except Exception as e2:\n",
        "                print(f\"ERROR in FP-Growth algorithm even with sampling: {str(e2)}\")\n",
        "                return pd.DataFrame(), pd.DataFrame(), pd.DataFrame(), pd.DataFrame()\n",
        "\n",
        "    if frequent_itemsets.empty:\n",
        "        print(\"No frequent itemsets found. Cannot generate rules.\")\n",
        "        return pd.DataFrame(), pd.DataFrame(), pd.DataFrame(), pd.DataFrame()\n",
        "\n",
        "    print(f\"Found {len(frequent_itemsets)} frequent itemsets with min_support={min_support}\")\n",
        "\n",
        "    # Save frequent itemsets\n",
        "    os.makedirs('output', exist_ok=True)\n",
        "    frequent_itemsets.to_csv('output/frequent_itemsets.csv')\n",
        "\n",
        "    # 2. Generate association rules with adaptive confidence threshold\n",
        "    rules = pd.DataFrame()\n",
        "\n",
        "    try:\n",
        "        rules = association_rules(frequent_itemsets,\n",
        "                                 metric='confidence',\n",
        "                                 min_threshold=min_confidence)\n",
        "\n",
        "        # If we found too few rules, try with a lower threshold\n",
        "        if len(rules) < 10:\n",
        "            old_confidence = min_confidence\n",
        "            min_confidence = max(min_confidence_floor, min_confidence / 1.5)\n",
        "            print(f\"Found too few rules ({len(rules)}). Reducing confidence from {old_confidence} to {min_confidence}\")\n",
        "\n",
        "            rules = association_rules(frequent_itemsets,\n",
        "                                    metric='confidence',\n",
        "                                    min_threshold=min_confidence)\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR in association_rules algorithm: {str(e)}\")\n",
        "        return pd.DataFrame(), pd.DataFrame(), pd.DataFrame(), pd.DataFrame()\n",
        "\n",
        "    if rules.empty:\n",
        "        print(\"No rules generated. Cannot proceed with rule filtering.\")\n",
        "        return frequent_itemsets, pd.DataFrame(), pd.DataFrame(), pd.DataFrame()\n",
        "\n",
        "    print(f\"Generated {len(rules)} rules with min_confidence={min_confidence}\")\n",
        "\n",
        "    # Save all rules\n",
        "    rules.to_csv('output/all_rules.csv', index=False)\n",
        "\n",
        "    # 3. Filter rules to focus on procedures\n",
        "    procedure_cols = [col for col in transactions_matrix.columns if col.startswith('Procedure_')]\n",
        "    if not procedure_cols:\n",
        "        print(\"ERROR: No procedures columns found in transaction matrix\")\n",
        "        return frequent_itemsets, rules, pd.DataFrame(), pd.DataFrame()\n",
        "\n",
        "    print(f\"Found {len(procedure_cols)} procedures columns to use for rule filtering\")\n",
        "\n",
        "    # Convert string representations of sets to actual sets, if needed\n",
        "    if isinstance(rules['antecedents'].iloc[0], str):\n",
        "        rules['antecedents'] = rules['antecedents'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
        "        rules['consequents'] = rules['consequents'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
        "\n",
        "    # Filter for rules that predict procedures\n",
        "    procedure_rules = rules[rules['consequents'].apply(\n",
        "        lambda x: any(item in procedure_cols for item in x)\n",
        "    )].copy()\n",
        "\n",
        "    if procedure_rules.empty:\n",
        "        print(\"WARNING: No rules found with procedures in the consequent\")\n",
        "    else:\n",
        "        print(f\"Found {len(procedure_rules)} rules with procedures in the consequent\")\n",
        "\n",
        "        # Additional filters to focus on more interesting rules\n",
        "        if len(procedure_rules) > 1000:\n",
        "            print(f\"Too many rules ({len(procedure_rules)}). Filtering to more interesting ones...\")\n",
        "\n",
        "            # Filter by lift (stronger associations)\n",
        "            high_lift_rules = procedure_rules[procedure_rules['lift'] > 1.5]\n",
        "            if len(high_lift_rules) >= 100:\n",
        "                procedure_rules = high_lift_rules\n",
        "                print(f\"Filtered to {len(procedure_rules)} rules with lift > 1.5\")\n",
        "\n",
        "        # Sort by lift and then confidence\n",
        "        procedure_rules = procedure_rules.sort_values(['lift', 'confidence'], ascending=[False, False])\n",
        "\n",
        "        # Save procedure rules\n",
        "        procedure_rules.to_csv('output/procedure_rules.csv', index=False)\n",
        "\n",
        "    # 4. Filter for diagnosis -> procedure/demographic rules\n",
        "    diagnosis_to_proc_demo_rules = filter_diagnosis_to_procedure_demographic_rules(rules, transactions_matrix)\n",
        "\n",
        "    # 5. Create human-readable versions of the rules\n",
        "    try:\n",
        "        # Load the feature mappings if available\n",
        "        if os.path.exists('output/feature_mappings.pkl'):\n",
        "            with open('output/feature_mappings.pkl', 'rb') as f:\n",
        "                feature_mappings = pickle.load(f)\n",
        "\n",
        "            # Create human-readable versions of the rules\n",
        "            if not procedure_rules.empty:\n",
        "                readable_rules = create_readable_rules(procedure_rules, feature_mappings)\n",
        "    except Exception as e:\n",
        "        print(f\"WARNING: Could not create readable rules: {str(e)}\")\n",
        "\n",
        "    return frequent_itemsets, rules, procedure_rules, diagnosis_to_proc_demo_rules\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    import argparse\n",
        "\n",
        "    parser = argparse.ArgumentParser(description='Run the association rule mining pipeline')\n",
        "    parser.add_argument('--sample_fraction', type=float, default=0.01, help='Fraction of data to sample')\n",
        "    parser.add_argument('--min_support', type=float, default=0.0006, help='Minimum support threshold')\n",
        "    parser.add_argument('--min_confidence', type=float, default=0.5, help='Minimum confidence threshold')\n",
        "    parser.add_argument('--skip_visualizations', action='store_true', help='Skip creating visualizations')\n",
        "    parser.add_argument('--skip_report', action='store_true', help='Skip generating HTML report')\n",
        "    parser.add_argument('--read_only', action='store_true', help='Only read existing data without reprocessing')\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    print(f\"Running pipeline with:\")\n",
        "    print(f\"  - sample_fraction: {args.sample_fraction}\")\n",
        "    print(f\"  - min_support: {args.min_support}\")\n",
        "    print(f\"  - min_confidence: {args.min_confidence}\")\n",
        "    print(f\"  - visualizations: {'Disabled' if args.skip_visualizations else 'Enabled'}\")\n",
        "    print(f\"  - HTML report: {'Disabled' if args.skip_report else 'Enabled'}\")\n",
        "    print(f\"  - mode: {'Read-only' if args.read_only else 'Full processing'}\")\n",
        "\n",
        "    # If we're in read-only mode, we'll just load existing files\n",
        "    if args.read_only:\n",
        "        print(\"\\nRunning in read-only mode. Loading existing data...\")\n",
        "\n",
        "        if os.path.exists('output/transaction_matrix.csv'):\n",
        "            transactions_matrix = pd.read_csv('output/transaction_matrix.csv', index_col=0)\n",
        "            print(f\"Loaded transaction matrix with shape {transactions_matrix.shape}\")\n",
        "\n",
        "            if os.path.exists('output/all_rules.csv'):\n",
        "                rules = pd.read_csv('output/all_rules.csv')\n",
        "                print(f\"Loaded {len(rules)} rules\")\n",
        "\n",
        "                # Convert string representations of sets to actual sets for visualization\n",
        "                if 'antecedents' in rules.columns and isinstance(rules['antecedents'].iloc[0], str):\n",
        "                    rules['antecedents'] = rules['antecedents'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
        "                    rules['consequents'] = rules['consequents'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
        "            else:\n",
        "                rules = pd.DataFrame()\n",
        "                print(\"No rules file found.\")\n",
        "\n",
        "            if os.path.exists('output/procedure_rules.csv'):\n",
        "                procedure_rules = pd.read_csv('output/procedure_rules.csv')\n",
        "                print(f\"Loaded {len(procedure_rules)} procedure rules\")\n",
        "\n",
        "                # Convert string representations of sets to actual sets for visualization\n",
        "                if 'antecedents' in procedure_rules.columns and isinstance(procedure_rules['antecedents'].iloc[0], str):\n",
        "                    procedure_rules['antecedents'] = procedure_rules['antecedents'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
        "                    procedure_rules['consequents'] = procedure_rules['consequents'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
        "            else:\n",
        "                procedure_rules = pd.DataFrame()\n",
        "                print(\"No procedure rules file found.\")\n",
        "\n",
        "            # Create visualizations if requested\n",
        "            if not args.skip_visualizations and visualization_available:\n",
        "                print(\"\\nCreating visualizations from existing data...\")\n",
        "\n",
        "                # Visualize feature distribution in transaction matrix\n",
        "                visualize_feature_distribution(transactions_matrix, save_path='output/feature_distribution.png')\n",
        "\n",
        "                # Visualize rule metrics\n",
        "                if not rules.empty:\n",
        "                    visualize_rule_metrics(rules, save_path='output/rule_metrics.png')\n",
        "                    visualize_rules_summary(rules, save_path='output/rules_summary.png')\n",
        "\n",
        "                # Visualize procedure rules network\n",
        "                if not procedure_rules.empty:\n",
        "                    visualize_rules_network(procedure_rules, max_rules=50, min_lift=1.0,\n",
        "                                          save_path='output/procedure_rules_network.png')\n",
        "        else:\n",
        "            print(\"ERROR: No transaction matrix file found. Cannot proceed in read-only mode.\")\n",
        "    else:\n",
        "        # Run the full pipeline\n",
        "        frequent_itemsets, rules, procedure_rules = main(\n",
        "            args.sample_fraction,\n",
        "            args.min_support,\n",
        "            args.min_confidence,\n",
        "            not args.skip_visualizations,\n",
        "            not args.skip_report\n",
        "        )\n",
        "\n",
        "        print(\"\\nPipeline completed successfully!\")\n",
        "\n",
        "        if not procedure_rules.empty:\n",
        "            print(f\"\\nTop 5 procedure rules by lift:\")\n",
        "            print(procedure_rules.sort_values('lift', ascending=False).head(5)[['antecedents', 'consequents', 'support', 'confidence', 'lift']])\n",
        "\n",
        "            print(\"\\nCheck the output directory for detailed results and human-readable formats.\")\n",
        "        else:\n",
        "            print(\"\\nNo procedure rules were found. Try adjusting the parameters or check your data.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "frequent_itemsets, rules, procedure_rules = main(\n",
        "    sample_fraction=0.01,\n",
        "    min_support=0.0006,\n",
        "    min_confidence=0.5,\n",
        "    visualize=False,\n",
        "    generate_report=False\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_XpWM3IY-F5c",
        "outputId": "e3e07d96-00fa-4d1d-a55a-9ed664b47674"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data with sampling fraction: 0.01\n",
            "Original dataset shapes:\n",
            "Patients dataset shape: (364627, 6)\n",
            "Admissions dataset shape: (30281, 16)\n",
            "Diagnoses dataset shape: (6364488, 5)\n",
            "d_icd_diagnoses dataset shape: (112107, 3)\n",
            "d_icd_procedures dataset shape: (86423, 3)\n",
            "Procedures dataset shape: (859655, 6)\n",
            "Procedures dataset shape: (331793, 10)\n",
            "Sample size of the data: 10000\n",
            "\n",
            "Sampled dataset shapes:\n",
            "Patients dataset shape: (0, 6)\n",
            "Admissions dataset shape: (10000, 16)\n",
            "Diagnoses dataset shape: (0, 5)\n",
            "Procedures dataset shape: (0, 6)\n",
            "Symptoms dataset shape: (0, 10)\n",
            "\n",
            "Patients data preview: \n",
            " Empty DataFrame\n",
            "Columns: [subject_id, gender, anchor_age, anchor_year, anchor_year_group, dod]\n",
            "Index: []\n",
            "Admissions data preview: \n",
            "       subject_id                                            hadm_id  \\\n",
            "10370   10202018                                           22900398   \n",
            "30048         56  E6564L68i149202683GD14E3208374440-20582-3:07S6...   \n",
            "10986   10214881                                           29025662   \n",
            "9701    10189149                                           25953331   \n",
            "5975    10118667                                           23758359   \n",
            "\n",
            "                 admittime                                 dischtime  \\\n",
            "10370  2163-02-14 12:15:00                       2163-02-16 14:25:00   \n",
            "30048                  :54  M6PP3684710573WW368IlO 7A488053U58962006   \n",
            "10986  2129-05-27 07:15:00                       2129-05-30 18:45:00   \n",
            "9701   2165-06-27 05:17:00                       2165-06-28 13:40:00   \n",
            "5975   2135-09-11 16:09:00                       2135-09-19 18:16:00   \n",
            "\n",
            "                                        deathtime  \\\n",
            "10370                                         NaN   \n",
            "30048  1 7858528SK127756EO8015759921I5857V6r57V61   \n",
            "10986                                         NaN   \n",
            "9701                                          NaN   \n",
            "5975                                          NaN   \n",
            "\n",
            "                                        admission_type admit_provider_id  \\\n",
            "10370                                         EW EMER.            P301YN   \n",
            "30048  577OR58530B9WR24G01582-3:07S6304513756H2Q705MCL               :54   \n",
            "10986                      SURGICAL SAME DAY ADMISSION            P78P8W   \n",
            "9701                                      DIRECT EMER.            P39X44   \n",
            "5975                                            URGENT            P32V6M   \n",
            "\n",
            "                                      admission_location discharge_location  \\\n",
            "10370                                     EMERGENCY ROOM   HOME HEALTH CARE   \n",
            "30048  M6PP368471057363444675034I45I570383T054675L34I...                NaN   \n",
            "10986                                 PHYSICIAN REFERRAL               HOME   \n",
            "9701                                  PHYSICIAN REFERRAL               HOME   \n",
            "5975                              TRANSFER FROM HOSPITAL               HOME   \n",
            "\n",
            "      insurance      language marital_status                          race  \\\n",
            "10370  Medicare       English        MARRIED                         WHITE   \n",
            "30048       NaN           NaN            NaN                           NaN   \n",
            "10986     Other       Spanish         SINGLE  HISPANIC/LATINO - SALVADORAN   \n",
            "9701   Medicare  Kabuverdianu        MARRIED            BLACK/CAPE VERDEAN   \n",
            "5975   Medicare       English         SINGLE                         WHITE   \n",
            "\n",
            "                 edregtime            edouttime hospital_expire_flag  \n",
            "10370  2163-02-14 07:02:00  2163-02-14 11:01:00                    0  \n",
            "30048                  NaN                  NaN                  NaN  \n",
            "10986                  NaN                  NaN                    0  \n",
            "9701   2165-06-26 17:58:00  2165-06-27 06:24:00                    0  \n",
            "5975   2135-09-11 03:36:00  2135-09-11 18:47:00                    0  \n",
            "Diagnoses data preview: \n",
            " Empty DataFrame\n",
            "Columns: [subject_id, hadm_id, seq_num, icd_code, icd_version]\n",
            "Index: []\n",
            "d_icd_diagnoses data preview: \n",
            "   icd_code  icd_version                             long_title\n",
            "0     0010            9         Cholera due to vibrio cholerae\n",
            "1     0011            9  Cholera due to vibrio cholerae el tor\n",
            "2     0019            9                   Cholera, unspecified\n",
            "3     0020            9                          Typhoid fever\n",
            "4     0021            9                    Paratyphoid fever A\n",
            "d_icd_procedures data preview: \n",
            "   icd_code  icd_version                                         long_title\n",
            "0     0001            9  Therapeutic ultrasound of vessels of head and ...\n",
            "1     0002            9                    Therapeutic ultrasound of heart\n",
            "2     0003            9  Therapeutic ultrasound of peripheral vascular ...\n",
            "3     0009            9                       Other therapeutic ultrasound\n",
            "4      001           10  Central Nervous System and Cranial Nerves, Bypass\n",
            "Procedures data preview: \n",
            " Empty DataFrame\n",
            "Columns: [subject_id, hadm_id, seq_num, chartdate, icd_code, icd_version]\n",
            "Index: []\n",
            "Symptoms data preview: \n",
            " Empty DataFrame\n",
            "Columns: [note_id, subject_id, hadm_id, Symptoms, allergies, discharge_diagnosis, discharge_disposition, primary_diagnosis, complaint, major_procedure]\n",
            "Index: []\n",
            "Patients missing values:  subject_id           0\n",
            "gender               0\n",
            "anchor_age           0\n",
            "anchor_year          0\n",
            "anchor_year_group    0\n",
            "dod                  0\n",
            "dtype: int64\n",
            "Admissions missing values:  subject_id                 8\n",
            "hadm_id                  103\n",
            "admittime                181\n",
            "dischtime                234\n",
            "deathtime               9572\n",
            "admission_type           320\n",
            "admit_provider_id        346\n",
            "admission_location       377\n",
            "discharge_location      3020\n",
            "insurance                591\n",
            "language                 465\n",
            "marital_status           688\n",
            "race                     466\n",
            "edregtime               3455\n",
            "edouttime               3467\n",
            "hospital_expire_flag     495\n",
            "dtype: int64\n",
            "Diagnoses missing values:  subject_id     0\n",
            "hadm_id        0\n",
            "seq_num        0\n",
            "icd_code       0\n",
            "icd_version    0\n",
            "dtype: int64\n",
            "Starting preprocessing...\n",
            "Initial shapes - Patients: (0, 6), Admissions: (10000, 16), Diagnoses: (0, 5)\n",
            "Created base transaction dataset with 0 rows and 8 columns\n",
            "Columns in transactions_base: ['hadm_id', 'subject_id', 'anchor_age', 'gender', 'age_category', 'primary_diagnosis_code', 'primary_diagnosis', 'Symptoms']\n",
            "Transactions base preview: \n",
            "Empty DataFrame\n",
            "Columns: [hadm_id, subject_id, anchor_age, gender, age_category, primary_diagnosis_code, primary_diagnosis, Symptoms]\n",
            "Index: []\n",
            "Starting feature engineering...\n",
            "Max features count set to 1000\n",
            "Using 0 common procedures for feature engineering out of 0 total procedures\n",
            "Filtered procedures dataset shape: (0, 7)\n",
            "Filtered procedures dataset preview: \n",
            "Empty DataFrame\n",
            "Columns: [subject_id, hadm_id, seq_num, chartdate, icd_code, icd_version, long_title]\n",
            "Index: []\n",
            "WARNING: No procedures match the frequency threshold. Reducing threshold.\n",
            "Using 0 procedures with reduced threshold\n",
            "WARNING: No procedures found after filtering. Skipping procedure feature engineering.\n",
            "Created demographic features with shape: (0, 0)\n",
            "Demographic features preview: \n",
            "Empty DataFrame\n",
            "Columns: []\n",
            "Index: []\n",
            "Processing symptom features...\n",
            "Created symptom features with shape: (0, 0)\n",
            "Symptoms preview:\n",
            "Empty DataFrame\n",
            "Columns: []\n",
            "Index: []\n",
            "WARNING: Procedures dataframe is empty.\n",
            "WARNING: Demographics dataframe is empty.\n",
            "WARNING: Symptoms dataframe is empty.\n",
            "WARNING: The following dataframes were empty and not included in the final dataset: Procedures, Demographics, Symptoms\n",
            "Using 0 most common diagnoses out of 0 total\n",
            "Filtered diagnoses dataset shape: (0, 8)\n",
            "Filtered diagnoses dataset preview: \n",
            "Empty DataFrame\n",
            "Columns: [hadm_id, subject_id, anchor_age, gender, age_category, primary_diagnosis_code, primary_diagnosis, Symptoms]\n",
            "Index: []\n",
            "ERROR: No diagnoses meet the frequency threshold. Unable to create meaningful rules.\n",
            "Please check your data or reduce the threshold further.\n",
            "Starting association rule mining...\n",
            "Found too few itemsets (0). Reducing support from 0.0006 to 0.0003\n",
            "No frequent itemsets found. Cannot generate rules.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/mlxtend/frequent_patterns/fpcommon.py:34: RuntimeWarning: invalid value encountered in divide\n",
            "  np.nansum(df.values, axis=0)\n",
            "/usr/local/lib/python3.11/dist-packages/mlxtend/frequent_patterns/fpcommon.py:34: RuntimeWarning: invalid value encountered in divide\n",
            "  np.nansum(df.values, axis=0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "files = [\n",
        "    \"patients.csv\",\n",
        "    \"admissions.csv\",\n",
        "    \"diagnoses.csv\",\n",
        "    \"d_icd_procedures.csv\",\n",
        "    \"Notes.csv\"\n",
        "]\n",
        "\n",
        "for file in files:\n",
        "    try:\n",
        "        df = pd.read_csv(file)\n",
        "        print(f\"✅ {file} loaded successfully, shape: {df.shape}\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ {file} failed: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5R5RkOg0ARmS",
        "outputId": "77be16ee-8502-41c0-abe9-65c5db7a3f49"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ patients.csv loaded successfully, shape: (364627, 6)\n",
            "✅ admissions.csv loaded successfully, shape: (30281, 16)\n",
            "❌ diagnoses.csv failed: [Errno 2] No such file or directory: 'diagnoses.csv'\n",
            "✅ d_icd_procedures.csv loaded successfully, shape: (86423, 3)\n",
            "✅ Notes.csv loaded successfully, shape: (331793, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "admissions = pd.read_csv(\"admissions.csv\", on_bad_lines='skip')  # pandas ≥ 1.3\n"
      ],
      "metadata": {
        "id": "b1Pol-W3Ac8E"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"admissions.csv\") as f:\n",
        "    lines = f.readlines()\n",
        "    print(lines[28694:28700])  # 0-индексация\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RSyZOSwnAkMB",
        "outputId": "0df48549-5e35-4909-d9e2-90c6de5ad22b"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['10550646,21218420,2124-11-07 00:32:00,2124-11-11 13:46:00,,OBSERVATION ADMIT,P045WF,EMERGENCY ROOM,HOME,00,EW EMER.,P71XWZ,EMERGENCY ROOM,DIED,Medicare,English,WIDOWED2,Engle,English,WIDOWED2M93QMB,PHYSICIAN R293QMB0NCY ROOM,HOME,00,EWEnglROOM,DIED,Medi,ELECT4.,P21DYB,WA700,24067e,Engli-04-2450:00,LK-IN/SE-04-2450SICIAN REDURE SITE,,Medicare,E0YD972-06-301te,Engli6,English,WDIRECT EM021glish,WDIRECT EM021gl,2112-04-06 15:59:00,2112-04-06 17:15:00,0\\n', '7,DIED,Med5:59:00,ED,BLAALTH CARE,M3-IN/SELF4BSERVATI125-07-07 08:34:00,0\\n', '10550646e,EnglINGLE,WHITE,2168-09-03 17:44:00,2168-09-03 18:31:00,0IED,WHITE,,,0\\n', '1055IAN REF150641,22663e,English,WIDME,M80,2English6-10-E-04-245A3:37:001OYKILLED NURSING FACILITY,SKILLED NURSING FACED,WHITE,,,0\\n', '1055IAN REF150L,HO4259852AN REF2W8,PHYSICIAN2AN RE-28 1735SXX,PH-12 11:4583134-01-07 15:31:00,0\\n', '10549659,20114661,2123-12-08 00:53:00,2123-122 21:21:0141-06-70,206h6-10-,2146-0N2AN RE-2810-,212Y ROOM,SKIL 08:34:00,0\\n']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Прочитать admissions.csv, пропуская строки с ошибками\n",
        "admissions = pd.read_csv(\"admissions.csv\", on_bad_lines='skip')\n",
        "\n",
        "# Пересохранить \"чистую\" версию\n",
        "admissions.to_csv(\"admissions_clean.csv\", index=False)\n",
        "\n",
        "print(\"✅ admissions_clean.csv создан без битых строк.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Itxpi_DLAmwp",
        "outputId": "2a10d60a-88bb-43ce-850b-43e292ede180"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ admissions_clean.csv создан без битых строк.\n"
          ]
        }
      ]
    }
  ]
}