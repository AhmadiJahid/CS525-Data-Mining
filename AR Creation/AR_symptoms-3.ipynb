{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "d6P4uZga2OjM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "cb37b10c-985c-4dd4-e7b5-c55e60ee6056"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "unexpected indent (<ipython-input-64-e85b3139fd04>, line 484)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-64-e85b3139fd04>\"\u001b[0;36m, line \u001b[0;32m484\u001b[0m\n\u001b[0;31m    def format_procedures(proc_list):\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
          ]
        }
      ],
      "source": [
        "# Main function remains largely the same, but with updated imports and function calls\n",
        "def main(sample_fraction=0.1, min_support=0.0006, min_confidence=0.5, visualize=True, generate_report=True):\n",
        "    \"\"\"\n",
        "    Main function to run the full pipeline.\n",
        "\n",
        "    Args:\n",
        "        sample_fraction (float): Fraction of data to sample\n",
        "        min_support (float): Minimum support threshold for FP-Growth\n",
        "        min_confidence (float): Minimum confidence threshold for rules\n",
        "        visualize (bool): Whether to create visualizations\n",
        "        generate_report (bool): Whether to generate HTML report\n",
        "\n",
        "    Returns:\n",
        "        tuple: (frequent_itemsets, rules, procedure_rules)\n",
        "    \"\"\"\n",
        "    # 1. Load data\n",
        "    patients, admissions, diagnoses, d_icd_diagnoses, d_icd_procedures, procedures, symptoms = load_data(sample_fraction)\n",
        "\n",
        "    # 2. Preprocess data\n",
        "    transactions_base, diagnoses_with_desc = preprocess_data(patients, admissions, diagnoses, d_icd_diagnoses, symptoms)\n",
        "\n",
        "    # 3. Check if the transaction_matrix.csv already exists\n",
        "    if os.path.exists('output/transaction_matrix.csv'):\n",
        "        print(\"Loading existing transaction matrix...\")\n",
        "        transactions_matrix = pd.read_csv('output/transaction_matrix.csv', index_col=0)\n",
        "\n",
        "        # Create human-readable versions if they don't exist yet\n",
        "        if not os.path.exists('output/detailed_transaction_matrix.csv'):\n",
        "            print(\"Creating human-readable transaction matrices...\")\n",
        "            create_readable_transaction_matrix(transactions_matrix)\n",
        "            #create_detailed_transaction_matrix(transactions_matrix, transactions_base, diagnoses_with_desc, procedures)\n",
        "    else:\n",
        "        # 4. Engineer features\n",
        "        transactions_matrix = engineer_features(transactions_base, procedures, d_icd_procedures, diagnoses_with_desc, symptoms)\n",
        "\n",
        "    # 5. Mine association rules\n",
        "    frequent_itemsets, rules, procedure_rules, diagnosis_to_procedure_rules = mine_association_rules(transactions_matrix, min_support, min_confidence)\n",
        "\n",
        "    # 6. Create visualizations if requested and available\n",
        "    if visualize and visualization_available:\n",
        "        print(\"\\nCreating visualizations...\")\n",
        "\n",
        "        # Visualize feature distribution in transaction matrix\n",
        "        visualize_feature_distribution(transactions_matrix, save_path='output/feature_distribution.png')\n",
        "\n",
        "        # Visualize rule metrics\n",
        "        if not rules.empty:\n",
        "            visualize_rule_metrics(rules, save_path='output/rule_metrics.png')\n",
        "            visualize_rules_summary(rules, save_path='output/rules_summary.png')\n",
        "\n",
        "        # Visualize procedure rules network\n",
        "        if not procedure_rules.empty:\n",
        "            visualize_rules_network(procedure_rules, max_rules=50, min_lift=1.0,\n",
        "                                   save_path='output/procedure_rules_network.png')\n",
        "\n",
        "    # 7. Generate HTML report if requested\n",
        "    if generate_report and visualization_available:\n",
        "        print(\"\\nGenerating HTML report...\")\n",
        "        report_path = create_html_report(transactions_matrix, rules, procedure_rules)\n",
        "        if report_path:\n",
        "            print(f\"HTML report generated at: {report_path}\")\n",
        "\n",
        "    return frequent_itemsets, rules, procedure_rules\n",
        "\n",
        "# Update main imports at the top\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from mlxtend.frequent_patterns import fpgrowth, association_rules  # Changed from apriori to fpgrowth\n",
        "import matplotlib.pyplot as plt\n",
        "import ast\n",
        "import os\n",
        "import pickle\n",
        "# Import visualization functions\n",
        "try:\n",
        "    import networkx as nx\n",
        "    from visualization import (\n",
        "        visualize_rules_network,\n",
        "        visualize_rule_metrics,\n",
        "        visualize_feature_distribution,\n",
        "        visualize_rules_summary,\n",
        "        create_html_report\n",
        "    )\n",
        "    visualization_available = True\n",
        "except ImportError:\n",
        "    print(\"Networkx not available. Network visualizations will be skipped.\")\n",
        "    visualization_available = False\n",
        "\n",
        "    # Define empty visualization functions to avoid errors\n",
        "    def visualize_rules_network(*args, **kwargs):\n",
        "        print(\"Networkx not available. Skipping network visualization.\")\n",
        "\n",
        "    def visualize_rule_metrics(*args, **kwargs):\n",
        "        print(\"Visualization functions not available. Skipping rule metrics visualization.\")\n",
        "\n",
        "    def visualize_feature_distribution(*args, **kwargs):\n",
        "        print(\"Visualization functions not available. Skipping feature distribution visualization.\")\n",
        "\n",
        "    def visualize_rules_summary(*args, **kwargs):\n",
        "        print(\"Visualization functions not available. Skipping rules summary visualization.\")\n",
        "\n",
        "    def create_html_report(*args, **kwargs):\n",
        "        print(\"Visualization functions not available. Skipping HTML report generation.\")\n",
        "        return None\n",
        "def sample_data(data, fraction=0.1, seed=42):\n",
        "    if data is None or data.empty:\n",
        "        return data\n",
        "\n",
        "    # For small datasets, use at least 1000 rows or the original size, whichever is smaller\n",
        "    min_rows = min(10000, len(data))\n",
        "\n",
        "    # Calculate how many rows to sample (at least min_rows)\n",
        "    sample_size = max(min_rows, int(len(data) * fraction))\n",
        "\n",
        "    print(\"Sample size of the data:\", sample_size)\n",
        "\n",
        "    # Sample the data with a fixed random seed for reproducibility\n",
        "    return data.sample(n=sample_size, random_state=seed)\n",
        "\n",
        "def clean_numeric_column(df, col):\n",
        "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "    df = df[df[col].notnull()]\n",
        "    df[col] = df[col].astype(int)\n",
        "    return df\n",
        "\n",
        "def load_data(sample_fraction=0.1):\n",
        "    # Paths\n",
        "    paths = {\n",
        "        'patients': 'patients.csv',\n",
        "        'admissions': 'admissions.csv',\n",
        "        'diagnoses': 'diagnoses_icd.csv',\n",
        "        'd_icd_diagnoses': 'd_icd_diagnoses.csv',\n",
        "        'd_icd_procedures': 'd_icd_procedures.csv',\n",
        "        'procedures': 'procedures_icd.csv',\n",
        "        'symptoms': 'Notes.csv',\n",
        "    }\n",
        "\n",
        "    print(f\"Loading data with sampling fraction: {sample_fraction}\")\n",
        "\n",
        "    # Load CSVs\n",
        "    patients = pd.read_csv(paths['patients'])\n",
        "    admissions = pd.read_csv(paths['admissions'])\n",
        "    diagnoses = pd.read_csv(paths['diagnoses'])\n",
        "    d_icd_diagnoses = pd.read_csv(paths['d_icd_diagnoses'])\n",
        "    d_icd_procedures = pd.read_csv(paths['d_icd_procedures'])\n",
        "    procedures = pd.read_csv(paths['procedures'])\n",
        "    symptoms = pd.read_csv(paths['symptoms'])\n",
        "\n",
        "    # Clean IDs\n",
        "    admissions = clean_numeric_column(admissions, 'hadm_id')\n",
        "    admissions = clean_numeric_column(admissions, 'subject_id')\n",
        "\n",
        "    # Sample admissions\n",
        "    if sample_fraction < 1.0:\n",
        "        admissions_sampled = sample_data(admissions, fraction=sample_fraction)\n",
        "    else:\n",
        "        admissions_sampled = admissions.copy()\n",
        "\n",
        "    # Filter other datasets by IDs\n",
        "    sampled_subject_ids = set(admissions_sampled['subject_id'])\n",
        "    sampled_hadm_ids = set(admissions_sampled['hadm_id'])\n",
        "\n",
        "    patients = patients[patients['subject_id'].isin(sampled_subject_ids)]\n",
        "    diagnoses = diagnoses[diagnoses['hadm_id'].isin(sampled_hadm_ids)]\n",
        "    procedures = procedures[procedures['hadm_id'].isin(sampled_hadm_ids)]\n",
        "    symptoms = symptoms[symptoms['hadm_id'].isin(sampled_hadm_ids)]\n",
        "\n",
        "    # Rename for clarity\n",
        "    admissions = admissions_sampled\n",
        "\n",
        "    # Print shapes\n",
        "    print(\"\\nSampled dataset shapes:\")\n",
        "    print(f\"Patients dataset shape: {patients.shape}\")\n",
        "    print(f\"Admissions dataset shape: {admissions.shape}\")\n",
        "    print(f\"Diagnoses dataset shape: {diagnoses.shape}\")\n",
        "    print(f\"Procedures dataset shape: {procedures.shape}\")\n",
        "    print(f\"Symptoms dataset shape: {symptoms.shape}\")\n",
        "\n",
        "    # Show previews\n",
        "    print(\"\\nPatients preview:\\n\", patients.head())\n",
        "    print(\"Admissions preview:\\n\", admissions.head())\n",
        "    print(\"Diagnoses preview:\\n\", diagnoses.head())\n",
        "    print(\"Procedures preview:\\n\", procedures.head())\n",
        "    print(\"Symptoms preview:\\n\", symptoms.head())\n",
        "\n",
        "    # Check for major empty datasets\n",
        "    for name, df in [('patients', patients), ('diagnoses', diagnoses), ('procedures', procedures), ('symptoms', symptoms)]:\n",
        "        if df.empty:\n",
        "            print(f\"⚠️ WARNING: {name} dataset is empty after filtering.\")\n",
        "\n",
        "    return patients, admissions, diagnoses, d_icd_diagnoses, d_icd_procedures, procedures, symptoms\n",
        "\n",
        "def create_readable_rules(rules_df, feature_mappings=None):\n",
        "    \"\"\"\n",
        "    Create a human-readable version of the rules.\n",
        "\n",
        "    Args:\n",
        "        rules_df (DataFrame): The rules DataFrame from association_rules function\n",
        "        feature_mappings (dict, optional): Mappings of feature codes to descriptions\n",
        "\n",
        "    Returns:\n",
        "        DataFrame: A DataFrame with human-readable rules\n",
        "    \"\"\"\n",
        "    print(\"Creating human-readable rules...\")\n",
        "\n",
        "    # Create a copy to avoid modifying the original\n",
        "    readable_rules = rules_df.copy()\n",
        "\n",
        "    # Function to format a single itemset to a readable string\n",
        "    def format_itemset(itemset, feature_mappings=None):\n",
        "        if isinstance(itemset, str):\n",
        "            # Parse string representation if needed\n",
        "            try:\n",
        "                itemset = ast.literal_eval(itemset)\n",
        "            except:\n",
        "                return itemset\n",
        "\n",
        "        items = []\n",
        "        for item in itemset:\n",
        "            if feature_mappings is not None:\n",
        "                # Try to map to a readable description\n",
        "                if item.startswith('Procedure_'):\n",
        "                    # Extract the code from the feature name\n",
        "                    proc_name = item.replace('Procedure_', '')\n",
        "                    # Get description if available\n",
        "                    if 'procedure_mapping' in feature_mappings:\n",
        "                        for code, desc in feature_mappings['procedure_mapping'].items():\n",
        "                            if desc == proc_name:\n",
        "                                items.append(f\"Procedure: {desc}\")\n",
        "                                break\n",
        "                        else:\n",
        "                            items.append(f\"Procedure: {proc_name}\")\n",
        "                    else:\n",
        "                        items.append(f\"Procedure: {proc_name}\")\n",
        "                elif item.startswith('Diagnosis_'):\n",
        "                    # Extract the code from the feature name\n",
        "                    diag_name = item.replace('Diagnosis_', '')\n",
        "                    # Get description if available\n",
        "                    if 'diagnosis_mapping' in feature_mappings:\n",
        "                        for code, desc in feature_mappings['diagnosis_mapping'].items():\n",
        "                            if desc == diag_name:\n",
        "                                items.append(f\"Diagnosis: {desc}\")\n",
        "                                break\n",
        "                        else:\n",
        "                            items.append(f\"Diagnosis: {diag_name}\")\n",
        "                    else:\n",
        "                        items.append(f\"Diagnosis: {diag_name}\")\n",
        "                elif item.startswith('Gender_'):\n",
        "                    items.append(f\"Gender: {item.replace('Gender_', '')}\")\n",
        "                elif item.startswith('Age_'):\n",
        "                    items.append(f\"Age Category: {item.replace('Age_', '')}\")\n",
        "                else:\n",
        "                    items.append(item)\n",
        "            else:\n",
        "                # Just clean up the feature name a bit\n",
        "                if item.startswith('Procedure_'):\n",
        "                    items.append(f\"Procedure: {item.replace('Procedure_', '')}\")\n",
        "                elif item.startswith('Diagnosis_'):\n",
        "                    items.append(f\"Diagnosis: {item.replace('Diagnosis_', '')}\")\n",
        "                elif item.startswith('Gender_'):\n",
        "                    items.append(f\"Gender: {item.replace('Gender_', '')}\")\n",
        "                elif item.startswith('Age_'):\n",
        "                    items.append(f\"Age Category: {item.replace('Age_', '')}\")\n",
        "                else:\n",
        "                    items.append(item)\n",
        "\n",
        "        return items\n",
        "\n",
        "    # Format antecedents and consequents\n",
        "    readable_rules['readable_antecedents'] = readable_rules['antecedents'].apply(\n",
        "        lambda x: format_itemset(x, feature_mappings)\n",
        "    )\n",
        "\n",
        "    readable_rules['readable_consequents'] = readable_rules['consequents'].apply(\n",
        "        lambda x: format_itemset(x, feature_mappings)\n",
        "    )\n",
        "\n",
        "    # Create rule strings\n",
        "    readable_rules['rule_string'] = readable_rules.apply(\n",
        "        lambda x: f\"{x['readable_antecedents']} => {x['readable_consequents']} \"\n",
        "                 f\"(Support: {x['support']:.3f}, Confidence: {x['confidence']:.3f}, Lift: {x['lift']:.3f})\",\n",
        "        axis=1\n",
        "    )\n",
        "\n",
        "    # Save to CSV\n",
        "    os.makedirs('output', exist_ok=True)\n",
        "    readable_rules[['rule_string', 'support', 'confidence', 'lift', 'readable_antecedents', 'readable_consequents']].to_csv('output/readable_rules.csv', index=False)\n",
        "\n",
        "    print(f\"Saved {len(readable_rules)} human-readable rules\")\n",
        "\n",
        "    return readable_rules\n",
        "def preprocess_data(patients, admissions, diagnoses, d_icd_diagnoses, symptoms):\n",
        "    def categorize_age(age):\n",
        "        if age < 18:\n",
        "            return 'Child'\n",
        "        elif age < 30:\n",
        "            return 'Young_Adult'\n",
        "        elif age < 50:\n",
        "            return 'Adult'\n",
        "        elif age < 70:\n",
        "            return 'Middle_Aged'\n",
        "        else:\n",
        "            return 'Elderly'\n",
        "\n",
        "    print(\"Starting preprocessing...\")\n",
        "    print(f\"Initial shapes - Patients: {patients.shape}, Admissions: {admissions.shape}, Diagnoses: {diagnoses.shape}\")\n",
        "\n",
        "    # 1. Handle missing values in patients\n",
        "    # No action needed as only dod is missing which is expected\n",
        "\n",
        "    #Handle missing values in admissions\n",
        "    essential_columns = ['hadm_id', 'subject_id']\n",
        "    admissions_subset = admissions[essential_columns].copy()\n",
        "\n",
        "    #admissions_subset[\"discharge_location\"] = admissions_subset[\"discharge_location\"].fillna(\"Unknown\")\n",
        "\n",
        "    #print(f\"Missing values in discharge_location after handling: {admissions_subset['discharge_location'].isna().sum()}\")\n",
        "\n",
        "    patients[\"age_category\"] = patients[\"anchor_age\"].apply(categorize_age)\n",
        "\n",
        "    #Merge diagnoses with description\n",
        "    diagnoses_with_desc= pd.merge(diagnoses, d_icd_diagnoses, how='left', left_on=[\"icd_code\", \"icd_version\"], right_on=[\"icd_code\", \"icd_version\"])\n",
        "    missing_desc = diagnoses_with_desc[diagnoses_with_desc[\"long_title\"].isnull()]\n",
        "\n",
        "    if len(missing_desc) > 0:\n",
        "        print(f\"WARNING: {len(missing_desc)} diagnosis codes have no description in the dictionary\")\n",
        "        #fill missing descriptions with code itself\n",
        "        diagnoses_with_desc[\"long_title\"] =  diagnoses_with_desc[\"long_title\"].fillna(\"Unlabeled_\" + diagnoses_with_desc[\"icd_code\"].astype(str))\n",
        "\n",
        "    #Merde patients with admissions (only essential columns)\n",
        "\n",
        "    patient_admissions = pd.merge(admissions_subset, patients[[\"subject_id\", \"anchor_age\", \"gender\", \"age_category\"]], how='left', on=\"subject_id\")\n",
        "\n",
        "    #get primary diagnosis\n",
        "\n",
        "    primary_diagnosis = diagnoses_with_desc[diagnoses_with_desc[\"seq_num\"] == 1].copy()\n",
        "\n",
        "    #create base transaction dataset\n",
        "    transactions_base = pd.merge(patient_admissions, primary_diagnosis[[\"subject_id\", \"hadm_id\", \"icd_code\", \"long_title\"]], how='inner', on=[\"subject_id\", \"hadm_id\"])\n",
        "\n",
        "    transactions_base = transactions_base.rename(columns={\"long_title\": \"primary_diagnosis\", \"icd_code\": \"primary_diagnosis_code\"})\n",
        "\n",
        "    #checking for missing values\n",
        "    missing_values = transactions_base.isnull().sum()\n",
        "\n",
        "    if missing_values.sum() > 0:\n",
        "        print(\"WARNING: Missing values found in transactions_base\")\n",
        "        print(missing_values[missing_values > 0])\n",
        "    transactions_with_symptoms = pd.merge(transactions_base, symptoms,\n",
        "                                          how='left', on='hadm_id')  # or 'subject_id' based on data\n",
        "\n",
        "    if 'Symptoms' in transactions_with_symptoms.columns:\n",
        "      transactions_with_symptoms['Symptoms'] = transactions_with_symptoms['Symptoms'].fillna(\"No_Symptom\")\n",
        "      transactions_base['Symptoms'] = transactions_with_symptoms['Symptoms']\n",
        "    else:\n",
        "      transactions_base['Symptoms'] = \"No_Symptom\"\n",
        "\n",
        "    print(f\"Created base transaction dataset with {len(transactions_base)} rows and {transactions_base.shape[1]} columns\")\n",
        "    print(f\"Columns in transactions_base: {transactions_base.columns.tolist()}\")\n",
        "    print(f\"Transactions base preview: \\n{transactions_base.head()}\")\n",
        "\n",
        "    return transactions_base, diagnoses_with_desc\n",
        "\n",
        "def create_readable_transaction_matrix(transactions_matrix):\n",
        "    \"\"\"\n",
        "    Converts a one-hot encoded transaction matrix to a human-readable format.\n",
        "\n",
        "    Args:\n",
        "        transactions_matrix (DataFrame): The one-hot encoded transaction matrix\n",
        "\n",
        "    Returns:\n",
        "        DataFrame: A DataFrame where each row contains the hadm_id and a list of active features\n",
        "    \"\"\"\n",
        "    print(\"Creating human-readable transaction matrix...\")\n",
        "\n",
        "    # Create a new DataFrame to store the results\n",
        "    readable_matrix = pd.DataFrame(index=transactions_matrix.index)\n",
        "    readable_matrix['hadm_id'] = readable_matrix.index\n",
        "    readable_matrix['active_features'] = ''\n",
        "\n",
        "    # For each row, collect the names of the columns where the value is 1\n",
        "    for idx in transactions_matrix.index:\n",
        "        # Get boolean series where True indicates a 1 in the original matrix\n",
        "        active_cols = transactions_matrix.loc[idx] == 1\n",
        "\n",
        "        # Get the names of active columns\n",
        "        active_features = active_cols.index[active_cols].tolist()\n",
        "\n",
        "        # Store in the new DataFrame\n",
        "        readable_matrix.loc[idx, 'active_features'] = str(active_features)\n",
        "\n",
        "    # Split the features by category for better readability\n",
        "    readable_matrix['demographics'] = readable_matrix['active_features'].apply(\n",
        "        lambda x: [f for f in eval(x) if f.startswith(('Gender_', 'Age_')) or f == 'anchor_age']\n",
        "    )\n",
        "\n",
        "    readable_matrix['procedures'] = readable_matrix['active_features'].apply(\n",
        "        lambda x: [f for f in eval(x) if f.startswith('Procedure_')]\n",
        "    )\n",
        "\n",
        "    readable_matrix['diagnoses'] = readable_matrix['active_features'].apply(\n",
        "        lambda x: [f for f in eval(x) if f.startswith('Diagnosis_')]\n",
        "    )\n",
        "\n",
        "    # Save to CSV\n",
        "    os.makedirs('output', exist_ok=True)\n",
        "    readable_matrix.to_csv('output/readable_transaction_matrix.csv', index=False)\n",
        "\n",
        "    print(f\"Saved human-readable transaction matrix with {len(readable_matrix)} rows\")\n",
        "\n",
        "    return readable_matrix\n",
        "\n",
        "def create_detailed_transaction_matrix(transactions_matrix, transactions_base, diagnoses_with_desc, procedures_with_desc):\n",
        "    \"\"\"\n",
        "    Creates a detailed, human-readable transaction matrix with decoded feature descriptions.\n",
        "\n",
        "    Args:\n",
        "        transactions_matrix (DataFrame): The one-hot encoded transaction matrix\n",
        "        transactions_base (DataFrame): The base transactions dataframe with raw data\n",
        "        diagnoses_with_desc (DataFrame): The diagnoses dataframe with descriptions\n",
        "        procedures_with_desc (DataFrame): The procedures dataframe with descriptions\n",
        "\n",
        "    Returns:\n",
        "        DataFrame: A detailed, human-readable transaction matrix\n",
        "    \"\"\"\n",
        "    print(\"Creating detailed human-readable transaction matrix...\")\n",
        "\n",
        "    # Create a mapping of hadm_id to patient info\n",
        "    patient_info = transactions_base[['hadm_id', 'subject_id', 'anchor_age', 'gender', 'age_category', 'primary_diagnosis', 'primary_diagnosis_code']].drop_duplicates()\n",
        "    patient_info = patient_info.drop_duplicates(subset='hadm_id')\n",
        "\n",
        "    patient_info_dict = patient_info.set_index('hadm_id').to_dict('index')\n",
        "    # Create a new dataframe\n",
        "    detailed_matrix = pd.DataFrame(index=transactions_matrix.index)\n",
        "    detailed_matrix['hadm_id'] = detailed_matrix.index\n",
        "\n",
        "    # Add patient demographic information\n",
        "    detailed_matrix['subject_id'] = detailed_matrix['hadm_id'].map(lambda x: patient_info_dict.get(x, {}).get('subject_id', 'Unknown'))\n",
        "    detailed_matrix['age'] = detailed_matrix['hadm_id'].map(lambda x: patient_info_dict.get(x, {}).get('anchor_age', 'Unknown'))\n",
        "    detailed_matrix['gender'] = detailed_matrix['hadm_id'].map(lambda x: patient_info_dict.get(x, {}).get('gender', 'Unknown'))\n",
        "    detailed_matrix['age_category'] = detailed_matrix['hadm_id'].map(lambda x: patient_info_dict.get(x, {}).get('age_category', 'Unknown'))\n",
        "    detailed_matrix['primary_diagnosis'] = detailed_matrix['hadm_id'].map(lambda x: patient_info_dict.get(x, {}).get('primary_diagnosis', 'Unknown'))\n",
        "    detailed_matrix['primary_diagnosis_code'] = detailed_matrix['hadm_id'].map(lambda x: patient_info_dict.get(x, {}).get('primary_diagnosis_code', 'Unknown'))\n",
        "\n",
        "    # Create column for active procedures\n",
        "    # Get procedure columns from transaction matrix\n",
        "    procedure_columns = [col for col in transactions_matrix.columns if col.startswith('Procedure_')]\n",
        "\n",
        "    # For each admission, find which procedures are active (value = 1)\n",
        "    def get_active_procedures(hadm_id):\n",
        "        if hadm_id not in transactions_matrix.index:\n",
        "            return []\n",
        "        row = transactions_matrix.loc[hadm_id]\n",
        "        active_procs = [col for col in procedure_columns if row[col] == 1]\n",
        "        proc_names = [col.replace('Procedure_', '') for col in active_procs]\n",
        "        return proc_names\n",
        "\n",
        "    #detailed_matrix['active_procedures'] = detailed_matrix['hadm_id'].apply(get_active_procedures)\n",
        "\n",
        "# 2. Создаем словарь описаний процедур\n",
        "    if procedures_with_desc is not None and 'icd_code' in procedures_with_desc.columns and 'long_title' in procedures_with_desc.columns:\n",
        "        proc_desc_dict = dict(zip(procedures_with_desc['icd_code'], procedures_with_desc['long_title']))\n",
        "    else:\n",
        "        print(\"WARNING: procedures_with_desc is missing required columns. Procedure names will be raw.\")\n",
        "        proc_desc_dict = {}\n",
        "\n",
        "# 3. Форматируем процедуры с описанием, если оно есть\n",
        "    def format_procedures(proc_list):\n",
        "        if not proc_list:\n",
        "            return []\n",
        "        return [f\"{proc_desc_dict.get(code, code)}\" for code in proc_list]\n",
        "\n",
        "    detailed_matrix['active_procedures'] = detailed_matrix['active_procedures'].apply(format_procedures)\n",
        "\n",
        "    # Get procedure descriptions from procedures_with_desc\n",
        "    if procedures_with_desc is not None and 'long_title' in procedures_with_desc.columns:\n",
        "      proc_desc_dict = dict(zip(\n",
        "        procedures_with_desc['long_title'],\n",
        "        procedures_with_desc['long_title']\n",
        "    ))\n",
        "    else:\n",
        "       print(\"WARNING: procedures_with_desc is missing or does not contain 'long_title'. Procedure names will be raw.\")\n",
        "       proc_desc_dict = {}\n",
        "\n",
        "        def format_procedures(proc_list):\n",
        "            if not proc_list:\n",
        "                return []\n",
        "            return [f\"{proc}\" for proc in proc_list]\n",
        "\n",
        "        detailed_matrix['active_procedures'] = detailed_matrix['active_procedures'].apply(format_procedures)\n",
        "\n",
        "    # Get diagnosis columns\n",
        "    diagnosis_columns = [col for col in transactions_matrix.columns if col.startswith('Diagnosis_')]\n",
        "\n",
        "    # For each admission, find which diagnoses are active\n",
        "    def get_active_diagnoses(hadm_id):\n",
        "        # Check if hadm_id exists in transactions_matrix\n",
        "        if hadm_id not in transactions_matrix.index:\n",
        "            return []\n",
        "\n",
        "        # Get active diagnoses\n",
        "        row = transactions_matrix.loc[hadm_id]\n",
        "        active_diags = [col for col in diagnosis_columns if row[col] == 1]\n",
        "\n",
        "        # Extract diagnosis names from column names\n",
        "        diag_names = [col.replace('Diagnosis_', '') for col in active_diags]\n",
        "\n",
        "        return diag_names\n",
        "\n",
        "    detailed_matrix['active_diagnoses'] = detailed_matrix['hadm_id'].apply(get_active_diagnoses)\n",
        "\n",
        "    # Count the number of active features in each category\n",
        "    detailed_matrix['procedure_count'] = detailed_matrix['active_procedures'].apply(len)\n",
        "    detailed_matrix['diagnosis_count'] = detailed_matrix['active_diagnoses'].apply(len)\n",
        "    detailed_matrix['total_feature_count'] = detailed_matrix['procedure_count'] + detailed_matrix['diagnosis_count']\n",
        "\n",
        "    # Save to CSV\n",
        "    os.makedirs('output', exist_ok=True)\n",
        "    detailed_matrix.to_csv('output/detailed_transaction_matrix.csv', index=False)\n",
        "\n",
        "    print(f\"Saved detailed human-readable transaction matrix with {len(detailed_matrix)} rows\")\n",
        "\n",
        "    return detailed_matrix\n",
        "\n",
        "def engineer_features(transactions_base, procedures, d_icd_procedures, diagnoses_with_desc, symptoms):\n",
        "    print(\"Starting feature engineering...\")\n",
        "\n",
        "    try:\n",
        "        max_features_count = int(os.environ.get(\"MAX_FEATURES_COUNT\", 1000))\n",
        "        print(f\"Max features count set to {max_features_count}\")\n",
        "    except (ValueError, TypeError):\n",
        "        print(\"Invalid MAX_FEATURES_COUNT environment variable. Using default value of 1000.\")\n",
        "        max_features_count = 1000\n",
        "\n",
        "    # 1. Merge procedures with descriptions\n",
        "    procedures_with_desc = pd.merge(procedures, d_icd_procedures, how='left', left_on=[\"icd_code\", \"icd_version\"], right_on=[\"icd_code\", \"icd_version\"])\n",
        "    missing_proc_desc = procedures_with_desc[procedures_with_desc[\"long_title\"].isnull()]\n",
        "    if len(missing_proc_desc) > 0:\n",
        "        print(f\"WARNING: {len(missing_proc_desc)} procedure codes have no description in the dictionary\")\n",
        "        procedures_with_desc[\"long_title\"] = procedures_with_desc[\"long_title\"].fillna(\"Unlabeled_\" + procedures_with_desc[\"icd_code\"].astype(str))\n",
        "\n",
        "    # Create procedure presence feature\n",
        "    procedure_counts = procedures_with_desc[\"long_title\"].value_counts()\n",
        "    min_procedure_freq = 2\n",
        "\n",
        "    common_procedures = procedure_counts[procedure_counts >= min_procedure_freq].index.tolist()\n",
        "    print(f\"Using {len(common_procedures)} common procedures for feature engineering out of {len(procedure_counts)} total procedures\")\n",
        "\n",
        "    procedures_filtered = procedures_with_desc[procedures_with_desc[\"long_title\"].isin(common_procedures)]\n",
        "\n",
        "    print(f\"Filtered procedures dataset shape: {procedures_filtered.shape}\")\n",
        "    print(f\"Filtered procedures dataset preview: \\n{procedures_filtered.head()}\")\n",
        "\n",
        "    # Checking if procedures are more than the frequency threshold\n",
        "    if len(procedures_filtered) == 0:\n",
        "        print(\"WARNING: No procedures match the frequency threshold. Reducing threshold.\")\n",
        "        min_procedure_freq = 10\n",
        "        common_procedures = procedure_counts[procedure_counts >= min_procedure_freq].index.tolist()\n",
        "        procedures_filtered = procedures_with_desc[procedures_with_desc['long_title'].isin(common_procedures)]\n",
        "        print(f\"Using {len(common_procedures)} procedures with reduced threshold\")\n",
        "\n",
        "    # Create mapping of procedure codes to descriptions for readability\n",
        "    procedure_mapping = procedures_with_desc[['icd_code', 'long_title']].drop_duplicates().set_index('icd_code')['long_title'].to_dict()\n",
        "\n",
        "    if len(procedures_filtered) > 0:\n",
        "        procedures_pivot = pd.get_dummies(procedures_filtered[[\"hadm_id\", \"long_title\"]], columns=[\"long_title\"], prefix=\"Procedure\", prefix_sep=\"_\")\n",
        "        procedures_by_admission= procedures_pivot.groupby(\"hadm_id\").max()\n",
        "        print(f\"Created procedures with admissons with shape: {procedures_by_admission.shape}\")\n",
        "    else:\n",
        "        print(\"WARNING: No procedures found after filtering. Skipping procedure feature engineering.\")\n",
        "        procedures_by_admission = pd.DataFrame(index = transactions_base[\"hadm_id\"].unique())\n",
        "\n",
        "    # Create demographic features - REMOVING anchor_age as requested\n",
        "    demographic_cols = [\"hadm_id\", \"gender\", \"age_category\"]  # Removed anchor_age\n",
        "\n",
        "    demographic_features = pd.get_dummies(transactions_base[demographic_cols], columns=[\"gender\", \"age_category\"], prefix=[\"Gender\", \"Age\"], prefix_sep=\"_\")\n",
        "\n",
        "    if \"hospital_expire_flag\" in demographic_features.columns:\n",
        "        demographic_features[\"Expired_In_Hospital\"] = demographic_features[\"hospital_expire_flag\"]\n",
        "        demographic_features = demographic_features.drop(\"hospital_expire_flag\", axis=1)\n",
        "\n",
        "    demographics_by_admission = demographic_features.groupby(\"hadm_id\").first()\n",
        "    print(f\"Created demographic features with shape: {demographics_by_admission.shape}\")\n",
        "\n",
        "    print(f\"Demographic features preview: \\n{demographics_by_admission.head()}\")\n",
        "        # === symptoms ===\n",
        "    print(\"Processing symptom features...\")\n",
        "    if 'hadm_id' not in symptoms.columns:\n",
        "        raise ValueError(\"The symptoms dataframe must contain a 'hadm_id' column to join with transactions_base.\")\n",
        "\n",
        "    if 'Symptoms' not in symptoms.columns:\n",
        "        raise ValueError(\"The symptoms dataframe must contain a 'symptom_text' column with symptom labels.\")\n",
        "\n",
        "    #deleting dublicates\n",
        "    symptoms = symptoms[['hadm_id', 'Symptoms']].dropna().drop_duplicates()\n",
        "\n",
        "    # One-hot encode: every unique symptom_text → new binary\n",
        "    symptom_dummies = pd.get_dummies(symptoms['Symptoms'], prefix='Symptom')\n",
        "    symptoms_encoded = pd.concat([symptoms[['hadm_id']].reset_index(drop=True), symptom_dummies.reset_index(drop=True)], axis=1)\n",
        "    symptoms_by_admission = symptoms_encoded.groupby('hadm_id').max()\n",
        "\n",
        "    print(f\"Created symptom features with shape: {symptoms_by_admission.shape}\")\n",
        "    print(f\"Symptoms preview:\\n{symptoms_by_admission.head()}\")\n",
        "\n",
        "\n",
        "    # Merge all features into one dataset\n",
        "    all_features = pd.DataFrame(index = transactions_base[\"hadm_id\"].unique())\n",
        "\n",
        "    # List to keep track of dataframes with potential join issues\n",
        "    empty_dfs = []\n",
        "    for name, df in [(\"Procedures\", procedures_by_admission), (\"Demographics\", demographics_by_admission), (\"Symptoms\", symptoms_by_admission)]:\n",
        "        if df.empty:\n",
        "            print(f\"WARNING: {name} dataframe is empty.\")\n",
        "            empty_dfs.append(name)\n",
        "            continue\n",
        "        before_rows = len(all_features)\n",
        "        all_features = all_features.join(df, how='left')\n",
        "        after_rows = len(all_features)\n",
        "\n",
        "        if before_rows != after_rows:\n",
        "            print(f\"WARNING: {name} dataframe caused a join issue. Rows before: {before_rows}, Rows after: {after_rows}\")\n",
        "\n",
        "    if empty_dfs:\n",
        "        print(f\"WARNING: The following dataframes were empty and not included in the final dataset: {', '.join(empty_dfs)}\")\n",
        "\n",
        "    # Fill missing values with 0\n",
        "    all_features = all_features.fillna(0)\n",
        "\n",
        "    # 8. Create diagnosis outcome features\n",
        "    diagnosis_counts = transactions_base['primary_diagnosis'].value_counts()\n",
        "\n",
        "    # Create mapping of diagnosis codes to descriptions for readability\n",
        "    diagnosis_mapping = transactions_base[['primary_diagnosis_code', 'primary_diagnosis']].drop_duplicates().set_index('primary_diagnosis_code')['primary_diagnosis'].to_dict()\n",
        "\n",
        "    # We want at least 10 diagnoses, but respect max_features budget\n",
        "    max_procedure_features = max_features_count // 2  # Reserve half for procedures, half for diagnoses\n",
        "    diagnosis_feature_limit = max(10, max_features_count - len(demographics_by_admission.columns) -\n",
        "                            min(max_procedure_features,\n",
        "                                len(procedures_by_admission.columns) if hasattr(procedures_by_admission, 'columns') else 0))\n",
        "\n",
        "    # Simply take the top N most frequent diagnoses\n",
        "    common_diagnoses = diagnosis_counts.nlargest(min(diagnosis_feature_limit, len(diagnosis_counts))).index.tolist()\n",
        "\n",
        "    print(f\"Using {len(common_diagnoses)} most common diagnoses out of {len(diagnosis_counts)} total\")\n",
        "    diagnoses_filtered = transactions_base[transactions_base['primary_diagnosis'].isin(common_diagnoses)]\n",
        "    print(f\"Filtered diagnoses dataset shape: {diagnoses_filtered.shape}\")\n",
        "    print(f\"Filtered diagnoses dataset preview: \\n{diagnoses_filtered.head()}\")\n",
        "\n",
        "    # Check if we have diagnoses left after filtering\n",
        "    if len(diagnoses_filtered) == 0:\n",
        "        print(\"ERROR: No diagnoses meet the frequency threshold. Unable to create meaningful rules.\")\n",
        "        print(\"Please check your data or reduce the threshold further.\")\n",
        "        # Return a minimal dataframe to avoid errors\n",
        "        return pd.DataFrame(columns=['no_features_available'])\n",
        "\n",
        "    diagnosis_pivot = pd.get_dummies(diagnoses_filtered[['hadm_id', 'primary_diagnosis']],\n",
        "                                    columns=['primary_diagnosis'],\n",
        "                                    prefix='Diagnosis',\n",
        "                                    prefix_sep='_')\n",
        "    diagnosis_by_admission = diagnosis_pivot.groupby('hadm_id').max()\n",
        "\n",
        "    print(f\"Created diagnosis features with shape: {diagnosis_by_admission.shape}\")\n",
        "    print(f\"Diagnosis features preview: \\n{diagnosis_by_admission.head()}\")\n",
        "\n",
        "    transactions_matrix = all_features.join(diagnosis_by_admission, how='inner')\n",
        "\n",
        "    # Printing transaction matrix shape\n",
        "    print(\"TRANSACTION MATRIX\")\n",
        "    print(f\"Transaction matrix shape after joining features and outcomes: {transactions_matrix.shape}\")\n",
        "    print(f\"Transaction matrix preview: \\n{transactions_matrix.head()}\")\n",
        "    if transactions_matrix.empty:\n",
        "        print(\"ERROR: Empty transaction matrix after joining features and outcomes.\")\n",
        "        print(\"Please check that hadm_ids are consistent across your datasets.\")\n",
        "        return pd.DataFrame(columns=['empty_transactions_matrix'])\n",
        "    if (transactions_matrix.nunique() > 2).all():\n",
        "        print(\"WARNING: No binary features found in transaction matrix. Check your data transformations.\")\n",
        "\n",
        "    # Check for excessive NaN values\n",
        "    nan_percentage = transactions_matrix.isna().mean().mean() * 100\n",
        "    if nan_percentage > 0:\n",
        "        print(f\"WARNING: Transaction matrix contains {nan_percentage:.2f}% NaN values\")\n",
        "        transactions_matrix = transactions_matrix.fillna(0)\n",
        "\n",
        "    print(f\"Final transaction matrix: {transactions_matrix.shape[0]} rows and {transactions_matrix.shape[1]} columns\")\n",
        "    print(f\"Features include {len(demographics_by_admission.columns)} demographic features, \"\n",
        "         f\"{len(procedures_by_admission.columns) if hasattr(procedures_by_admission, 'columns') else 0} procedure features, \"\n",
        "         f\"and {len(diagnosis_by_admission.columns) if hasattr(diagnosis_by_admission, 'columns') else 0} diagnosis outcomes\")\n",
        "\n",
        "    # Save the transaction matrix\n",
        "    os.makedirs('output', exist_ok=True)\n",
        "    transactions_matrix.to_csv('output/transaction_matrix.csv')\n",
        "\n",
        "    # Save mappings for later use in readability\n",
        "    mappings = {\n",
        "        'procedure_mapping': procedure_mapping,\n",
        "        'diagnosis_mapping': diagnosis_mapping\n",
        "    }\n",
        "\n",
        "    with open('output/feature_mappings.pkl', 'wb') as f:\n",
        "        pickle.dump(mappings, f)\n",
        "\n",
        "    # Create and save human-readable versions of the transaction matrix\n",
        "    # readable_matrix = create_readable_transaction_matrix(transactions_matrix)\n",
        "    # detailed_matrix = create_detailed_transaction_matrix(transactions_matrix, transactions_base, diagnoses_with_desc, procedures_with_desc)\n",
        "\n",
        "    # For debug purposes, also save feature counts\n",
        "    feature_counts = pd.Series({\n",
        "        'demographic_features': len(demographics_by_admission.columns),\n",
        "        'procedure_features': len(procedures_by_admission.columns) if hasattr(procedures_by_admission, 'columns') else 0,\n",
        "        'diagnosis_features': len(diagnosis_by_admission.columns) if hasattr(diagnosis_by_admission, 'columns') else 0,\n",
        "        'total_features': transactions_matrix.shape[1],\n",
        "        'total_transactions': transactions_matrix.shape[0]\n",
        "    })\n",
        "    feature_counts.to_csv('output/feature_counts.csv')\n",
        "\n",
        "    return transactions_matrix\n",
        "\n",
        "\n",
        "def filter_diagnosis_to_procedure_demographic_rules(rules_df, transactions_matrix):\n",
        "    \"\"\"\n",
        "    Filter rules where antecedents (LHS) are diagnoses and consequents (RHS) are\n",
        "    procedures or demographic features.\n",
        "\n",
        "    Args:\n",
        "        rules_df (DataFrame): The complete set of association rules\n",
        "        transactions_matrix (DataFrame): The transaction matrix to identify feature types\n",
        "\n",
        "    Returns:\n",
        "        DataFrame: Filtered rules\n",
        "    \"\"\"\n",
        "    if rules_df.empty:\n",
        "        print(\"No rules to filter.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    # Identify feature types from the transaction matrix\n",
        "    diagnosis_cols = [col for col in transactions_matrix.columns if col.startswith('Diagnosis_')]\n",
        "    procedure_cols = [col for col in transactions_matrix.columns if col.startswith('Procedure_')]\n",
        "    # Modify demographic cols to exclude anchor_age\n",
        "    demographic_cols = [col for col in transactions_matrix.columns\n",
        "                       if col.startswith(('Gender_', 'Age_'))]  # Removed anchor_age\n",
        "\n",
        "    # Filter rules where:\n",
        "    # 1. Antecedents (LHS) contain only diagnosis features\n",
        "    # 2. Consequents (RHS) contain only procedure or demographic features\n",
        "    filtered_rules = rules_df[rules_df.apply(\n",
        "        lambda row: (\n",
        "            # Check that all antecedents are diagnoses\n",
        "            all(item in diagnosis_cols for item in row['antecedents'])\n",
        "            and\n",
        "            # Check that all consequents are either procedures or demographics\n",
        "            all(item in procedure_cols or item in demographic_cols for item in row['consequents'])\n",
        "        ),\n",
        "        axis=1\n",
        "    )]\n",
        "\n",
        "    # Check if we found any matching rules\n",
        "    if filtered_rules.empty:\n",
        "        print(\"No rules matching the Diagnosis → Procedure/Demographic pattern.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    print(f\"Found {len(filtered_rules)} rules where diagnoses predict procedures or demographics.\")\n",
        "\n",
        "    # Sort by lift for most interesting rules first\n",
        "    filtered_rules = filtered_rules.sort_values('lift', ascending=False)\n",
        "\n",
        "    # Save the filtered rules\n",
        "    os.makedirs('output', exist_ok=True)\n",
        "    filtered_rules.to_csv('output/diagnosis_to_proc_demo_rules.csv', index=False)\n",
        "\n",
        "    # Create a human-readable version\n",
        "    try:\n",
        "        # Load the feature mappings if available\n",
        "        if os.path.exists('output/feature_mappings.pkl'):\n",
        "            with open('output/feature_mappings.pkl', 'rb') as f:\n",
        "                feature_mappings = pickle.load(f)\n",
        "\n",
        "            # Create a human-readable version of the filtered rules\n",
        "            readable_filtered_rules = create_readable_rules(filtered_rules, feature_mappings)\n",
        "            readable_filtered_rules.to_csv('output/readable_diagnosis_to_proc_demo_rules.csv', index=False)\n",
        "    except Exception as e:\n",
        "        print(f\"WARNING: Could not create readable filtered rules: {str(e)}\")\n",
        "\n",
        "    return filtered_rules\n",
        "\n",
        "def mine_association_rules(transactions_matrix, min_support=0.0006, min_confidence=0.5):\n",
        "    print(\"Starting association rule mining...\")\n",
        "\n",
        "    # Convert the DataFrame to a one-hot encoded format\n",
        "    transactions_matrix_bool = transactions_matrix.astype(bool)\n",
        "\n",
        "    min_support_floor = min_support/10\n",
        "    min_confidence_floor = min_confidence/2\n",
        "\n",
        "    # Check if we should use a sample\n",
        "    sample_size = 0\n",
        "    try:\n",
        "        sample_size = int(os.environ.get('SAMPLE_SIZE', '0'))\n",
        "    except (ValueError, TypeError):\n",
        "        sample_size = 0\n",
        "\n",
        "    if sample_size > 0 and sample_size < transactions_matrix.shape[0]:\n",
        "        print(f\"Using a sample of {sample_size} transactions\")\n",
        "        transactions_matrix_bool = transactions_matrix_bool.sample(sample_size)\n",
        "\n",
        "    # 1. Find frequent itemsets with adaptive support threshold using FP-Growth\n",
        "    # Try to find a reasonable number of itemsets\n",
        "    frequent_itemsets = pd.DataFrame()\n",
        "\n",
        "    try:\n",
        "        # Using fpgrowth instead of apriori\n",
        "        frequent_itemsets = fpgrowth(transactions_matrix_bool,\n",
        "                                   min_support=min_support,\n",
        "                                   use_colnames=True,\n",
        "                                   max_len=4)  # Limit to combinations of at most 4 items\n",
        "\n",
        "        # If we found too few itemsets, try with a lower threshold\n",
        "        if len(frequent_itemsets) < 10:\n",
        "            old_support = min_support\n",
        "            min_support = max(min_support_floor, min_support / 2)\n",
        "            print(f\"Found too few itemsets ({len(frequent_itemsets)}). Reducing support from {old_support} to {min_support}\")\n",
        "\n",
        "            frequent_itemsets = fpgrowth(transactions_matrix_bool,\n",
        "                                       min_support=min_support,\n",
        "                                       use_colnames=True,\n",
        "                                       max_len=4)\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR in FP-Growth algorithm: {str(e)}\")\n",
        "        print(\"Trying with a smaller dataset...\")\n",
        "\n",
        "        # Sample the data if it's too large\n",
        "        if transactions_matrix.shape[0] > 10000:\n",
        "            sample_size = min(10000, int(transactions_matrix.shape[0] * 0.5))\n",
        "            transactions_sample = transactions_matrix.sample(sample_size)\n",
        "            try:\n",
        "                frequent_itemsets = fpgrowth(transactions_sample.astype(bool),\n",
        "                                           min_support=min_support,\n",
        "                                           use_colnames=True,\n",
        "                                           max_len=3)\n",
        "                print(f\"Successfully ran FP-Growth on a sample of {sample_size} transactions\")\n",
        "            except Exception as e2:\n",
        "                print(f\"ERROR in FP-Growth algorithm even with sampling: {str(e2)}\")\n",
        "                return pd.DataFrame(), pd.DataFrame(), pd.DataFrame(), pd.DataFrame()\n",
        "\n",
        "    if frequent_itemsets.empty:\n",
        "        print(\"No frequent itemsets found. Cannot generate rules.\")\n",
        "        return pd.DataFrame(), pd.DataFrame(), pd.DataFrame(), pd.DataFrame()\n",
        "\n",
        "    print(f\"Found {len(frequent_itemsets)} frequent itemsets with min_support={min_support}\")\n",
        "\n",
        "    # Save frequent itemsets\n",
        "    os.makedirs('output', exist_ok=True)\n",
        "    frequent_itemsets.to_csv('output/frequent_itemsets.csv')\n",
        "\n",
        "    # 2. Generate association rules with adaptive confidence threshold\n",
        "    rules = pd.DataFrame()\n",
        "\n",
        "    try:\n",
        "        rules = association_rules(frequent_itemsets,\n",
        "                                 metric='confidence',\n",
        "                                 min_threshold=min_confidence)\n",
        "\n",
        "        # If we found too few rules, try with a lower threshold\n",
        "        if len(rules) < 10:\n",
        "            old_confidence = min_confidence\n",
        "            min_confidence = max(min_confidence_floor, min_confidence / 1.5)\n",
        "            print(f\"Found too few rules ({len(rules)}). Reducing confidence from {old_confidence} to {min_confidence}\")\n",
        "\n",
        "            rules = association_rules(frequent_itemsets,\n",
        "                                    metric='confidence',\n",
        "                                    min_threshold=min_confidence)\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR in association_rules algorithm: {str(e)}\")\n",
        "        return pd.DataFrame(), pd.DataFrame(), pd.DataFrame(), pd.DataFrame()\n",
        "\n",
        "    if rules.empty:\n",
        "        print(\"No rules generated. Cannot proceed with rule filtering.\")\n",
        "        return frequent_itemsets, pd.DataFrame(), pd.DataFrame(), pd.DataFrame()\n",
        "\n",
        "    print(f\"Generated {len(rules)} rules with min_confidence={min_confidence}\")\n",
        "\n",
        "    # Save all rules\n",
        "    rules.to_csv('output/all_rules.csv', index=False)\n",
        "\n",
        "    # 3. Filter rules to focus on procedures\n",
        "    procedure_cols = [col for col in transactions_matrix.columns if col.startswith('Procedure_')]\n",
        "    if not procedure_cols:\n",
        "        print(\"ERROR: No procedures columns found in transaction matrix\")\n",
        "        return frequent_itemsets, rules, pd.DataFrame(), pd.DataFrame()\n",
        "\n",
        "    print(f\"Found {len(procedure_cols)} procedures columns to use for rule filtering\")\n",
        "\n",
        "    # Convert string representations of sets to actual sets, if needed\n",
        "    if isinstance(rules['antecedents'].iloc[0], str):\n",
        "        rules['antecedents'] = rules['antecedents'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
        "        rules['consequents'] = rules['consequents'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
        "\n",
        "    # Filter for rules that predict procedures\n",
        "    procedure_rules = rules[rules['consequents'].apply(\n",
        "        lambda x: any(item in procedure_cols for item in x)\n",
        "    )].copy()\n",
        "\n",
        "    if procedure_rules.empty:\n",
        "        print(\"WARNING: No rules found with procedures in the consequent\")\n",
        "    else:\n",
        "        print(f\"Found {len(procedure_rules)} rules with procedures in the consequent\")\n",
        "\n",
        "        # Additional filters to focus on more interesting rules\n",
        "        if len(procedure_rules) > 1000:\n",
        "            print(f\"Too many rules ({len(procedure_rules)}). Filtering to more interesting ones...\")\n",
        "\n",
        "            # Filter by lift (stronger associations)\n",
        "            high_lift_rules = procedure_rules[procedure_rules['lift'] > 1.5]\n",
        "            if len(high_lift_rules) >= 100:\n",
        "                procedure_rules = high_lift_rules\n",
        "                print(f\"Filtered to {len(procedure_rules)} rules with lift > 1.5\")\n",
        "\n",
        "        # Sort by lift and then confidence\n",
        "        procedure_rules = procedure_rules.sort_values(['lift', 'confidence'], ascending=[False, False])\n",
        "\n",
        "        # Save procedure rules\n",
        "        procedure_rules.to_csv('output/procedure_rules.csv', index=False)\n",
        "\n",
        "    # 4. Filter for diagnosis -> procedure/demographic rules\n",
        "    diagnosis_to_proc_demo_rules = filter_diagnosis_to_procedure_demographic_rules(rules, transactions_matrix)\n",
        "\n",
        "    # 5. Create human-readable versions of the rules\n",
        "    try:\n",
        "        # Load the feature mappings if available\n",
        "        if os.path.exists('output/feature_mappings.pkl'):\n",
        "            with open('output/feature_mappings.pkl', 'rb') as f:\n",
        "                feature_mappings = pickle.load(f)\n",
        "\n",
        "            # Create human-readable versions of the rules\n",
        "            if not procedure_rules.empty:\n",
        "                readable_rules = create_readable_rules(procedure_rules, feature_mappings)\n",
        "    except Exception as e:\n",
        "        print(f\"WARNING: Could not create readable rules: {str(e)}\")\n",
        "\n",
        "    return frequent_itemsets, rules, procedure_rules, diagnosis_to_proc_demo_rules\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    import argparse\n",
        "\n",
        "    parser = argparse.ArgumentParser(description='Run the association rule mining pipeline')\n",
        "    parser.add_argument('--sample_fraction', type=float, default=0.01, help='Fraction of data to sample')\n",
        "    parser.add_argument('--min_support', type=float, default=0.0006, help='Minimum support threshold')\n",
        "    parser.add_argument('--min_confidence', type=float, default=0.5, help='Minimum confidence threshold')\n",
        "    parser.add_argument('--skip_visualizations', action='store_true', help='Skip creating visualizations')\n",
        "    parser.add_argument('--skip_report', action='store_true', help='Skip generating HTML report')\n",
        "    parser.add_argument('--read_only', action='store_true', help='Only read existing data without reprocessing')\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    print(f\"Running pipeline with:\")\n",
        "    print(f\"  - sample_fraction: {args.sample_fraction}\")\n",
        "    print(f\"  - min_support: {args.min_support}\")\n",
        "    print(f\"  - min_confidence: {args.min_confidence}\")\n",
        "    print(f\"  - visualizations: {'Disabled' if args.skip_visualizations else 'Enabled'}\")\n",
        "    print(f\"  - HTML report: {'Disabled' if args.skip_report else 'Enabled'}\")\n",
        "    print(f\"  - mode: {'Read-only' if args.read_only else 'Full processing'}\")\n",
        "\n",
        "    # If we're in read-only mode, we'll just load existing files\n",
        "    if args.read_only:\n",
        "        print(\"\\nRunning in read-only mode. Loading existing data...\")\n",
        "\n",
        "        if os.path.exists('output/transaction_matrix.csv'):\n",
        "            transactions_matrix = pd.read_csv('output/transaction_matrix.csv', index_col=0)\n",
        "            print(f\"Loaded transaction matrix with shape {transactions_matrix.shape}\")\n",
        "\n",
        "            if os.path.exists('output/all_rules.csv'):\n",
        "                rules = pd.read_csv('output/all_rules.csv')\n",
        "                print(f\"Loaded {len(rules)} rules\")\n",
        "\n",
        "                # Convert string representations of sets to actual sets for visualization\n",
        "                if 'antecedents' in rules.columns and isinstance(rules['antecedents'].iloc[0], str):\n",
        "                    rules['antecedents'] = rules['antecedents'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
        "                    rules['consequents'] = rules['consequents'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
        "            else:\n",
        "                rules = pd.DataFrame()\n",
        "                print(\"No rules file found.\")\n",
        "\n",
        "            if os.path.exists('output/procedure_rules.csv'):\n",
        "                procedure_rules = pd.read_csv('output/procedure_rules.csv')\n",
        "                print(f\"Loaded {len(procedure_rules)} procedure rules\")\n",
        "\n",
        "                # Convert string representations of sets to actual sets for visualization\n",
        "                if 'antecedents' in procedure_rules.columns and isinstance(procedure_rules['antecedents'].iloc[0], str):\n",
        "                    procedure_rules['antecedents'] = procedure_rules['antecedents'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
        "                    procedure_rules['consequents'] = procedure_rules['consequents'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
        "            else:\n",
        "                procedure_rules = pd.DataFrame()\n",
        "                print(\"No procedure rules file found.\")\n",
        "\n",
        "            # Create visualizations if requested\n",
        "            if not args.skip_visualizations and visualization_available:\n",
        "                print(\"\\nCreating visualizations from existing data...\")\n",
        "\n",
        "                # Visualize feature distribution in transaction matrix\n",
        "                visualize_feature_distribution(transactions_matrix, save_path='output/feature_distribution.png')\n",
        "\n",
        "                # Visualize rule metrics\n",
        "                if not rules.empty:\n",
        "                    visualize_rule_metrics(rules, save_path='output/rule_metrics.png')\n",
        "                    visualize_rules_summary(rules, save_path='output/rules_summary.png')\n",
        "\n",
        "                # Visualize procedure rules network\n",
        "                if not procedure_rules.empty:\n",
        "                    visualize_rules_network(procedure_rules, max_rules=50, min_lift=1.0,\n",
        "                                          save_path='output/procedure_rules_network.png')\n",
        "        else:\n",
        "            print(\"ERROR: No transaction matrix file found. Cannot proceed in read-only mode.\")\n",
        "    else:\n",
        "        # Run the full pipeline\n",
        "        frequent_itemsets, rules, procedure_rules = main(\n",
        "            args.sample_fraction,\n",
        "            args.min_support,\n",
        "            args.min_confidence,\n",
        "            not args.skip_visualizations,\n",
        "            not args.skip_report\n",
        "        )\n",
        "\n",
        "        print(\"\\nPipeline completed successfully!\")\n",
        "\n",
        "        if not procedure_rules.empty:\n",
        "            print(f\"\\nTop 5 procedure rules by lift:\")\n",
        "            print(procedure_rules.sort_values('lift', ascending=False).head(5)[['antecedents', 'consequents', 'support', 'confidence', 'lift']])\n",
        "\n",
        "            print(\"\\nCheck the output directory for detailed results and human-readable formats.\")\n",
        "        else:\n",
        "            print(\"\\nNo procedure rules were found. Try adjusting the parameters or check your data.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "frequent_itemsets, rules, procedure_rules = main(\n",
        "    sample_fraction=0.01,\n",
        "    min_support=0.0006,\n",
        "    min_confidence=0.5,\n",
        "    visualize=False,\n",
        "    generate_report=False\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "_XpWM3IY-F5c",
        "outputId": "3796314b-03e9-4bb1-b612-e5017aea2df6"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data with sampling fraction: 0.01\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-47-107afd87260f>:122: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[col] = df[col].astype(int)\n",
            "<ipython-input-47-107afd87260f>:122: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[col] = df[col].astype(int)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample size of the data: 10000\n",
            "\n",
            "Sampled dataset shapes:\n",
            "Patients dataset shape: (6212, 6)\n",
            "Admissions dataset shape: (10000, 16)\n",
            "Diagnoses dataset shape: (115916, 5)\n",
            "Procedures dataset shape: (16198, 6)\n",
            "Symptoms dataset shape: (5997, 10)\n",
            "\n",
            "Patients preview:\n",
            "     subject_id gender  anchor_age  anchor_year anchor_year_group         dod\n",
            "0     10000032      F          52         2180       2014 - 2016  2180-09-09\n",
            "4     10000084      M          72         2160       2017 - 2019  2161-02-13\n",
            "6     10000108      M          25         2163       2014 - 2016         NaN\n",
            "24    10000690      F          86         2150       2008 - 2010  2152-01-30\n",
            "27    10000826      F          32         2146       2008 - 2010         NaN\n",
            "Admissions preview:\n",
            "        subject_id   hadm_id            admittime            dischtime  \\\n",
            "28329    10544221  28259408  2125-06-07 11:41:00  2125-06-12 17:20:00   \n",
            "15318    10294586  29446464  2158-11-09 07:15:00  2158-11-12 14:45:00   \n",
            "18851    10358620  21200755  2152-02-16 00:52:00  2152-02-27 17:20:00   \n",
            "10673    10208153  22756057  2150-03-02 15:58:00  2150-03-03 12:08:00   \n",
            "16782    10318991  26872972  2163-04-24 11:53:00  2163-04-24 19:04:00   \n",
            "\n",
            "      deathtime               admission_type admit_provider_id  \\\n",
            "28329       NaN                     EW EMER.            P17UWG   \n",
            "15318       NaN  SURGICAL SAME DAY ADMISSION            P98FWS   \n",
            "18851       NaN            OBSERVATION ADMIT            P7925V   \n",
            "10673       NaN                 DIRECT EMER.            P88B7R   \n",
            "16782       NaN               EU OBSERVATION            P36VUP   \n",
            "\n",
            "          admission_location discharge_location insurance language  \\\n",
            "28329         PROCEDURE SITE               HOME  Medicaid  English   \n",
            "15318     PHYSICIAN REFERRAL               HOME   Private  English   \n",
            "18851  WALK-IN/SELF REFERRAL              REHAB   Private  English   \n",
            "10673     PHYSICIAN REFERRAL               HOME   Private  English   \n",
            "16782         EMERGENCY ROOM                NaN  Medicaid  English   \n",
            "\n",
            "      marital_status                    race            edregtime  \\\n",
            "28329        MARRIED                   OTHER                  NaN   \n",
            "15318        MARRIED  WHITE - OTHER EUROPEAN                  NaN   \n",
            "18851        MARRIED                   WHITE  2152-02-15 17:25:00   \n",
            "10673        MARRIED                   WHITE                  NaN   \n",
            "16782         SINGLE  BLACK/AFRICAN AMERICAN  2163-04-24 07:55:00   \n",
            "\n",
            "                 edouttime hospital_expire_flag  \n",
            "28329                  NaN                    0  \n",
            "15318                  NaN                    0  \n",
            "18851  2152-02-16 02:17:00                    0  \n",
            "10673                  NaN                    0  \n",
            "16782  2163-04-24 19:04:00                    0  \n",
            "Diagnoses preview:\n",
            "     subject_id   hadm_id  seq_num icd_code  icd_version\n",
            "26    10000032  29079034        1    45829            9\n",
            "27    10000032  29079034        2    07044            9\n",
            "28    10000032  29079034        3     7994            9\n",
            "29    10000032  29079034        4     2761            9\n",
            "30    10000032  29079034        5    78959            9\n",
            "Procedures preview:\n",
            "     subject_id   hadm_id  seq_num   chartdate icd_code  icd_version\n",
            "10    10000690  25860671        1  2150-11-08     3893            9\n",
            "15    10000826  21086876        1  2146-12-18     5491            9\n",
            "16    10000826  21086876        2  2146-12-24     5491            9\n",
            "17    10000826  21086876        3  2146-12-20     5491            9\n",
            "20    10000935  24955974        1  2183-11-06     8938            9\n",
            "Symptoms preview:\n",
            "            note_id  subject_id   hadm_id  \\\n",
            "2   10000032-DS-23    10000032  29079034   \n",
            "11  10000826-DS-18    10000826  21086876   \n",
            "15  10000935-DS-19    10000935  21738619   \n",
            "17  10000935-DS-21    10000935  25849114   \n",
            "18  10000980-DS-20    10000980  29654838   \n",
            "\n",
            "                                             Symptoms  \\\n",
            "2                               altered mental status   \n",
            "11  abdominal distention, back pain, fever; leukoc...   \n",
            "15                            nausea, vomiting, cough   \n",
            "17                                shortness of breath   \n",
            "18                                shortness of breath   \n",
            "\n",
            "                                            allergies  \\\n",
            "2                                  percocet / vicodin   \n",
            "11                                           tramadol   \n",
            "15  sulfa (sulfonamide antibiotics) / codeine / ba...   \n",
            "17  sulfa (sulfonamide antibiotics) / codeine / ba...   \n",
            "18        no known allergies / adverse drug reactions   \n",
            "\n",
            "                                  discharge_diagnosis discharge_disposition  \\\n",
            "2                                            primary:                  home   \n",
            "11            primary diagnosis: alcoholic hepatitis.                  home   \n",
            "15              liver and lung mets of unkown primary                  home   \n",
            "17                                           primary:     home with service   \n",
            "18  -hypertension with hypertensive urgency\\n-myoc...     home with service   \n",
            "\n",
            "   primary_diagnosis  complaint            major_procedure  \n",
            "2                NaN        NaN                       none  \n",
            "11               NaN        NaN          paracentesis x 3.  \n",
            "15               NaN        NaN                       none  \n",
            "17               NaN        NaN  percutaneous liver biopsy  \n",
            "18               NaN        NaN                       none  \n",
            "Starting preprocessing...\n",
            "Initial shapes - Patients: (6212, 6), Admissions: (10000, 16), Diagnoses: (115916, 5)\n",
            "Created base transaction dataset with 9990 rows and 8 columns\n",
            "Columns in transactions_base: ['hadm_id', 'subject_id', 'anchor_age', 'gender', 'age_category', 'primary_diagnosis_code', 'primary_diagnosis', 'Symptoms']\n",
            "Transactions base preview: \n",
            "    hadm_id  subject_id  anchor_age gender age_category  \\\n",
            "0  28259408    10544221        45.0      M        Adult   \n",
            "1  29446464    10294586        58.0      M  Middle_Aged   \n",
            "2  21200755    10358620        36.0      F        Adult   \n",
            "3  22756057    10208153        56.0      F  Middle_Aged   \n",
            "4  26872972    10318991        42.0      F        Adult   \n",
            "\n",
            "  primary_diagnosis_code                                  primary_diagnosis  \\\n",
            "0                   5739                      Unspecified disorder of liver   \n",
            "1                   2252               Benign neoplasm of cerebral meninges   \n",
            "2                   G360                       Neuromyelitis optica [Devic]   \n",
            "3                  44021  Atherosclerosis of native arteries of the extr...   \n",
            "4                  78650                            Chest pain, unspecified   \n",
            "\n",
            "                              Symptoms  \n",
            "0                                fever  \n",
            "1            right parietal meningioma  \n",
            "2                           No_Symptom  \n",
            "3  bilateral intermittent claudication  \n",
            "4                           No_Symptom  \n",
            "Loading existing transaction matrix...\n",
            "Creating human-readable transaction matrices...\n",
            "Creating human-readable transaction matrix...\n",
            "Saved human-readable transaction matrix with 6424 rows\n",
            "Creating detailed human-readable transaction matrix...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'long_title'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3805\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3806\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'long_title'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-65-d8186eb0f0a2>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m frequent_itemsets, rules, procedure_rules = main(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0msample_fraction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mmin_support\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0006\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mmin_confidence\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mvisualize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-47-107afd87260f>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(sample_fraction, min_support, min_confidence, visualize, generate_report)\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Creating human-readable transaction matrices...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mcreate_readable_transaction_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransactions_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0mcreate_detailed_transaction_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransactions_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransactions_base\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiagnoses_with_desc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocedures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;31m# 4. Engineer features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-47-107afd87260f>\u001b[0m in \u001b[0;36mcreate_detailed_transaction_matrix\u001b[0;34m(transactions_matrix, transactions_base, diagnoses_with_desc, procedures_with_desc)\u001b[0m\n\u001b[1;32m    467\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mprocedures_with_desc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mprocedures_with_desc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m         proc_desc_dict = dict(zip(\n\u001b[0;32m--> 469\u001b[0;31m             \u001b[0mprocedures_with_desc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'long_title'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    470\u001b[0m             \u001b[0mprocedures_with_desc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'long_title'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    471\u001b[0m         ))\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4101\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4102\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4103\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4104\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3810\u001b[0m             ):\n\u001b[1;32m   3811\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mInvalidIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3812\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3813\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3814\u001b[0m             \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'long_title'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!grep -R \"create_detailed_transaction_matrix\""
      ],
      "metadata": {
        "id": "BJgI4eqJ6oDs"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "files = [\n",
        "    \"patients.csv\",\n",
        "    \"admissions.csv\",\n",
        "    \"diagnoses.csv\",\n",
        "    \"d_icd_procedures.csv\",\n",
        "    \"Notes.csv\"\n",
        "]\n",
        "\n",
        "for file in files:\n",
        "    try:\n",
        "        df = pd.read_csv(file)\n",
        "        print(f\"✅ {file} loaded successfully, shape: {df.shape}\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ {file} failed: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5R5RkOg0ARmS",
        "outputId": "77be16ee-8502-41c0-abe9-65c5db7a3f49"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ patients.csv loaded successfully, shape: (364627, 6)\n",
            "✅ admissions.csv loaded successfully, shape: (30281, 16)\n",
            "❌ diagnoses.csv failed: [Errno 2] No such file or directory: 'diagnoses.csv'\n",
            "✅ d_icd_procedures.csv loaded successfully, shape: (86423, 3)\n",
            "✅ Notes.csv loaded successfully, shape: (331793, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "admissions = pd.read_csv(\"admissions.csv\", on_bad_lines='skip')  # pandas ≥ 1.3\n"
      ],
      "metadata": {
        "id": "b1Pol-W3Ac8E"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"admissions.csv\") as f:\n",
        "    lines = f.readlines()\n",
        "    print(lines[28694:28700])  # 0-индексация\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RSyZOSwnAkMB",
        "outputId": "0df48549-5e35-4909-d9e2-90c6de5ad22b"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['10550646,21218420,2124-11-07 00:32:00,2124-11-11 13:46:00,,OBSERVATION ADMIT,P045WF,EMERGENCY ROOM,HOME,00,EW EMER.,P71XWZ,EMERGENCY ROOM,DIED,Medicare,English,WIDOWED2,Engle,English,WIDOWED2M93QMB,PHYSICIAN R293QMB0NCY ROOM,HOME,00,EWEnglROOM,DIED,Medi,ELECT4.,P21DYB,WA700,24067e,Engli-04-2450:00,LK-IN/SE-04-2450SICIAN REDURE SITE,,Medicare,E0YD972-06-301te,Engli6,English,WDIRECT EM021glish,WDIRECT EM021gl,2112-04-06 15:59:00,2112-04-06 17:15:00,0\\n', '7,DIED,Med5:59:00,ED,BLAALTH CARE,M3-IN/SELF4BSERVATI125-07-07 08:34:00,0\\n', '10550646e,EnglINGLE,WHITE,2168-09-03 17:44:00,2168-09-03 18:31:00,0IED,WHITE,,,0\\n', '1055IAN REF150641,22663e,English,WIDME,M80,2English6-10-E-04-245A3:37:001OYKILLED NURSING FACILITY,SKILLED NURSING FACED,WHITE,,,0\\n', '1055IAN REF150L,HO4259852AN REF2W8,PHYSICIAN2AN RE-28 1735SXX,PH-12 11:4583134-01-07 15:31:00,0\\n', '10549659,20114661,2123-12-08 00:53:00,2123-122 21:21:0141-06-70,206h6-10-,2146-0N2AN RE-2810-,212Y ROOM,SKIL 08:34:00,0\\n']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Прочитать admissions.csv, пропуская строки с ошибками\n",
        "admissions = pd.read_csv(\"admissions.csv\", on_bad_lines='skip')\n",
        "\n",
        "# Пересохранить \"чистую\" версию\n",
        "admissions.to_csv(\"admissions_clean.csv\", index=False)\n",
        "\n",
        "print(\"✅ admissions_clean.csv создан без битых строк.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Itxpi_DLAmwp",
        "outputId": "2a10d60a-88bb-43ce-850b-43e292ede180"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ admissions_clean.csv создан без битых строк.\n"
          ]
        }
      ]
    }
  ]
}