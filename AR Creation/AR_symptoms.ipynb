{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d6P4uZga2OjM"
      },
      "outputs": [],
      "source": [
        "# Main function remains largely the same, but with updated imports and function calls\n",
        "def main(sample_fraction=0.1, min_support=0.0006, min_confidence=0.5, visualize=True, generate_report=True):\n",
        "    \"\"\"\n",
        "    Main function to run the full pipeline.\n",
        "\n",
        "    Args:\n",
        "        sample_fraction (float): Fraction of data to sample\n",
        "        min_support (float): Minimum support threshold for FP-Growth\n",
        "        min_confidence (float): Minimum confidence threshold for rules\n",
        "        visualize (bool): Whether to create visualizations\n",
        "        generate_report (bool): Whether to generate HTML report\n",
        "\n",
        "    Returns:\n",
        "        tuple: (frequent_itemsets, rules, procedure_rules)\n",
        "    \"\"\"\n",
        "    # 1. Load data\n",
        "    patients, admissions, diagnoses, d_icd_diagnoses, d_icd_procedures, procedures = load_data(sample_fraction)\n",
        "\n",
        "    # 2. Preprocess data\n",
        "    transactions_base, diagnoses_with_desc = preprocess_data(patients, admissions, diagnoses, d_icd_diagnoses, symptoms)\n",
        "\n",
        "    # 3. Check if the transaction_matrix.csv already exists\n",
        "    if os.path.exists('output/transaction_matrix.csv'):\n",
        "        print(\"Loading existing transaction matrix...\")\n",
        "        transactions_matrix = pd.read_csv('output/transaction_matrix.csv', index_col=0)\n",
        "\n",
        "        # Create human-readable versions if they don't exist yet\n",
        "        if not os.path.exists('output/detailed_transaction_matrix.csv'):\n",
        "            print(\"Creating human-readable transaction matrices...\")\n",
        "            create_readable_transaction_matrix(transactions_matrix)\n",
        "            create_detailed_transaction_matrix(transactions_matrix, transactions_base, diagnoses_with_desc, procedures)\n",
        "    else:\n",
        "        # 4. Engineer features\n",
        "        transactions_matrix = engineer_features(transactions_base, procedures, d_icd_procedures, diagnoses_with_desc, symptoms)\n",
        "\n",
        "    # 5. Mine association rules\n",
        "    frequent_itemsets, rules, procedure_rules, diagnosis_to_procedure_rules = mine_association_rules(transactions_matrix, min_support, min_confidence)\n",
        "\n",
        "    # 6. Create visualizations if requested and available\n",
        "    if visualize and visualization_available:\n",
        "        print(\"\\nCreating visualizations...\")\n",
        "\n",
        "        # Visualize feature distribution in transaction matrix\n",
        "        visualize_feature_distribution(transactions_matrix, save_path='output/feature_distribution.png')\n",
        "\n",
        "        # Visualize rule metrics\n",
        "        if not rules.empty:\n",
        "            visualize_rule_metrics(rules, save_path='output/rule_metrics.png')\n",
        "            visualize_rules_summary(rules, save_path='output/rules_summary.png')\n",
        "\n",
        "        # Visualize procedure rules network\n",
        "        if not procedure_rules.empty:\n",
        "            visualize_rules_network(procedure_rules, max_rules=50, min_lift=1.0,\n",
        "                                   save_path='output/procedure_rules_network.png')\n",
        "\n",
        "    # 7. Generate HTML report if requested\n",
        "    if generate_report and visualization_available:\n",
        "        print(\"\\nGenerating HTML report...\")\n",
        "        report_path = create_html_report(transactions_matrix, rules, procedure_rules)\n",
        "        if report_path:\n",
        "            print(f\"HTML report generated at: {report_path}\")\n",
        "\n",
        "    return frequent_itemsets, rules, procedure_rules\n",
        "\n",
        "# Update main imports at the top\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from mlxtend.frequent_patterns import fpgrowth, association_rules  # Changed from apriori to fpgrowth\n",
        "import matplotlib.pyplot as plt\n",
        "import ast\n",
        "import os\n",
        "import pickle\n",
        "# Import visualization functions\n",
        "try:\n",
        "    import networkx as nx\n",
        "    from visualization import (\n",
        "        visualize_rules_network,\n",
        "        visualize_rule_metrics,\n",
        "        visualize_feature_distribution,\n",
        "        visualize_rules_summary,\n",
        "        create_html_report\n",
        "    )\n",
        "    visualization_available = True\n",
        "except ImportError:\n",
        "    print(\"Networkx not available. Network visualizations will be skipped.\")\n",
        "    visualization_available = False\n",
        "\n",
        "    # Define empty visualization functions to avoid errors\n",
        "    def visualize_rules_network(*args, **kwargs):\n",
        "        print(\"Networkx not available. Skipping network visualization.\")\n",
        "\n",
        "    def visualize_rule_metrics(*args, **kwargs):\n",
        "        print(\"Visualization functions not available. Skipping rule metrics visualization.\")\n",
        "\n",
        "    def visualize_feature_distribution(*args, **kwargs):\n",
        "        print(\"Visualization functions not available. Skipping feature distribution visualization.\")\n",
        "\n",
        "    def visualize_rules_summary(*args, **kwargs):\n",
        "        print(\"Visualization functions not available. Skipping rules summary visualization.\")\n",
        "\n",
        "    def create_html_report(*args, **kwargs):\n",
        "        print(\"Visualization functions not available. Skipping HTML report generation.\")\n",
        "        return None\n",
        "def sample_data(data, fraction=0.1, seed=42):\n",
        "    if data is None or data.empty:\n",
        "        return data\n",
        "\n",
        "    # For small datasets, use at least 1000 rows or the original size, whichever is smaller\n",
        "    min_rows = min(10000, len(data))\n",
        "\n",
        "    # Calculate how many rows to sample (at least min_rows)\n",
        "    sample_size = max(min_rows, int(len(data) * fraction))\n",
        "\n",
        "    print(\"Sample size of the data:\", sample_size)\n",
        "\n",
        "    # Sample the data with a fixed random seed for reproducibility\n",
        "    return data.sample(n=sample_size, random_state=seed)\n",
        "\n",
        "def load_data(sample_fraction=0.1):\n",
        "    patients_path = 'AR Creation/Data/patients.csv'\n",
        "    admissions_path = 'AR Creation/Data/admissions.csv'\n",
        "    diagnoses_path = 'AR Creation/Data/diagnoses_icd.csv'\n",
        "    d_icd_diagnoses_path = 'AR Creation/Data/d_icd_diagnoses.csv'\n",
        "    d_icd_procedures_path = 'AR Creation/Data/d_icd_procedures.csv'\n",
        "    procedures_path = 'AR Creation/Data/procedures_icd.csv'\n",
        "    symptoms_path = 'AR Creation/Data/Notes.csv'\n",
        "\n",
        "    print(f\"Loading data with sampling fraction: {sample_fraction}\")\n",
        "\n",
        "    patients = pd.read_csv(patients_path)\n",
        "    admissions = pd.read_csv(admissions_path)\n",
        "    diagnoses = pd.read_csv(diagnoses_path)\n",
        "    d_icd_diagnoses = pd.read_csv(d_icd_diagnoses_path)\n",
        "    d_icd_procedures = pd.read_csv(d_icd_procedures_path)\n",
        "    procedures = pd.read_csv(procedures_path)\n",
        "    symptoms = pd.read_csv(symptoms_path)\n",
        "\n",
        "    print(\"Original dataset shapes:\")\n",
        "    print(f\"Patients dataset shape: {patients.shape}\")\n",
        "    print(f\"Admissions dataset shape: {admissions.shape}\")\n",
        "    print(f\"Diagnoses dataset shape: {diagnoses.shape}\")\n",
        "    print(f\"d_icd_diagnoses dataset shape: {d_icd_diagnoses.shape}\")\n",
        "    print(f\"d_icd_procedures dataset shape: {d_icd_procedures.shape}\")\n",
        "    print(f\"Procedures dataset shape: {procedures.shape}\")\n",
        "    print(f\"Procedures dataset shape: {symptoms.shape}\")\n",
        "\n",
        "    # Sample the main data tables that contain patient-level information\n",
        "    # We don't sample the reference tables (d_icd_*)\n",
        "    if sample_fraction < 1.0:\n",
        "        # First, sample patients\n",
        "        admissions_sampled = sample_data(admissions, fraction=sample_fraction)\n",
        "\n",
        "        # Then filter other tables to only include the sampled patients\n",
        "        sampled_subject_ids = set(admissions_sampled['subject_id'])\n",
        "        sampled_hadm_ids = set(admissions_sampled['hadm_id'])\n",
        "        patients= patients[patients['subject_id'].isin(sampled_subject_ids)]\n",
        "        diagnoses = diagnoses[diagnoses['hadm_id'].isin(sampled_hadm_ids)]\n",
        "        procedures = procedures[procedures['hadm_id'].isin(sampled_hadm_ids)]\n",
        "        symptoms = symptoms[symptoms['hadm_id'].isin(sampled_hadm_ids)]\n",
        "\n",
        "        admissions = admissions_sampled\n",
        "\n",
        "        print(\"\\nSampled dataset shapes:\")\n",
        "        print(f\"Patients dataset shape: {patients.shape}\")\n",
        "        print(f\"Admissions dataset shape: {admissions.shape}\")\n",
        "        print(f\"Diagnoses dataset shape: {diagnoses.shape}\")\n",
        "        print(f\"Procedures dataset shape: {procedures.shape}\")\n",
        "        print(f\"Symptoms dataset shape: {symptoms.shape}\")\n",
        "\n",
        "    print(\"\\nPatients data preview: \\n\", patients.head())\n",
        "    print(\"Admissions data preview: \\n\", admissions.head())\n",
        "    print(\"Diagnoses data preview: \\n\", diagnoses.head())\n",
        "    print(\"d_icd_diagnoses data preview: \\n\", d_icd_diagnoses.head())\n",
        "    print(\"d_icd_procedures data preview: \\n\", d_icd_procedures.head())\n",
        "    print(\"Procedures data preview: \\n\", procedures.head())\n",
        "    print(\"Symptoms data preview: \\n\", symptoms.head())\n",
        "    # Check for missing values\n",
        "    print(\"Patients missing values: \", patients.isnull().sum())\n",
        "    print(\"Admissions missing values: \", admissions.isnull().sum())\n",
        "    print(\"Diagnoses missing values: \", diagnoses.isnull().sum())\n",
        "\n",
        "    return patients, admissions, diagnoses, d_icd_diagnoses, d_icd_procedures, procedures\n",
        "def create_readable_rules(rules_df, feature_mappings=None):\n",
        "    \"\"\"\n",
        "    Create a human-readable version of the rules.\n",
        "\n",
        "    Args:\n",
        "        rules_df (DataFrame): The rules DataFrame from association_rules function\n",
        "        feature_mappings (dict, optional): Mappings of feature codes to descriptions\n",
        "\n",
        "    Returns:\n",
        "        DataFrame: A DataFrame with human-readable rules\n",
        "    \"\"\"\n",
        "    print(\"Creating human-readable rules...\")\n",
        "\n",
        "    # Create a copy to avoid modifying the original\n",
        "    readable_rules = rules_df.copy()\n",
        "\n",
        "    # Function to format a single itemset to a readable string\n",
        "    def format_itemset(itemset, feature_mappings=None):\n",
        "        if isinstance(itemset, str):\n",
        "            # Parse string representation if needed\n",
        "            try:\n",
        "                itemset = ast.literal_eval(itemset)\n",
        "            except:\n",
        "                return itemset\n",
        "\n",
        "        items = []\n",
        "        for item in itemset:\n",
        "            if feature_mappings is not None:\n",
        "                # Try to map to a readable description\n",
        "                if item.startswith('Procedure_'):\n",
        "                    # Extract the code from the feature name\n",
        "                    proc_name = item.replace('Procedure_', '')\n",
        "                    # Get description if available\n",
        "                    if 'procedure_mapping' in feature_mappings:\n",
        "                        for code, desc in feature_mappings['procedure_mapping'].items():\n",
        "                            if desc == proc_name:\n",
        "                                items.append(f\"Procedure: {desc}\")\n",
        "                                break\n",
        "                        else:\n",
        "                            items.append(f\"Procedure: {proc_name}\")\n",
        "                    else:\n",
        "                        items.append(f\"Procedure: {proc_name}\")\n",
        "                elif item.startswith('Diagnosis_'):\n",
        "                    # Extract the code from the feature name\n",
        "                    diag_name = item.replace('Diagnosis_', '')\n",
        "                    # Get description if available\n",
        "                    if 'diagnosis_mapping' in feature_mappings:\n",
        "                        for code, desc in feature_mappings['diagnosis_mapping'].items():\n",
        "                            if desc == diag_name:\n",
        "                                items.append(f\"Diagnosis: {desc}\")\n",
        "                                break\n",
        "                        else:\n",
        "                            items.append(f\"Diagnosis: {diag_name}\")\n",
        "                    else:\n",
        "                        items.append(f\"Diagnosis: {diag_name}\")\n",
        "                elif item.startswith('Gender_'):\n",
        "                    items.append(f\"Gender: {item.replace('Gender_', '')}\")\n",
        "                elif item.startswith('Age_'):\n",
        "                    items.append(f\"Age Category: {item.replace('Age_', '')}\")\n",
        "                else:\n",
        "                    items.append(item)\n",
        "            else:\n",
        "                # Just clean up the feature name a bit\n",
        "                if item.startswith('Procedure_'):\n",
        "                    items.append(f\"Procedure: {item.replace('Procedure_', '')}\")\n",
        "                elif item.startswith('Diagnosis_'):\n",
        "                    items.append(f\"Diagnosis: {item.replace('Diagnosis_', '')}\")\n",
        "                elif item.startswith('Gender_'):\n",
        "                    items.append(f\"Gender: {item.replace('Gender_', '')}\")\n",
        "                elif item.startswith('Age_'):\n",
        "                    items.append(f\"Age Category: {item.replace('Age_', '')}\")\n",
        "                else:\n",
        "                    items.append(item)\n",
        "\n",
        "        return items\n",
        "\n",
        "    # Format antecedents and consequents\n",
        "    readable_rules['readable_antecedents'] = readable_rules['antecedents'].apply(\n",
        "        lambda x: format_itemset(x, feature_mappings)\n",
        "    )\n",
        "\n",
        "    readable_rules['readable_consequents'] = readable_rules['consequents'].apply(\n",
        "        lambda x: format_itemset(x, feature_mappings)\n",
        "    )\n",
        "\n",
        "    # Create rule strings\n",
        "    readable_rules['rule_string'] = readable_rules.apply(\n",
        "        lambda x: f\"{x['readable_antecedents']} => {x['readable_consequents']} \"\n",
        "                 f\"(Support: {x['support']:.3f}, Confidence: {x['confidence']:.3f}, Lift: {x['lift']:.3f})\",\n",
        "        axis=1\n",
        "    )\n",
        "\n",
        "    # Save to CSV\n",
        "    os.makedirs('output', exist_ok=True)\n",
        "    readable_rules[['rule_string', 'support', 'confidence', 'lift', 'readable_antecedents', 'readable_consequents']].to_csv('output/readable_rules.csv', index=False)\n",
        "\n",
        "    print(f\"Saved {len(readable_rules)} human-readable rules\")\n",
        "\n",
        "    return readable_rules\n",
        "def preprocess_data(patients, admissions, diagnoses, d_icd_diagnoses, symptoms):\n",
        "    def categorize_age(age):\n",
        "        if age < 18:\n",
        "            return 'Child'\n",
        "        elif age < 30:\n",
        "            return 'Young_Adult'\n",
        "        elif age < 50:\n",
        "            return 'Adult'\n",
        "        elif age < 70:\n",
        "            return 'Middle_Aged'\n",
        "        else:\n",
        "            return 'Elderly'\n",
        "\n",
        "    print(\"Starting preprocessing...\")\n",
        "    print(f\"Initial shapes - Patients: {patients.shape}, Admissions: {admissions.shape}, Diagnoses: {diagnoses.shape}\")\n",
        "\n",
        "    # 1. Handle missing values in patients\n",
        "    # No action needed as only dod is missing which is expected\n",
        "\n",
        "    #Handle missing values in admissions\n",
        "    essential_columns = ['hadm_id', 'subject_id']\n",
        "    admissions_subset = admissions[essential_columns].copy()\n",
        "\n",
        "    #admissions_subset[\"discharge_location\"] = admissions_subset[\"discharge_location\"].fillna(\"Unknown\")\n",
        "\n",
        "    #print(f\"Missing values in discharge_location after handling: {admissions_subset['discharge_location'].isna().sum()}\")\n",
        "\n",
        "    patients[\"age_category\"] = patients[\"anchor_age\"].apply(categorize_age)\n",
        "\n",
        "    #Merge diagnoses with description\n",
        "    diagnoses_with_desc= pd.merge(diagnoses, d_icd_diagnoses, how='left', left_on=[\"icd_code\", \"icd_version\"], right_on=[\"icd_code\", \"icd_version\"])\n",
        "    missing_desc = diagnoses_with_desc[diagnoses_with_desc[\"long_title\"].isnull()]\n",
        "\n",
        "    if len(missing_desc) > 0:\n",
        "        print(f\"WARNING: {len(missing_desc)} diagnosis codes have no description in the dictionary\")\n",
        "        #fill missing descriptions with code itself\n",
        "        diagnoses_with_desc[\"long_title\"] =  diagnoses_with_desc[\"long_title\"].fillna(\"Unlabeled_\" + diagnoses_with_desc[\"icd_code\"].astype(str))\n",
        "\n",
        "    #Merde patients with admissions (only essential columns)\n",
        "\n",
        "    patient_admissions = pd.merge(admissions_subset, patients[[\"subject_id\", \"anchor_age\", \"gender\", \"age_category\"]], how='left', on=\"subject_id\")\n",
        "\n",
        "    #get primary diagnosis\n",
        "\n",
        "    primary_diagnosis = diagnoses_with_desc[diagnoses_with_desc[\"seq_num\"] == 1].copy()\n",
        "\n",
        "    #create base transaction dataset\n",
        "    transactions_base = pd.merge(patient_admissions, primary_diagnosis[[\"subject_id\", \"hadm_id\", \"icd_code\", \"long_title\"]], how='inner', on=[\"subject_id\", \"hadm_id\"])\n",
        "\n",
        "    transactions_base = transactions_base.rename(columns={\"long_title\": \"primary_diagnosis\", \"icd_code\": \"primary_diagnosis_code\"})\n",
        "\n",
        "    #checking for missing values\n",
        "    missing_values = transactions_base.isnull().sum()\n",
        "\n",
        "    if missing_values.sum() > 0:\n",
        "        print(\"WARNING: Missing values found in transactions_base\")\n",
        "        print(missing_values[missing_values > 0])\n",
        "    transactions_with_symptoms = pd.merge(transactions_base, symptoms,\n",
        "                                          how='left', on='hadm_id')  # or 'subject_id' based on data\n",
        "\n",
        "    transactions_base['Symptoms'] = transactions_base['Symptoms'].fillna(\"No_Symptom\")\n",
        "    print(f\"Created base transaction dataset with {len(transactions_base)} rows and {transactions_base.shape[1]} columns\")\n",
        "    print(f\"Columns in transactions_base: {transactions_base.columns.tolist()}\")\n",
        "    print(f\"Transactions base preview: \\n{transactions_base.head()}\")\n",
        "\n",
        "    return transactions_base, diagnoses_with_desc\n",
        "\n",
        "def create_readable_transaction_matrix(transactions_matrix):\n",
        "    \"\"\"\n",
        "    Converts a one-hot encoded transaction matrix to a human-readable format.\n",
        "\n",
        "    Args:\n",
        "        transactions_matrix (DataFrame): The one-hot encoded transaction matrix\n",
        "\n",
        "    Returns:\n",
        "        DataFrame: A DataFrame where each row contains the hadm_id and a list of active features\n",
        "    \"\"\"\n",
        "    print(\"Creating human-readable transaction matrix...\")\n",
        "\n",
        "    # Create a new DataFrame to store the results\n",
        "    readable_matrix = pd.DataFrame(index=transactions_matrix.index)\n",
        "    readable_matrix['hadm_id'] = readable_matrix.index\n",
        "    readable_matrix['active_features'] = ''\n",
        "\n",
        "    # For each row, collect the names of the columns where the value is 1\n",
        "    for idx in transactions_matrix.index:\n",
        "        # Get boolean series where True indicates a 1 in the original matrix\n",
        "        active_cols = transactions_matrix.loc[idx] == 1\n",
        "\n",
        "        # Get the names of active columns\n",
        "        active_features = active_cols.index[active_cols].tolist()\n",
        "\n",
        "        # Store in the new DataFrame\n",
        "        readable_matrix.loc[idx, 'active_features'] = str(active_features)\n",
        "\n",
        "    # Split the features by category for better readability\n",
        "    readable_matrix['demographics'] = readable_matrix['active_features'].apply(\n",
        "        lambda x: [f for f in eval(x) if f.startswith(('Gender_', 'Age_')) or f == 'anchor_age']\n",
        "    )\n",
        "\n",
        "    readable_matrix['procedures'] = readable_matrix['active_features'].apply(\n",
        "        lambda x: [f for f in eval(x) if f.startswith('Procedure_')]\n",
        "    )\n",
        "\n",
        "    readable_matrix['diagnoses'] = readable_matrix['active_features'].apply(\n",
        "        lambda x: [f for f in eval(x) if f.startswith('Diagnosis_')]\n",
        "    )\n",
        "\n",
        "    # Save to CSV\n",
        "    os.makedirs('output', exist_ok=True)\n",
        "    readable_matrix.to_csv('output/readable_transaction_matrix.csv', index=False)\n",
        "\n",
        "    print(f\"Saved human-readable transaction matrix with {len(readable_matrix)} rows\")\n",
        "\n",
        "    return readable_matrix\n",
        "\n",
        "def create_detailed_transaction_matrix(transactions_matrix, transactions_base, diagnoses_with_desc, procedures_with_desc):\n",
        "    \"\"\"\n",
        "    Creates a detailed, human-readable transaction matrix with decoded feature descriptions.\n",
        "\n",
        "    Args:\n",
        "        transactions_matrix (DataFrame): The one-hot encoded transaction matrix\n",
        "        transactions_base (DataFrame): The base transactions dataframe with raw data\n",
        "        diagnoses_with_desc (DataFrame): The diagnoses dataframe with descriptions\n",
        "        procedures_with_desc (DataFrame): The procedures dataframe with descriptions\n",
        "\n",
        "    Returns:\n",
        "        DataFrame: A detailed, human-readable transaction matrix\n",
        "    \"\"\"\n",
        "    print(\"Creating detailed human-readable transaction matrix...\")\n",
        "\n",
        "    # Create a mapping of hadm_id to patient info\n",
        "    patient_info = transactions_base[['hadm_id', 'subject_id', 'anchor_age', 'gender', 'age_category', 'primary_diagnosis', 'primary_diagnosis_code']].drop_duplicates()\n",
        "    patient_info_dict = patient_info.set_index('hadm_id').to_dict('index')\n",
        "\n",
        "    # Create a new dataframe\n",
        "    detailed_matrix = pd.DataFrame(index=transactions_matrix.index)\n",
        "    detailed_matrix['hadm_id'] = detailed_matrix.index\n",
        "\n",
        "    # Add patient demographic information\n",
        "    detailed_matrix['subject_id'] = detailed_matrix['hadm_id'].map(lambda x: patient_info_dict.get(x, {}).get('subject_id', 'Unknown'))\n",
        "    detailed_matrix['age'] = detailed_matrix['hadm_id'].map(lambda x: patient_info_dict.get(x, {}).get('anchor_age', 'Unknown'))\n",
        "    detailed_matrix['gender'] = detailed_matrix['hadm_id'].map(lambda x: patient_info_dict.get(x, {}).get('gender', 'Unknown'))\n",
        "    detailed_matrix['age_category'] = detailed_matrix['hadm_id'].map(lambda x: patient_info_dict.get(x, {}).get('age_category', 'Unknown'))\n",
        "    detailed_matrix['primary_diagnosis'] = detailed_matrix['hadm_id'].map(lambda x: patient_info_dict.get(x, {}).get('primary_diagnosis', 'Unknown'))\n",
        "    detailed_matrix['primary_diagnosis_code'] = detailed_matrix['hadm_id'].map(lambda x: patient_info_dict.get(x, {}).get('primary_diagnosis_code', 'Unknown'))\n",
        "\n",
        "    # Create column for active procedures\n",
        "    # Get procedure columns from transaction matrix\n",
        "    procedure_columns = [col for col in transactions_matrix.columns if col.startswith('Procedure_')]\n",
        "\n",
        "    # For each admission, find which procedures are active (value = 1)\n",
        "    def get_active_procedures(hadm_id):\n",
        "        # Check if hadm_id exists in transactions_matrix\n",
        "        if hadm_id not in transactions_matrix.index:\n",
        "            return []\n",
        "\n",
        "        # Get active procedures\n",
        "        row = transactions_matrix.loc[hadm_id]\n",
        "        active_procs = [col for col in procedure_columns if row[col] == 1]\n",
        "\n",
        "        # Extract procedure names from column names\n",
        "        proc_names = [col.replace('Procedure_', '') for col in active_procs]\n",
        "\n",
        "        return proc_names\n",
        "\n",
        "    # Map procedure ICD codes to descriptions if available\n",
        "    detailed_matrix['active_procedures'] = detailed_matrix['hadm_id'].apply(get_active_procedures)\n",
        "\n",
        "    # Get procedure descriptions from procedures_with_desc\n",
        "    if procedures_with_desc is not None and not procedures_with_desc.empty:\n",
        "        proc_desc_dict = dict(zip(\n",
        "            procedures_with_desc['long_title'],\n",
        "            procedures_with_desc['long_title']\n",
        "        ))\n",
        "\n",
        "        def format_procedures(proc_list):\n",
        "            if not proc_list:\n",
        "                return []\n",
        "            return [f\"{proc}\" for proc in proc_list]\n",
        "\n",
        "        detailed_matrix['active_procedures'] = detailed_matrix['active_procedures'].apply(format_procedures)\n",
        "\n",
        "    # Get diagnosis columns\n",
        "    diagnosis_columns = [col for col in transactions_matrix.columns if col.startswith('Diagnosis_')]\n",
        "\n",
        "    # For each admission, find which diagnoses are active\n",
        "    def get_active_diagnoses(hadm_id):\n",
        "        # Check if hadm_id exists in transactions_matrix\n",
        "        if hadm_id not in transactions_matrix.index:\n",
        "            return []\n",
        "\n",
        "        # Get active diagnoses\n",
        "        row = transactions_matrix.loc[hadm_id]\n",
        "        active_diags = [col for col in diagnosis_columns if row[col] == 1]\n",
        "\n",
        "        # Extract diagnosis names from column names\n",
        "        diag_names = [col.replace('Diagnosis_', '') for col in active_diags]\n",
        "\n",
        "        return diag_names\n",
        "\n",
        "    detailed_matrix['active_diagnoses'] = detailed_matrix['hadm_id'].apply(get_active_diagnoses)\n",
        "\n",
        "    # Count the number of active features in each category\n",
        "    detailed_matrix['procedure_count'] = detailed_matrix['active_procedures'].apply(len)\n",
        "    detailed_matrix['diagnosis_count'] = detailed_matrix['active_diagnoses'].apply(len)\n",
        "    detailed_matrix['total_feature_count'] = detailed_matrix['procedure_count'] + detailed_matrix['diagnosis_count']\n",
        "\n",
        "    # Save to CSV\n",
        "    os.makedirs('output', exist_ok=True)\n",
        "    detailed_matrix.to_csv('output/detailed_transaction_matrix.csv', index=False)\n",
        "\n",
        "    print(f\"Saved detailed human-readable transaction matrix with {len(detailed_matrix)} rows\")\n",
        "\n",
        "    return detailed_matrix\n",
        "\n",
        "def engineer_features(transactions_base, procedures, d_icd_procedures, diagnoses_with_desc, symptoms):\n",
        "    print(\"Starting feature engineering...\")\n",
        "\n",
        "    try:\n",
        "        max_features_count = int(os.environ.get(\"MAX_FEATURES_COUNT\", 1000))\n",
        "        print(f\"Max features count set to {max_features_count}\")\n",
        "    except (ValueError, TypeError):\n",
        "        print(\"Invalid MAX_FEATURES_COUNT environment variable. Using default value of 1000.\")\n",
        "        max_features_count = 1000\n",
        "\n",
        "    # 1. Merge procedures with descriptions\n",
        "    procedures_with_desc = pd.merge(procedures, d_icd_procedures, how='left', left_on=[\"icd_code\", \"icd_version\"], right_on=[\"icd_code\", \"icd_version\"])\n",
        "    missing_proc_desc = procedures_with_desc[procedures_with_desc[\"long_title\"].isnull()]\n",
        "    if len(missing_proc_desc) > 0:\n",
        "        print(f\"WARNING: {len(missing_proc_desc)} procedure codes have no description in the dictionary\")\n",
        "        procedures_with_desc[\"long_title\"] = procedures_with_desc[\"long_title\"].fillna(\"Unlabeled_\" + procedures_with_desc[\"icd_code\"].astype(str))\n",
        "\n",
        "    # Create procedure presence feature\n",
        "    procedure_counts = procedures_with_desc[\"long_title\"].value_counts()\n",
        "    min_procedure_freq = 2\n",
        "\n",
        "    common_procedures = procedure_counts[procedure_counts >= min_procedure_freq].index.tolist()\n",
        "    print(f\"Using {len(common_procedures)} common procedures for feature engineering out of {len(procedure_counts)} total procedures\")\n",
        "\n",
        "    procedures_filtered = procedures_with_desc[procedures_with_desc[\"long_title\"].isin(common_procedures)]\n",
        "\n",
        "    print(f\"Filtered procedures dataset shape: {procedures_filtered.shape}\")\n",
        "    print(f\"Filtered procedures dataset preview: \\n{procedures_filtered.head()}\")\n",
        "\n",
        "    # Checking if procedures are more than the frequency threshold\n",
        "    if len(procedures_filtered) == 0:\n",
        "        print(\"WARNING: No procedures match the frequency threshold. Reducing threshold.\")\n",
        "        min_procedure_freq = 10\n",
        "        common_procedures = procedure_counts[procedure_counts >= min_procedure_freq].index.tolist()\n",
        "        procedures_filtered = procedures_with_desc[procedures_with_desc['long_title'].isin(common_procedures)]\n",
        "        print(f\"Using {len(common_procedures)} procedures with reduced threshold\")\n",
        "\n",
        "    # Create mapping of procedure codes to descriptions for readability\n",
        "    procedure_mapping = procedures_with_desc[['icd_code', 'long_title']].drop_duplicates().set_index('icd_code')['long_title'].to_dict()\n",
        "\n",
        "    if len(procedures_filtered) > 0:\n",
        "        procedures_pivot = pd.get_dummies(procedures_filtered[[\"hadm_id\", \"long_title\"]], columns=[\"long_title\"], prefix=\"Procedure\", prefix_sep=\"_\")\n",
        "        procedures_by_admission= procedures_pivot.groupby(\"hadm_id\").max()\n",
        "        print(f\"Created procedures with admissons with shape: {procedures_by_admission.shape}\")\n",
        "    else:\n",
        "        print(\"WARNING: No procedures found after filtering. Skipping procedure feature engineering.\")\n",
        "        procedures_by_admission = pd.DataFrame(index = transactions_base[\"hadm_id\"].unique())\n",
        "\n",
        "    # Create demographic features - REMOVING anchor_age as requested\n",
        "    demographic_cols = [\"hadm_id\", \"gender\", \"age_category\"]  # Removed anchor_age\n",
        "\n",
        "    demographic_features = pd.get_dummies(transactions_base[demographic_cols], columns=[\"gender\", \"age_category\"], prefix=[\"Gender\", \"Age\"], prefix_sep=\"_\")\n",
        "\n",
        "    if \"hospital_expire_flag\" in demographic_features.columns:\n",
        "        demographic_features[\"Expired_In_Hospital\"] = demographic_features[\"hospital_expire_flag\"]\n",
        "        demographic_features = demographic_features.drop(\"hospital_expire_flag\", axis=1)\n",
        "\n",
        "    demographics_by_admission = demographic_features.groupby(\"hadm_id\").first()\n",
        "    print(f\"Created demographic features with shape: {demographics_by_admission.shape}\")\n",
        "\n",
        "    print(f\"Demographic features preview: \\n{demographics_by_admission.head()}\")\n",
        "        # === symptoms ===\n",
        "    print(\"Processing symptom features...\")\n",
        "    if 'hadm_id' not in symptoms.columns:\n",
        "        raise ValueError(\"The symptoms dataframe must contain a 'hadm_id' column to join with transactions_base.\")\n",
        "\n",
        "    if 'Symptom' not in symptoms.columns:\n",
        "        raise ValueError(\"The symptoms dataframe must contain a 'symptom_text' column with symptom labels.\")\n",
        "\n",
        "    #deleting dublicates\n",
        "    symptoms = symptoms[['hadm_id', 'Symptoms']].dropna().drop_duplicates()\n",
        "\n",
        "    # One-hot encode: every unique symptom_text → new binary\n",
        "    symptom_dummies = pd.get_dummies(symptoms['Symptoms'], prefix='Symptom')\n",
        "    symptoms_encoded = pd.concat([symptoms['hadm_id'], symptom_dummies], axis=1)\n",
        "    symptoms_by_admission = symptoms_encoded.groupby('hadm_id').max()\n",
        "\n",
        "    print(f\"Created symptom features with shape: {symptoms_by_admission.shape}\")\n",
        "    print(f\"Symptoms preview:\\n{symptoms_by_admission.head()}\")\n",
        "\n",
        "\n",
        "    # Merge all features into one dataset\n",
        "    all_features = pd.DataFrame(index = transactions_base[\"hadm_id\"].unique())\n",
        "\n",
        "    # List to keep track of dataframes with potential join issues\n",
        "    empty_dfs = []\n",
        "    for name, df in [(\"Procedures\", procedures_by_admission), (\"Demographics\", demographics_by_admission), (\"Symptoms\", symptoms_by_admission)]:\n",
        "        if df.empty:\n",
        "            print(f\"WARNING: {name} dataframe is empty.\")\n",
        "            empty_dfs.append(name)\n",
        "            continue\n",
        "        before_rows = len(all_features)\n",
        "        all_features = all_features.join(df, how='left')\n",
        "        after_rows = len(all_features)\n",
        "\n",
        "        if before_rows != after_rows:\n",
        "            print(f\"WARNING: {name} dataframe caused a join issue. Rows before: {before_rows}, Rows after: {after_rows}\")\n",
        "\n",
        "    if empty_dfs:\n",
        "        print(f\"WARNING: The following dataframes were empty and not included in the final dataset: {', '.join(empty_dfs)}\")\n",
        "\n",
        "    # Fill missing values with 0\n",
        "    all_features = all_features.fillna(0)\n",
        "\n",
        "    # 8. Create diagnosis outcome features\n",
        "    diagnosis_counts = transactions_base['primary_diagnosis'].value_counts()\n",
        "\n",
        "    # Create mapping of diagnosis codes to descriptions for readability\n",
        "    diagnosis_mapping = transactions_base[['primary_diagnosis_code', 'primary_diagnosis']].drop_duplicates().set_index('primary_diagnosis_code')['primary_diagnosis'].to_dict()\n",
        "\n",
        "    # We want at least 10 diagnoses, but respect max_features budget\n",
        "    max_procedure_features = max_features_count // 2  # Reserve half for procedures, half for diagnoses\n",
        "    diagnosis_feature_limit = max(10, max_features_count - len(demographics_by_admission.columns) -\n",
        "                            min(max_procedure_features,\n",
        "                                len(procedures_by_admission.columns) if hasattr(procedures_by_admission, 'columns') else 0))\n",
        "\n",
        "    # Simply take the top N most frequent diagnoses\n",
        "    common_diagnoses = diagnosis_counts.nlargest(min(diagnosis_feature_limit, len(diagnosis_counts))).index.tolist()\n",
        "\n",
        "    print(f\"Using {len(common_diagnoses)} most common diagnoses out of {len(diagnosis_counts)} total\")\n",
        "    diagnoses_filtered = transactions_base[transactions_base['primary_diagnosis'].isin(common_diagnoses)]\n",
        "    print(f\"Filtered diagnoses dataset shape: {diagnoses_filtered.shape}\")\n",
        "    print(f\"Filtered diagnoses dataset preview: \\n{diagnoses_filtered.head()}\")\n",
        "\n",
        "    # Check if we have diagnoses left after filtering\n",
        "    if len(diagnoses_filtered) == 0:\n",
        "        print(\"ERROR: No diagnoses meet the frequency threshold. Unable to create meaningful rules.\")\n",
        "        print(\"Please check your data or reduce the threshold further.\")\n",
        "        # Return a minimal dataframe to avoid errors\n",
        "        return pd.DataFrame(columns=['no_features_available'])\n",
        "\n",
        "    diagnosis_pivot = pd.get_dummies(diagnoses_filtered[['hadm_id', 'primary_diagnosis']],\n",
        "                                    columns=['primary_diagnosis'],\n",
        "                                    prefix='Diagnosis',\n",
        "                                    prefix_sep='_')\n",
        "    diagnosis_by_admission = diagnosis_pivot.groupby('hadm_id').max()\n",
        "\n",
        "    print(f\"Created diagnosis features with shape: {diagnosis_by_admission.shape}\")\n",
        "    print(f\"Diagnosis features preview: \\n{diagnosis_by_admission.head()}\")\n",
        "\n",
        "    transactions_matrix = all_features.join(diagnosis_by_admission, how='inner')\n",
        "\n",
        "    # Printing transaction matrix shape\n",
        "    print(\"TRANSACTION MATRIX\")\n",
        "    print(f\"Transaction matrix shape after joining features and outcomes: {transactions_matrix.shape}\")\n",
        "    print(f\"Transaction matrix preview: \\n{transactions_matrix.head()}\")\n",
        "    if transactions_matrix.empty:\n",
        "        print(\"ERROR: Empty transaction matrix after joining features and outcomes.\")\n",
        "        print(\"Please check that hadm_ids are consistent across your datasets.\")\n",
        "        return pd.DataFrame(columns=['empty_transactions_matrix'])\n",
        "    if (transactions_matrix.nunique() > 2).all():\n",
        "        print(\"WARNING: No binary features found in transaction matrix. Check your data transformations.\")\n",
        "\n",
        "    # Check for excessive NaN values\n",
        "    nan_percentage = transactions_matrix.isna().mean().mean() * 100\n",
        "    if nan_percentage > 0:\n",
        "        print(f\"WARNING: Transaction matrix contains {nan_percentage:.2f}% NaN values\")\n",
        "        transactions_matrix = transactions_matrix.fillna(0)\n",
        "\n",
        "    print(f\"Final transaction matrix: {transactions_matrix.shape[0]} rows and {transactions_matrix.shape[1]} columns\")\n",
        "    print(f\"Features include {len(demographics_by_admission.columns)} demographic features, \"\n",
        "         f\"{len(procedures_by_admission.columns) if hasattr(procedures_by_admission, 'columns') else 0} procedure features, \"\n",
        "         f\"and {len(diagnosis_by_admission.columns) if hasattr(diagnosis_by_admission, 'columns') else 0} diagnosis outcomes\")\n",
        "\n",
        "    # Save the transaction matrix\n",
        "    os.makedirs('output', exist_ok=True)\n",
        "    transactions_matrix.to_csv('output/transaction_matrix.csv')\n",
        "\n",
        "    # Save mappings for later use in readability\n",
        "    mappings = {\n",
        "        'procedure_mapping': procedure_mapping,\n",
        "        'diagnosis_mapping': diagnosis_mapping\n",
        "    }\n",
        "\n",
        "    with open('output/feature_mappings.pkl', 'wb') as f:\n",
        "        pickle.dump(mappings, f)\n",
        "\n",
        "    # Create and save human-readable versions of the transaction matrix\n",
        "    readable_matrix = create_readable_transaction_matrix(transactions_matrix)\n",
        "    detailed_matrix = create_detailed_transaction_matrix(transactions_matrix, transactions_base, diagnoses_with_desc, procedures_with_desc)\n",
        "\n",
        "    # For debug purposes, also save feature counts\n",
        "    feature_counts = pd.Series({\n",
        "        'demographic_features': len(demographics_by_admission.columns),\n",
        "        'procedure_features': len(procedures_by_admission.columns) if hasattr(procedures_by_admission, 'columns') else 0,\n",
        "        'diagnosis_features': len(diagnosis_by_admission.columns) if hasattr(diagnosis_by_admission, 'columns') else 0,\n",
        "        'total_features': transactions_matrix.shape[1],\n",
        "        'total_transactions': transactions_matrix.shape[0]\n",
        "    })\n",
        "    feature_counts.to_csv('output/feature_counts.csv')\n",
        "\n",
        "    return transactions_matrix\n",
        "\n",
        "\n",
        "def filter_diagnosis_to_procedure_demographic_rules(rules_df, transactions_matrix):\n",
        "    \"\"\"\n",
        "    Filter rules where antecedents (LHS) are diagnoses and consequents (RHS) are\n",
        "    procedures or demographic features.\n",
        "\n",
        "    Args:\n",
        "        rules_df (DataFrame): The complete set of association rules\n",
        "        transactions_matrix (DataFrame): The transaction matrix to identify feature types\n",
        "\n",
        "    Returns:\n",
        "        DataFrame: Filtered rules\n",
        "    \"\"\"\n",
        "    if rules_df.empty:\n",
        "        print(\"No rules to filter.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    # Identify feature types from the transaction matrix\n",
        "    diagnosis_cols = [col for col in transactions_matrix.columns if col.startswith('Diagnosis_')]\n",
        "    procedure_cols = [col for col in transactions_matrix.columns if col.startswith('Procedure_')]\n",
        "    # Modify demographic cols to exclude anchor_age\n",
        "    demographic_cols = [col for col in transactions_matrix.columns\n",
        "                       if col.startswith(('Gender_', 'Age_'))]  # Removed anchor_age\n",
        "\n",
        "    # Filter rules where:\n",
        "    # 1. Antecedents (LHS) contain only diagnosis features\n",
        "    # 2. Consequents (RHS) contain only procedure or demographic features\n",
        "    filtered_rules = rules_df[rules_df.apply(\n",
        "        lambda row: (\n",
        "            # Check that all antecedents are diagnoses\n",
        "            all(item in diagnosis_cols for item in row['antecedents'])\n",
        "            and\n",
        "            # Check that all consequents are either procedures or demographics\n",
        "            all(item in procedure_cols or item in demographic_cols for item in row['consequents'])\n",
        "        ),\n",
        "        axis=1\n",
        "    )]\n",
        "\n",
        "    # Check if we found any matching rules\n",
        "    if filtered_rules.empty:\n",
        "        print(\"No rules matching the Diagnosis → Procedure/Demographic pattern.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    print(f\"Found {len(filtered_rules)} rules where diagnoses predict procedures or demographics.\")\n",
        "\n",
        "    # Sort by lift for most interesting rules first\n",
        "    filtered_rules = filtered_rules.sort_values('lift', ascending=False)\n",
        "\n",
        "    # Save the filtered rules\n",
        "    os.makedirs('output', exist_ok=True)\n",
        "    filtered_rules.to_csv('output/diagnosis_to_proc_demo_rules.csv', index=False)\n",
        "\n",
        "    # Create a human-readable version\n",
        "    try:\n",
        "        # Load the feature mappings if available\n",
        "        if os.path.exists('output/feature_mappings.pkl'):\n",
        "            with open('output/feature_mappings.pkl', 'rb') as f:\n",
        "                feature_mappings = pickle.load(f)\n",
        "\n",
        "            # Create a human-readable version of the filtered rules\n",
        "            readable_filtered_rules = create_readable_rules(filtered_rules, feature_mappings)\n",
        "            readable_filtered_rules.to_csv('output/readable_diagnosis_to_proc_demo_rules.csv', index=False)\n",
        "    except Exception as e:\n",
        "        print(f\"WARNING: Could not create readable filtered rules: {str(e)}\")\n",
        "\n",
        "    return filtered_rules\n",
        "\n",
        "def mine_association_rules(transactions_matrix, min_support=0.0006, min_confidence=0.5):\n",
        "    print(\"Starting association rule mining...\")\n",
        "\n",
        "    # Convert the DataFrame to a one-hot encoded format\n",
        "    transactions_matrix_bool = transactions_matrix.astype(bool)\n",
        "\n",
        "    min_support_floor = min_support/10\n",
        "    min_confidence_floor = min_confidence/2\n",
        "\n",
        "    # Check if we should use a sample\n",
        "    sample_size = 0\n",
        "    try:\n",
        "        sample_size = int(os.environ.get('SAMPLE_SIZE', '0'))\n",
        "    except (ValueError, TypeError):\n",
        "        sample_size = 0\n",
        "\n",
        "    if sample_size > 0 and sample_size < transactions_matrix.shape[0]:\n",
        "        print(f\"Using a sample of {sample_size} transactions\")\n",
        "        transactions_matrix_bool = transactions_matrix_bool.sample(sample_size)\n",
        "\n",
        "    # 1. Find frequent itemsets with adaptive support threshold using FP-Growth\n",
        "    # Try to find a reasonable number of itemsets\n",
        "    frequent_itemsets = pd.DataFrame()\n",
        "\n",
        "    try:\n",
        "        # Using fpgrowth instead of apriori\n",
        "        frequent_itemsets = fpgrowth(transactions_matrix_bool,\n",
        "                                   min_support=min_support,\n",
        "                                   use_colnames=True,\n",
        "                                   max_len=4)  # Limit to combinations of at most 4 items\n",
        "\n",
        "        # If we found too few itemsets, try with a lower threshold\n",
        "        if len(frequent_itemsets) < 10:\n",
        "            old_support = min_support\n",
        "            min_support = max(min_support_floor, min_support / 2)\n",
        "            print(f\"Found too few itemsets ({len(frequent_itemsets)}). Reducing support from {old_support} to {min_support}\")\n",
        "\n",
        "            frequent_itemsets = fpgrowth(transactions_matrix_bool,\n",
        "                                       min_support=min_support,\n",
        "                                       use_colnames=True,\n",
        "                                       max_len=4)\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR in FP-Growth algorithm: {str(e)}\")\n",
        "        print(\"Trying with a smaller dataset...\")\n",
        "\n",
        "        # Sample the data if it's too large\n",
        "        if transactions_matrix.shape[0] > 10000:\n",
        "            sample_size = min(10000, int(transactions_matrix.shape[0] * 0.5))\n",
        "            transactions_sample = transactions_matrix.sample(sample_size)\n",
        "            try:\n",
        "                frequent_itemsets = fpgrowth(transactions_sample.astype(bool),\n",
        "                                           min_support=min_support,\n",
        "                                           use_colnames=True,\n",
        "                                           max_len=3)\n",
        "                print(f\"Successfully ran FP-Growth on a sample of {sample_size} transactions\")\n",
        "            except Exception as e2:\n",
        "                print(f\"ERROR in FP-Growth algorithm even with sampling: {str(e2)}\")\n",
        "                return pd.DataFrame(), pd.DataFrame(), pd.DataFrame(), pd.DataFrame()\n",
        "\n",
        "    if frequent_itemsets.empty:\n",
        "        print(\"No frequent itemsets found. Cannot generate rules.\")\n",
        "        return pd.DataFrame(), pd.DataFrame(), pd.DataFrame(), pd.DataFrame()\n",
        "\n",
        "    print(f\"Found {len(frequent_itemsets)} frequent itemsets with min_support={min_support}\")\n",
        "\n",
        "    # Save frequent itemsets\n",
        "    os.makedirs('output', exist_ok=True)\n",
        "    frequent_itemsets.to_csv('output/frequent_itemsets.csv')\n",
        "\n",
        "    # 2. Generate association rules with adaptive confidence threshold\n",
        "    rules = pd.DataFrame()\n",
        "\n",
        "    try:\n",
        "        rules = association_rules(frequent_itemsets,\n",
        "                                 metric='confidence',\n",
        "                                 min_threshold=min_confidence)\n",
        "\n",
        "        # If we found too few rules, try with a lower threshold\n",
        "        if len(rules) < 10:\n",
        "            old_confidence = min_confidence\n",
        "            min_confidence = max(min_confidence_floor, min_confidence / 1.5)\n",
        "            print(f\"Found too few rules ({len(rules)}). Reducing confidence from {old_confidence} to {min_confidence}\")\n",
        "\n",
        "            rules = association_rules(frequent_itemsets,\n",
        "                                    metric='confidence',\n",
        "                                    min_threshold=min_confidence)\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR in association_rules algorithm: {str(e)}\")\n",
        "        return pd.DataFrame(), pd.DataFrame(), pd.DataFrame(), pd.DataFrame()\n",
        "\n",
        "    if rules.empty:\n",
        "        print(\"No rules generated. Cannot proceed with rule filtering.\")\n",
        "        return frequent_itemsets, pd.DataFrame(), pd.DataFrame(), pd.DataFrame()\n",
        "\n",
        "    print(f\"Generated {len(rules)} rules with min_confidence={min_confidence}\")\n",
        "\n",
        "    # Save all rules\n",
        "    rules.to_csv('output/all_rules.csv', index=False)\n",
        "\n",
        "    # 3. Filter rules to focus on procedures\n",
        "    procedure_cols = [col for col in transactions_matrix.columns if col.startswith('Procedure_')]\n",
        "    if not procedure_cols:\n",
        "        print(\"ERROR: No procedures columns found in transaction matrix\")\n",
        "        return frequent_itemsets, rules, pd.DataFrame(), pd.DataFrame()\n",
        "\n",
        "    print(f\"Found {len(procedure_cols)} procedures columns to use for rule filtering\")\n",
        "\n",
        "    # Convert string representations of sets to actual sets, if needed\n",
        "    if isinstance(rules['antecedents'].iloc[0], str):\n",
        "        rules['antecedents'] = rules['antecedents'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
        "        rules['consequents'] = rules['consequents'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
        "\n",
        "    # Filter for rules that predict procedures\n",
        "    procedure_rules = rules[rules['consequents'].apply(\n",
        "        lambda x: any(item in procedure_cols for item in x)\n",
        "    )].copy()\n",
        "\n",
        "    if procedure_rules.empty:\n",
        "        print(\"WARNING: No rules found with procedures in the consequent\")\n",
        "    else:\n",
        "        print(f\"Found {len(procedure_rules)} rules with procedures in the consequent\")\n",
        "\n",
        "        # Additional filters to focus on more interesting rules\n",
        "        if len(procedure_rules) > 1000:\n",
        "            print(f\"Too many rules ({len(procedure_rules)}). Filtering to more interesting ones...\")\n",
        "\n",
        "            # Filter by lift (stronger associations)\n",
        "            high_lift_rules = procedure_rules[procedure_rules['lift'] > 1.5]\n",
        "            if len(high_lift_rules) >= 100:\n",
        "                procedure_rules = high_lift_rules\n",
        "                print(f\"Filtered to {len(procedure_rules)} rules with lift > 1.5\")\n",
        "\n",
        "        # Sort by lift and then confidence\n",
        "        procedure_rules = procedure_rules.sort_values(['lift', 'confidence'], ascending=[False, False])\n",
        "\n",
        "        # Save procedure rules\n",
        "        procedure_rules.to_csv('output/procedure_rules.csv', index=False)\n",
        "\n",
        "    # 4. Filter for diagnosis -> procedure/demographic rules\n",
        "    diagnosis_to_proc_demo_rules = filter_diagnosis_to_procedure_demographic_rules(rules, transactions_matrix)\n",
        "\n",
        "    # 5. Create human-readable versions of the rules\n",
        "    try:\n",
        "        # Load the feature mappings if available\n",
        "        if os.path.exists('output/feature_mappings.pkl'):\n",
        "            with open('output/feature_mappings.pkl', 'rb') as f:\n",
        "                feature_mappings = pickle.load(f)\n",
        "\n",
        "            # Create human-readable versions of the rules\n",
        "            if not procedure_rules.empty:\n",
        "                readable_rules = create_readable_rules(procedure_rules, feature_mappings)\n",
        "    except Exception as e:\n",
        "        print(f\"WARNING: Could not create readable rules: {str(e)}\")\n",
        "\n",
        "    return frequent_itemsets, rules, procedure_rules, diagnosis_to_proc_demo_rules\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    import argparse\n",
        "\n",
        "    parser = argparse.ArgumentParser(description='Run the association rule mining pipeline')\n",
        "    parser.add_argument('--sample_fraction', type=float, default=0.01, help='Fraction of data to sample')\n",
        "    parser.add_argument('--min_support', type=float, default=0.0006, help='Minimum support threshold')\n",
        "    parser.add_argument('--min_confidence', type=float, default=0.5, help='Minimum confidence threshold')\n",
        "    parser.add_argument('--skip_visualizations', action='store_true', help='Skip creating visualizations')\n",
        "    parser.add_argument('--skip_report', action='store_true', help='Skip generating HTML report')\n",
        "    parser.add_argument('--read_only', action='store_true', help='Only read existing data without reprocessing')\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    print(f\"Running pipeline with:\")\n",
        "    print(f\"  - sample_fraction: {args.sample_fraction}\")\n",
        "    print(f\"  - min_support: {args.min_support}\")\n",
        "    print(f\"  - min_confidence: {args.min_confidence}\")\n",
        "    print(f\"  - visualizations: {'Disabled' if args.skip_visualizations else 'Enabled'}\")\n",
        "    print(f\"  - HTML report: {'Disabled' if args.skip_report else 'Enabled'}\")\n",
        "    print(f\"  - mode: {'Read-only' if args.read_only else 'Full processing'}\")\n",
        "\n",
        "    # If we're in read-only mode, we'll just load existing files\n",
        "    if args.read_only:\n",
        "        print(\"\\nRunning in read-only mode. Loading existing data...\")\n",
        "\n",
        "        if os.path.exists('output/transaction_matrix.csv'):\n",
        "            transactions_matrix = pd.read_csv('output/transaction_matrix.csv', index_col=0)\n",
        "            print(f\"Loaded transaction matrix with shape {transactions_matrix.shape}\")\n",
        "\n",
        "            if os.path.exists('output/all_rules.csv'):\n",
        "                rules = pd.read_csv('output/all_rules.csv')\n",
        "                print(f\"Loaded {len(rules)} rules\")\n",
        "\n",
        "                # Convert string representations of sets to actual sets for visualization\n",
        "                if 'antecedents' in rules.columns and isinstance(rules['antecedents'].iloc[0], str):\n",
        "                    rules['antecedents'] = rules['antecedents'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
        "                    rules['consequents'] = rules['consequents'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
        "            else:\n",
        "                rules = pd.DataFrame()\n",
        "                print(\"No rules file found.\")\n",
        "\n",
        "            if os.path.exists('output/procedure_rules.csv'):\n",
        "                procedure_rules = pd.read_csv('output/procedure_rules.csv')\n",
        "                print(f\"Loaded {len(procedure_rules)} procedure rules\")\n",
        "\n",
        "                # Convert string representations of sets to actual sets for visualization\n",
        "                if 'antecedents' in procedure_rules.columns and isinstance(procedure_rules['antecedents'].iloc[0], str):\n",
        "                    procedure_rules['antecedents'] = procedure_rules['antecedents'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
        "                    procedure_rules['consequents'] = procedure_rules['consequents'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
        "            else:\n",
        "                procedure_rules = pd.DataFrame()\n",
        "                print(\"No procedure rules file found.\")\n",
        "\n",
        "            # Create visualizations if requested\n",
        "            if not args.skip_visualizations and visualization_available:\n",
        "                print(\"\\nCreating visualizations from existing data...\")\n",
        "\n",
        "                # Visualize feature distribution in transaction matrix\n",
        "                visualize_feature_distribution(transactions_matrix, save_path='output/feature_distribution.png')\n",
        "\n",
        "                # Visualize rule metrics\n",
        "                if not rules.empty:\n",
        "                    visualize_rule_metrics(rules, save_path='output/rule_metrics.png')\n",
        "                    visualize_rules_summary(rules, save_path='output/rules_summary.png')\n",
        "\n",
        "                # Visualize procedure rules network\n",
        "                if not procedure_rules.empty:\n",
        "                    visualize_rules_network(procedure_rules, max_rules=50, min_lift=1.0,\n",
        "                                          save_path='output/procedure_rules_network.png')\n",
        "        else:\n",
        "            print(\"ERROR: No transaction matrix file found. Cannot proceed in read-only mode.\")\n",
        "    else:\n",
        "        # Run the full pipeline\n",
        "        frequent_itemsets, rules, procedure_rules = main(\n",
        "            args.sample_fraction,\n",
        "            args.min_support,\n",
        "            args.min_confidence,\n",
        "            not args.skip_visualizations,\n",
        "            not args.skip_report\n",
        "        )\n",
        "\n",
        "        print(\"\\nPipeline completed successfully!\")\n",
        "\n",
        "        if not procedure_rules.empty:\n",
        "            print(f\"\\nTop 5 procedure rules by lift:\")\n",
        "            print(procedure_rules.sort_values('lift', ascending=False).head(5)[['antecedents', 'consequents', 'support', 'confidence', 'lift']])\n",
        "\n",
        "            print(\"\\nCheck the output directory for detailed results and human-readable formats.\")\n",
        "        else:\n",
        "            print(\"\\nNo procedure rules were found. Try adjusting the parameters or check your data.\")"
      ]
    }
  ]
}