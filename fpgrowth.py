# -*- coding: utf-8 -*-
"""fpgrowth.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OW16hDYdokuWInjO4OeERhIONUI2X3I4
"""

import pandas as pd
from mlxtend.frequent_patterns import fpgrowth, association_rules
from mlxtend.preprocessing import TransactionEncoder

data_dir = ""

# Path to data
patients_path =  data_dir +'patients.csv'
admissions_path = data_dir +'admissions.csv'
diagnoses_path = data_dir +'diagnoses_icd.csv'
lab_events_path = data_dir +'labevents_sample.csv'
d_icd_labs_path =data_dir +'d_labitems.csv'
d_icd_diagnoses_path = data_dir +'d_icd_diagnoses.csv'
d_icd_procedures_path = data_dir +'d_icd_procedures.csv'
procedures_path =data_dir + 'procedures_icd.csv'
prescriptions_path = data_dir +'prescriptions_sample.csv'
notes_path =data_dir + 'Notes.csv'

# Load the data
patients = pd.read_csv(patients_path, usecols=['subject_id', 'gender'])
admissions = pd.read_csv(admissions_path, usecols=['subject_id', 'hadm_id', 'race'])
diagnoses = pd.read_csv(diagnoses_path, usecols=['subject_id', 'hadm_id', 'icd_code'])
d_icd_diagnoses = pd.read_csv(d_icd_diagnoses_path, usecols=['icd_code', 'long_title'])
lab_events = pd.read_csv(lab_events_path, usecols=['subject_id', 'hadm_id', 'itemid', 'valuenum', 'ref_range_lower','ref_range_upper','flag'])
d_icd_labs = pd.read_csv(d_icd_labs_path, usecols=['itemid', 'label'])
procedures = pd.read_csv(procedures_path, usecols=['subject_id', 'hadm_id', 'icd_code'])
d_icd_procedures = pd.read_csv(d_icd_procedures_path, usecols=['icd_code', 'long_title'])
prescriptions = pd.read_csv(prescriptions_path, usecols=['subject_id', 'hadm_id', 'drug', 'dose_val_rx','dose_unit_rx'],  encoding='utf-16', on_bad_lines='skip')
notes = pd.read_csv(notes_path, usecols=['subject_id', 'hadm_id', 'Symptoms', 'allergies'])

threshold = 0.0001
min_count = max(1, int(threshold * len(admissions['hadm_id'].unique())))
print(f"Minimum count for frequent itemsets: {min_count}")

# Filter Diagnoses
# Count frequency of each diagnosis code
diag_counts = diagnoses['icd_code'].value_counts()
# Get only frequent codes
frequent_diags = diag_counts[diag_counts >= min_count].index
# Filter diagnoses
diagnoses = diagnoses[diagnoses['icd_code'].isin(frequent_diags)]
print(f"Remaining diagnoses: {diagnoses['icd_code'].nunique()}")

#import frequencies to csv
frequencies = pd.DataFrame(diagnoses['icd_code'].value_counts())
frequencies.to_csv('diagnoses_frequencies.csv', index=True)

# Count frequency of each procedure code
proc_counts = procedures['icd_code'].value_counts()

# Get only frequent codes
frequent_procs = proc_counts[proc_counts >= min_count].index

# Filter procedures
procedures = procedures[procedures['icd_code'].isin(frequent_procs)]
print(f"Remaining procedures: {procedures['icd_code'].nunique()}")

#import frequencies to csv
# frequencies = pd.DataFrame(procedures['icd_code'].value_counts())
# frequencies.to_csv('procedures_frequencies.csv', index=True)

# Count number of labevents item ids with abnormal flag

lab_counts = lab_events['itemid'].value_counts()

# Get frequent lab result patterns
frequent_labs = lab_counts[lab_counts >= min_count].index

# Filter lab events
lab_events = lab_events[
    lab_events['itemid'].isin(frequent_labs)
]
print(f"Remaining lab results: {lab_events['itemid'].nunique()}")

#import frequencies to csv
frequencies = pd.DataFrame(lab_events['itemid'].value_counts())
frequencies.to_csv('lab_events_frequencies.csv', index=True)

pres_counts = prescriptions['drug'].value_counts()
print(f"Total prescriptions: {len(pres_counts)}")
# Get only frequent codes
frequent_pres = pres_counts[pres_counts >= min_count].index
# Filter prescriptions
prescriptions = prescriptions[prescriptions['drug'].isin(frequent_pres)]
print(f"Remaining prescriptions: {prescriptions['drug'].nunique()}")

print("Merging dataframes...")

# Merge diagnoses with d_icd_diagnoses
diagnoses = diagnoses.merge(d_icd_diagnoses[['icd_code', 'long_title']],
                            on='icd_code', how='left')
print(f"Diagnoses after merge: {diagnoses.shape}")
print(diagnoses.head())

# Merge procedures with d_icd_procedures
procedures = procedures.merge(d_icd_procedures[['icd_code', 'long_title']],
                              on='icd_code', how='left')
print(f"Procedures after merge: {procedures.shape}")
print(procedures.head())

# Merge lab_events with d_icd_labs (d_labitems)
lab_events_with_desc = lab_events.merge(d_icd_labs[['itemid', 'label']],
                                       on='itemid', how='left')
print(f"Lab events after merge: {lab_events_with_desc.shape}")

# Process lab events (abnormal results only, with range status)
print("Processing lab events...")
lab_events_with_desc['lab_result'] = (
    lab_events_with_desc['itemid'].astype(str) + '_' +
    lab_events_with_desc['label'].fillna('Unknown')
)
lab_events_with_desc = lab_events_with_desc.dropna(subset=['hadm_id', 'flag'])
print(f"Filtered lab events (non-null flag): {lab_events_with_desc.shape}")

if lab_events_with_desc.empty:
    print("WARNING: No abnormal lab events found.")
    lab_grouped = pd.DataFrame(columns=['hadm_id', 'labs'])
else:
    # Categorize lab results as Below, Above, or Unknown
    def classify_range(row):
        if pd.notnull(row['valuenum']) and pd.notnull(row['ref_range_lower']) and row['valuenum'] < row['ref_range_lower']:
            return 'Below'
        elif pd.notnull(row['valuenum']) and pd.notnull(row['ref_range_upper']) and row['valuenum'] > row['ref_range_upper']:
            return 'Above'
        return 'Unknown'

    lab_events_with_desc['range_status'] = lab_events_with_desc.apply(classify_range, axis=1)
    lab_events_with_desc['lab_result'] = (
        lab_events_with_desc['itemid'].astype(str) + '_' +
        # lab_events_with_desc['label'].fillna('Unknown') + '_' +
        lab_events_with_desc['range_status']
    )
    lab_events_with_desc = lab_events_with_desc[lab_events_with_desc['range_status'] != 'Unknown']
    print(f"Lab events after filtering Unknown status: {lab_events_with_desc.shape}")


# Group lab events by hadm_id
lab_grouped = (lab_events_with_desc.groupby('hadm_id')['lab_result']
               .apply(lambda x: list(x.unique()))
               .reset_index()
               .rename(columns={'lab_result': 'labs'}))
print(f"Lab grouped shape: {lab_grouped.shape}")
print(lab_grouped.head())



procedures['combined_title'] = (
    procedures['icd_code'].astype(str)
    # procedures['long_title'].fillna('Unknown')
)

# Group procedures by hadm_id, collecting unique combined titles
procedures_grouped = (procedures.groupby('hadm_id')['combined_title']
                      .apply(lambda x: list(x.unique()))
                      .reset_index()
                      .rename(columns={'combined_title': 'procedures'}))

# Print shape and sample of grouped procedures
print(f"Procedures grouped shape: {procedures_grouped.shape}")
print(procedures_grouped.head())

diagnoses['combined_title'] = (
    diagnoses['icd_code'].astype(str)
    # diagnoses['long_title'].fillna('Unknown')
)

# Group diagnoses by hadm_id
diagnoses_grouped = (diagnoses.groupby('hadm_id')['combined_title'] #icd_code
                     .apply(lambda x: list(x.unique()))
                     .reset_index()
                     .rename(columns={'combined_title': 'diagnoses'}))
print(f"Diagnoses grouped shape: {diagnoses_grouped.shape}")
print(diagnoses_grouped.head())

prescriptions['combined_title'] = (
    prescriptions['drug'].astype(str) + '_' +
    prescriptions['dose_val_rx'].fillna('Unknown').astype(str) + '_' +
    prescriptions['dose_unit_rx'].fillna('Unknown').astype(str)
)


# Group prescriptions by hadm_id
prescriptions_grouped = (prescriptions.groupby('hadm_id')['combined_title']
                          .apply(lambda x: list(x.unique()))
                          .reset_index()
                          .rename(columns={'combined_title': 'prescriptions'}))
print(f"Prescriptions grouped shape: {prescriptions_grouped.shape}")

# import as csv file
# prescriptions_grouped.to_csv('prescriptions_grouped.csv', index=False)

print(prescriptions_grouped.head())

notes = pd.read_csv(notes_path, usecols=['hadm_id', 'Symptoms', 'allergies'])

# Preprocess Symptoms and allergies to ensure they are lists with prefixed, formatted items
def format_items(value, prefix):
    if pd.isna(value) or value is None or value == '':
        return []
    try:
        if isinstance(value, str):
            # Split by comma, clean, and format each item
            items = [item.strip().lower().replace(' ', '_') for item in value.split(',') if item.strip() and item.lower() != 'none']
            return [f"{prefix}{item}" for item in items]
        if isinstance(value, list):
            # Clean and format list items
            items = [item.strip().lower().replace(' ', '_') for item in value if isinstance(item, str) and item.strip() and item.lower() != 'none']
            return [f"{prefix}{item}" for item in items]
        print(f"Unexpected value type for {prefix}: {value} (type: {type(value)})")
        return []
    except Exception as e:
        print(f"Error processing {prefix} value {value}: {e}")
        return []

# Apply formatting and ensure no NaN values remain
notes['Symptoms'] = notes['Symptoms'].apply(lambda x: format_items(x, 'Symptom_'))
notes['allergies'] = notes['allergies'].apply(lambda x: format_items(x, 'Allergy_'))

# Group by hadm_id, keeping Symptoms and allergies separate
notes_grouped = notes.groupby('hadm_id').agg({
    'Symptoms': lambda x: list(set(item for sublist in x for item in sublist if isinstance(sublist, list))),
    'allergies': lambda x: list(set(item for sublist in x for item in sublist if isinstance(sublist, list)))
}).reset_index()

# Print shape and sample of grouped notes
print(f"\nNotes grouped shape: {notes_grouped.shape}")
print("Sample of grouped notes:")
print(notes_grouped.head())

# Admissions with patients
admissions_patients = admissions.merge(patients[['subject_id', 'gender']], on='subject_id', how='left')
print(f"Admissions with patients shape: {admissions_patients.shape}")

# Create base dataframe with all hadm_ids
transactions_df = pd.DataFrame({'hadm_id': admissions['hadm_id'].unique()})

# Merge all grouped data
print("Combining data for transactions...")
#transactions_df = transactions_df.merge(admissions_patients[['hadm_id', 'gender', 'race']], on='hadm_id', how='left')
transactions_df = transactions_df.merge(diagnoses_grouped, on='hadm_id', how='left')
transactions_df = transactions_df.merge(procedures_grouped, on='hadm_id', how='left')
# transactions_df = transactions_df.merge(lab_grouped, on='hadm_id', how='left')
transactions_df = transactions_df.merge(prescriptions_grouped, on='hadm_id', how='left')
# transactions_df = transactions_df.merge(notes_grouped, on='hadm_id', how='left')
print(f"Transactions dataframe shape: {transactions_df.shape}")

# Generate transactions
transactions = []
for _, row in transactions_df.iterrows():
    transaction = []

    # Helper function to safely add items
    def add_items(items, prefix=''):
        if isinstance(items, list):
            transaction.extend([f"{prefix}{item}" for item in items if pd.notna(item) and str(item).strip()])

    # Diagnoses
    if 'diagnoses' in row:
        add_items(row['diagnoses'], 'DIA_')

    # Procedures
    if 'procedures' in row:
        add_items(row['procedures'], 'PRO_')

    # Lab results
    if 'labs' in row:
        add_items(row['labs'], 'LAB_')

    # Gender
    if 'gender' in row and pd.notna(row['gender']):
        transaction.append(f"Gender_{row['gender']}")

    # Race
    if 'race' in row and pd.notna(row['race']):
        transaction.append(f"Race_{row['race'].replace(' ', '_')}")

    # Symptoms
    if 'Symptoms' in row:
        add_items(row['Symptoms'])

    # Allergies
    if 'allergies' in row:
        add_items(row['allergies'])

    # Prescriptions
    if 'prescriptions' in row:
        add_items(row['prescriptions'], 'PRE_')

    transactions.append(transaction)

# Print sample transactions
print("\nSample transactions:")
for i, t in enumerate(transactions[:10], 1):
    print(f"Transaction {i}: {t[:10]}...")

# Sort transactions by number of items (descending or ascending as needed)
sorted_transactions = sorted(transactions, key=len, reverse=True)  # Use reverse=False for ascending

# Convert to DataFrame for CSV export
df_sorted_transactions = pd.DataFrame({
    'transaction_id': range(1, len(sorted_transactions) + 1),
    'num_items': [len(t) for t in sorted_transactions],
    'items': [', '.join(t) for t in sorted_transactions]
})

# Save to CSV
# df_sorted_transactions.to_csv('sorted_transactions.csv', index=False)

# Print a few to confirm
print(df_sorted_transactions.head())

# #Filter Rare Items First
# from collections import Counter

# # Count item frequencies across all transactions
# item_counts = Counter(item for transaction in transactions for item in transaction)

# # Keep only items that appear at least min_freq times
# min_freq = 50  # Adjust based on your data size
# frequent_items = {item for item, count in item_counts.items() if count >= min_freq}

# # Filter transactions
# filtered_transactions = [
#     [item for item in txn if item in frequent_items]
#     for txn in transactions
# ]

# Create a mask for rows with non-empty transactions
mask = [len(t) > 0 for t in transactions]

# Filter transactions and transactions_df together
filtered_transactions = [t for t in transactions if len(t) > 0]
filtered_df = transactions_df.loc[mask].reset_index(drop=True)

# Now lengths should match
print(len(filtered_transactions), filtered_df.shape[0])

# Create the DataFrame
transactions_df_out = pd.DataFrame({
    'hadm_id': filtered_df['hadm_id'],
    'transaction': filtered_transactions
})

# Save to CSV
transactions_df_out.to_csv("transactions.csv", index=False)
print("Transactions saved to transactions.csv")

# Extract the list of transactions from the DataFrame
transactions_list = transactions_df_out['transaction'].tolist()

# Initialize encoder and fit_transform the transactions
encoder = TransactionEncoder()
onehot = encoder.fit_transform(transactions_list)

# Convert to DataFrame for easier analysis
df_onehot = pd.DataFrame(onehot, columns=encoder.columns_)

print(f"One-hot encoded shape: {df_onehot.shape}")

# Get item frequencies from your one-hot encoded data
item_frequencies = df_onehot.sum().sort_values(ascending=False)

# Display top N frequent items
print("Top Most Frequent Items:")
print(item_frequencies.head(30))

# Convert to DataFrame and reset index to get items as a column
freq_df = item_frequencies.reset_index()
freq_df.columns = ['Item', 'Frequency']  # Name the columns

# Save to CSV with both item names and frequencies
freq_df.to_csv("frequent_items.csv", index=False)

# Removing redundant columns
# Define patterns to exclude
# unwanted_patterns = [
#     'no_known_allergies',
#     'patient_recorded_as_having_no_known_allergies',
#     'Allergy_*********per_pt_has_lots_of_allergies._daughter_will_bring_\nlist***********_/_ampicillin_/_cortisone_/_nitrofurantoin_/',
#     'Allergy____',
#     '_____'  # For the "____" case
# ]

# # Filter transactions to remove these items
# filtered_transactions = [
#     [item for item in txn
#      if not any(pattern in str(item).lower() for pattern in unwanted_patterns)]
#     for txn in transactions
# ]

#sample 1000 rows from the onehot dataframe
#df_onehot_sample = df_onehot.sample(n=10000, random_state=1)

print(df_onehot.shape)
#frequent_itemsets = fpgrowth(df_onehot_sample, min_support=0.1, use_colnames=True)
frequent_itemsets = fpgrowth(df_onehot, min_support=0.001, use_colnames=True)
print(f"Frequent itemsets shape: {frequent_itemsets.shape}")
print("Frequent Itemsets:\n", frequent_itemsets)
frequent_itemsets.to_csv("frequent_itemsets.csv", index=False)

rules = association_rules(frequent_itemsets, metric="confidence", min_threshold=0.1)
print("\nAssociation Rules:\n", rules)
rules.to_csv("association_rules.csv", index=False)

