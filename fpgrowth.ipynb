{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "8b4aa07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from mlxtend.frequent_patterns import fpgrowth, association_rules\n",
    "from mlxtend.preprocessing import TransactionEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "796f1096",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"\"\n",
    "\n",
    "# Path to data\n",
    "patients_path =  data_dir +'patients.csv'\n",
    "admissions_path = data_dir +'admissions.csv'\n",
    "diagnoses_path = data_dir +'diagnoses_icd.csv'\n",
    "lab_events_path = data_dir +'labevents_sample.csv'\n",
    "d_icd_labs_path =data_dir +'d_labitems.csv'\n",
    "d_icd_diagnoses_path = data_dir +'d_icd_diagnoses.csv'\n",
    "d_icd_procedures_path = data_dir +'d_icd_procedures.csv'\n",
    "procedures_path =data_dir + 'procedures_icd.csv'\n",
    "prescriptions_path = data_dir +'prescriptions_sample.csv'\n",
    "notes_path =data_dir + 'Notes.csv'\n",
    "\n",
    "# Load the data\n",
    "patients = pd.read_csv(patients_path, usecols=['subject_id', 'gender'])\n",
    "admissions = pd.read_csv(admissions_path, usecols=['subject_id', 'hadm_id', 'race'])\n",
    "diagnoses = pd.read_csv(diagnoses_path, usecols=['subject_id', 'hadm_id', 'icd_code'])\n",
    "d_icd_diagnoses = pd.read_csv(d_icd_diagnoses_path, usecols=['icd_code', 'long_title'])\n",
    "lab_events = pd.read_csv(lab_events_path, usecols=['subject_id', 'hadm_id', 'itemid', 'valuenum', 'ref_range_lower','ref_range_upper','flag'])\n",
    "d_icd_labs = pd.read_csv(d_icd_labs_path, usecols=['itemid', 'label'])\n",
    "procedures = pd.read_csv(procedures_path, usecols=['subject_id', 'hadm_id', 'icd_code'])\n",
    "d_icd_procedures = pd.read_csv(d_icd_procedures_path, usecols=['icd_code', 'long_title'])\n",
    "prescriptions = pd.read_csv(prescriptions_path, usecols=['subject_id', 'hadm_id', 'drug', 'dose_val_rx','dose_unit_rx'],  encoding='utf-16', on_bad_lines='skip')\n",
    "notes = pd.read_csv(notes_path, usecols=['subject_id', 'hadm_id', 'Symptoms', 'allergies'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "aea30402",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum count for frequent itemsets: 54\n"
     ]
    }
   ],
   "source": [
    "threshold = 0.0001\n",
    "min_count = max(1, int(threshold * len(admissions['hadm_id'].unique())))\n",
    "print(f\"Minimum count for frequent itemsets: {min_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "b5b98ccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining diagnoses: 6332\n"
     ]
    }
   ],
   "source": [
    "# Filter Diagnoses\n",
    "# Count frequency of each diagnosis code\n",
    "diag_counts = diagnoses['icd_code'].value_counts()\n",
    "# Get only frequent codes\n",
    "frequent_diags = diag_counts[diag_counts >= min_count].index\n",
    "# Filter diagnoses\n",
    "diagnoses = diagnoses[diagnoses['icd_code'].isin(frequent_diags)]\n",
    "print(f\"Remaining diagnoses: {diagnoses['icd_code'].nunique()}\")\n",
    "\n",
    "#import frequencies to csv\n",
    "frequencies = pd.DataFrame(diagnoses['icd_code'].value_counts())\n",
    "frequencies.to_csv('diagnoses_frequencies.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "54c19f07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining procedures: 1701\n"
     ]
    }
   ],
   "source": [
    "# Count frequency of each procedure code\n",
    "proc_counts = procedures['icd_code'].value_counts()\n",
    "\n",
    "# Get only frequent codes\n",
    "frequent_procs = proc_counts[proc_counts >= min_count].index\n",
    "\n",
    "# Filter procedures\n",
    "procedures = procedures[procedures['icd_code'].isin(frequent_procs)]\n",
    "print(f\"Remaining procedures: {procedures['icd_code'].nunique()}\")\n",
    "\n",
    "#import frequencies to csv\n",
    "# frequencies = pd.DataFrame(procedures['icd_code'].value_counts())\n",
    "# frequencies.to_csv('procedures_frequencies.csv', index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "c639dc4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining lab results: 254\n"
     ]
    }
   ],
   "source": [
    "# Count number of labevents item ids with abnormal flag\n",
    "\n",
    "lab_counts = lab_events['itemid'].value_counts()\n",
    "\n",
    "# Get frequent lab result patterns\n",
    "frequent_labs = lab_counts[lab_counts >= min_count].index\n",
    "\n",
    "# Filter lab events\n",
    "lab_events = lab_events[\n",
    "    lab_events['itemid'].isin(frequent_labs)\n",
    "]\n",
    "print(f\"Remaining lab results: {lab_events['itemid'].nunique()}\")\n",
    "\n",
    "#import frequencies to csv\n",
    "frequencies = pd.DataFrame(lab_events['itemid'].value_counts())\n",
    "frequencies.to_csv('lab_events_frequencies.csv', index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "fb85e181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total prescriptions: 561\n",
      "Remaining prescriptions: 43\n"
     ]
    }
   ],
   "source": [
    "pres_counts = prescriptions['drug'].value_counts()\n",
    "print(f\"Total prescriptions: {len(pres_counts)}\")\n",
    "# Get only frequent codes\n",
    "frequent_pres = pres_counts[pres_counts >= min_count].index\n",
    "# Filter prescriptions\n",
    "prescriptions = prescriptions[prescriptions['drug'].isin(frequent_pres)]\n",
    "print(f\"Remaining prescriptions: {prescriptions['drug'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "219adb6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging dataframes...\n",
      "Diagnoses after merge: (6278083, 4)\n",
      "   subject_id   hadm_id icd_code  \\\n",
      "0    10000032  22595853     5723   \n",
      "1    10000032  22595853    78959   \n",
      "2    10000032  22595853     5715   \n",
      "3    10000032  22595853    07070   \n",
      "4    10000032  22595853      496   \n",
      "\n",
      "                                          long_title  \n",
      "0                                Portal hypertension  \n",
      "1                                      Other ascites  \n",
      "2      Cirrhosis of liver without mention of alcohol  \n",
      "3  Unspecified viral hepatitis C without hepatic ...  \n",
      "4  Chronic airway obstruction, not elsewhere clas...  \n",
      "Procedures after merge: (764313, 4)\n",
      "   subject_id   hadm_id icd_code  \\\n",
      "0    10000032  22595853     5491   \n",
      "1    10000032  22841357     5491   \n",
      "2    10000032  25742920     5491   \n",
      "3    10000068  25022803     8938   \n",
      "4    10000117  27988844  0QS734Z   \n",
      "\n",
      "                                          long_title  \n",
      "0                    Percutaneous abdominal drainage  \n",
      "1                    Percutaneous abdominal drainage  \n",
      "2                    Percutaneous abdominal drainage  \n",
      "3        Other nonoperative respiratory measurements  \n",
      "4  Reposition Left Upper Femur with Internal Fixa...  \n",
      "Lab events after merge: (494190, 8)\n",
      "Processing lab events...\n",
      "Filtered lab events (non-null flag): (96151, 9)\n",
      "Lab events after filtering Unknown status: (93776, 10)\n",
      "Lab grouped shape: (1380, 2)\n",
      "      hadm_id                                               labs\n",
      "0  20010003.0  [51221_Below, 51222_Below, 51265_Below, 51277_...\n",
      "1  20015927.0  [51221_Below, 51222_Below, 51248_Above, 51250_...\n",
      "2  20019162.0  [51221_Below, 51222_Below, 51265_Below, 51279_...\n",
      "3  20023045.0  [51221_Below, 51222_Below, 51279_Below, 50924_...\n",
      "4  20023531.0  [50912_Above, 50931_Above, 50970_Above, 51006_...\n"
     ]
    }
   ],
   "source": [
    "print(\"Merging dataframes...\")\n",
    "\n",
    "# Merge diagnoses with d_icd_diagnoses\n",
    "diagnoses = diagnoses.merge(d_icd_diagnoses[['icd_code', 'long_title']], \n",
    "                            on='icd_code', how='left')\n",
    "print(f\"Diagnoses after merge: {diagnoses.shape}\")\n",
    "print(diagnoses.head())\n",
    "\n",
    "# Merge procedures with d_icd_procedures\n",
    "procedures = procedures.merge(d_icd_procedures[['icd_code', 'long_title']], \n",
    "                              on='icd_code', how='left')\n",
    "print(f\"Procedures after merge: {procedures.shape}\")\n",
    "print(procedures.head())\n",
    "\n",
    "# Merge lab_events with d_icd_labs (d_labitems)\n",
    "lab_events_with_desc = lab_events.merge(d_icd_labs[['itemid', 'label']], \n",
    "                                       on='itemid', how='left')\n",
    "print(f\"Lab events after merge: {lab_events_with_desc.shape}\")\n",
    "\n",
    "# Process lab events (abnormal results only, with range status)\n",
    "print(\"Processing lab events...\")\n",
    "lab_events_with_desc['lab_result'] = (\n",
    "    lab_events_with_desc['itemid'].astype(str) + '_' + \n",
    "    lab_events_with_desc['label'].fillna('Unknown')\n",
    ")\n",
    "lab_events_with_desc = lab_events_with_desc.dropna(subset=['hadm_id', 'flag'])\n",
    "print(f\"Filtered lab events (non-null flag): {lab_events_with_desc.shape}\")\n",
    "\n",
    "if lab_events_with_desc.empty:\n",
    "    print(\"WARNING: No abnormal lab events found.\")\n",
    "    lab_grouped = pd.DataFrame(columns=['hadm_id', 'labs'])\n",
    "else:\n",
    "    # Categorize lab results as Below, Above, or Unknown\n",
    "    def classify_range(row):\n",
    "        if pd.notnull(row['valuenum']) and pd.notnull(row['ref_range_lower']) and row['valuenum'] < row['ref_range_lower']:\n",
    "            return 'Below'\n",
    "        elif pd.notnull(row['valuenum']) and pd.notnull(row['ref_range_upper']) and row['valuenum'] > row['ref_range_upper']:\n",
    "            return 'Above'\n",
    "        return 'Unknown'\n",
    "\n",
    "    lab_events_with_desc['range_status'] = lab_events_with_desc.apply(classify_range, axis=1)\n",
    "    lab_events_with_desc['lab_result'] = (\n",
    "        lab_events_with_desc['itemid'].astype(str) + '_' + \n",
    "        # lab_events_with_desc['label'].fillna('Unknown') + '_' + \n",
    "        lab_events_with_desc['range_status']\n",
    "    )\n",
    "    lab_events_with_desc = lab_events_with_desc[lab_events_with_desc['range_status'] != 'Unknown']\n",
    "    print(f\"Lab events after filtering Unknown status: {lab_events_with_desc.shape}\")\n",
    "\n",
    "\n",
    "# Group lab events by hadm_id\n",
    "lab_grouped = (lab_events_with_desc.groupby('hadm_id')['lab_result']\n",
    "               .apply(lambda x: list(x.unique()))\n",
    "               .reset_index()\n",
    "               .rename(columns={'lab_result': 'labs'}))\n",
    "print(f\"Lab grouped shape: {lab_grouped.shape}\")\n",
    "print(lab_grouped.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b5053a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "a25680de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procedures grouped shape: (274352, 2)\n",
      "    hadm_id                                     procedures\n",
      "0  20000041                                         [8154]\n",
      "1  20000045                                      [3E0436Z]\n",
      "2  20000069                             [0KQM0ZZ, 10E0XZZ]\n",
      "3  20000102                                   [7359, 7309]\n",
      "4  20000147  [02100Z9, B211YZZ, 021209W, 06BQ4ZZ, 5A1221Z]\n"
     ]
    }
   ],
   "source": [
    "procedures['combined_title'] = (\n",
    "    procedures['icd_code'].astype(str)\n",
    "    # procedures['long_title'].fillna('Unknown')\n",
    ")\n",
    "\n",
    "# Group procedures by hadm_id, collecting unique combined titles\n",
    "procedures_grouped = (procedures.groupby('hadm_id')['combined_title']\n",
    "                      .apply(lambda x: list(x.unique()))\n",
    "                      .reset_index()\n",
    "                      .rename(columns={'combined_title': 'procedures'}))\n",
    "\n",
    "# Print shape and sample of grouped procedures\n",
    "print(f\"Procedures grouped shape: {procedures_grouped.shape}\")\n",
    "print(procedures_grouped.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "94f37657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diagnoses grouped shape: (544361, 2)\n",
      "    hadm_id                                          diagnoses\n",
      "0  20000019  [0389, 59080, 75310, 5849, 2761, 99591, 25000,...\n",
      "1  20000024  [D500, K521, I10, E538, M810, R270, Z9181, H54...\n",
      "2  20000034  [K831, K8689, K861, K869, R1032, R8279, J439, ...\n",
      "3  20000041  [71536, 25002, V8541, 4019, 2724, V4586, 53081...\n",
      "4  20000045  [A419, N390, C7951, C787, K56699, C779, K5100,...\n"
     ]
    }
   ],
   "source": [
    "diagnoses['combined_title'] = (\n",
    "    diagnoses['icd_code'].astype(str)\n",
    "    # diagnoses['long_title'].fillna('Unknown')\n",
    ")\n",
    "\n",
    "# Group diagnoses by hadm_id\n",
    "diagnoses_grouped = (diagnoses.groupby('hadm_id')['combined_title'] #icd_code\n",
    "                     .apply(lambda x: list(x.unique()))\n",
    "                     .reset_index()\n",
    "                     .rename(columns={'combined_title': 'diagnoses'}))\n",
    "print(f\"Diagnoses grouped shape: {diagnoses_grouped.shape}\")\n",
    "print(diagnoses_grouped.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "4dae9a20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prescriptions grouped shape: (252, 2)\n",
      "    hadm_id                                      prescriptions\n",
      "0  20030125  [Omeprazole_20_mg, OxycoDONE (Immediate Releas...\n",
      "1  20032235  [Morphine Sulfate_2-4_mg, Lorazepam_0.5_mg, He...\n",
      "2  20144849  [Sodium Chloride 0.9%  Flush_3-10_mL, Acetamin...\n",
      "3  20195471  [Warfarin_7.5_mg, Omeprazole_20_mg, HYDROmorph...\n",
      "4  20214994  [HYDROmorphone (Dilaudid)_0.25_mg, Potassium C...\n"
     ]
    }
   ],
   "source": [
    "prescriptions['combined_title'] = (\n",
    "    prescriptions['drug'].astype(str) + '_' + \n",
    "    prescriptions['dose_val_rx'].fillna('Unknown').astype(str) + '_' + \n",
    "    prescriptions['dose_unit_rx'].fillna('Unknown').astype(str)\n",
    ")\n",
    "\n",
    "\n",
    "# Group prescriptions by hadm_id\n",
    "prescriptions_grouped = (prescriptions.groupby('hadm_id')['combined_title']\n",
    "                          .apply(lambda x: list(x.unique()))\n",
    "                          .reset_index()\n",
    "                          .rename(columns={'combined_title': 'prescriptions'}))\n",
    "print(f\"Prescriptions grouped shape: {prescriptions_grouped.shape}\")\n",
    "\n",
    "# import as csv file\n",
    "# prescriptions_grouped.to_csv('prescriptions_grouped.csv', index=False)\n",
    "\n",
    "print(prescriptions_grouped.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "527f8c0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Notes grouped shape: (331793, 3)\n",
      "Sample of grouped notes:\n",
      "    hadm_id                                           Symptoms  \\\n",
      "0  20000019  [Symptom_fever, Symptom_flank_pain, Symptom_na...   \n",
      "1  20000024               [Symptom_weakness, Symptom_diarrhea]   \n",
      "2  20000034                                                 []   \n",
      "3  20000041                              [Symptom_l_knee_pain]   \n",
      "4  20000057  [Symptom_cough, Symptom_ankle_pain_(s/p_mechan...   \n",
      "\n",
      "                                           allergies  \n",
      "0  [Allergy_no_known_allergies_/_adverse_drug_rea...  \n",
      "1                                  [Allergy_aspirin]  \n",
      "2  [Allergy_no_known_allergies_/_adverse_drug_rea...  \n",
      "3                                    [Allergy_latex]  \n",
      "4  [Allergy_no_known_allergies_/_adverse_drug_rea...  \n"
     ]
    }
   ],
   "source": [
    "notes = pd.read_csv(notes_path, usecols=['hadm_id', 'Symptoms', 'allergies'])\n",
    "\n",
    "# Preprocess Symptoms and allergies to ensure they are lists with prefixed, formatted items\n",
    "def format_items(value, prefix):\n",
    "    if pd.isna(value) or value is None or value == '':\n",
    "        return []\n",
    "    try:\n",
    "        if isinstance(value, str):\n",
    "            # Split by comma, clean, and format each item\n",
    "            items = [item.strip().lower().replace(' ', '_') for item in value.split(',') if item.strip() and item.lower() != 'none']\n",
    "            return [f\"{prefix}{item}\" for item in items]\n",
    "        if isinstance(value, list):\n",
    "            # Clean and format list items\n",
    "            items = [item.strip().lower().replace(' ', '_') for item in value if isinstance(item, str) and item.strip() and item.lower() != 'none']\n",
    "            return [f\"{prefix}{item}\" for item in items]\n",
    "        print(f\"Unexpected value type for {prefix}: {value} (type: {type(value)})\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {prefix} value {value}: {e}\")\n",
    "        return []\n",
    "\n",
    "# Apply formatting and ensure no NaN values remain\n",
    "notes['Symptoms'] = notes['Symptoms'].apply(lambda x: format_items(x, 'Symptom_'))\n",
    "notes['allergies'] = notes['allergies'].apply(lambda x: format_items(x, 'Allergy_'))\n",
    "\n",
    "# Group by hadm_id, keeping Symptoms and allergies separate\n",
    "notes_grouped = notes.groupby('hadm_id').agg({\n",
    "    'Symptoms': lambda x: list(set(item for sublist in x for item in sublist if isinstance(sublist, list))),\n",
    "    'allergies': lambda x: list(set(item for sublist in x for item in sublist if isinstance(sublist, list)))\n",
    "}).reset_index()\n",
    "\n",
    "# Print shape and sample of grouped notes\n",
    "print(f\"\\nNotes grouped shape: {notes_grouped.shape}\")\n",
    "print(\"Sample of grouped notes:\")\n",
    "print(notes_grouped.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "11aeed4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Admissions with patients shape: (546028, 4)\n"
     ]
    }
   ],
   "source": [
    "# Admissions with patients\n",
    "admissions_patients = admissions.merge(patients[['subject_id', 'gender']], on='subject_id', how='left')\n",
    "print(f\"Admissions with patients shape: {admissions_patients.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "c838e20f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining data for transactions...\n",
      "Transactions dataframe shape: (546028, 4)\n"
     ]
    }
   ],
   "source": [
    "# Create base dataframe with all hadm_ids\n",
    "transactions_df = pd.DataFrame({'hadm_id': admissions['hadm_id'].unique()})\n",
    "\n",
    "# Merge all grouped data\n",
    "print(\"Combining data for transactions...\")\n",
    "#transactions_df = transactions_df.merge(admissions_patients[['hadm_id', 'gender', 'race']], on='hadm_id', how='left')\n",
    "transactions_df = transactions_df.merge(diagnoses_grouped, on='hadm_id', how='left')\n",
    "transactions_df = transactions_df.merge(procedures_grouped, on='hadm_id', how='left')\n",
    "# transactions_df = transactions_df.merge(lab_grouped, on='hadm_id', how='left')\n",
    "transactions_df = transactions_df.merge(prescriptions_grouped, on='hadm_id', how='left')\n",
    "# transactions_df = transactions_df.merge(notes_grouped, on='hadm_id', how='left')\n",
    "print(f\"Transactions dataframe shape: {transactions_df.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "240ca92b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample transactions:\n",
      "Transaction 1: ['DIA_5723', 'DIA_78959', 'DIA_5715', 'DIA_07070', 'DIA_496', 'DIA_29680', 'DIA_30981', 'DIA_V1582', 'PRO_5491', 'PRE_Furosemide_40_mg']...\n",
      "Transaction 2: ['DIA_07071', 'DIA_78959', 'DIA_2875', 'DIA_2761', 'DIA_496', 'DIA_5715', 'DIA_V08', 'DIA_3051', 'PRO_5491', 'PRE_Furosemide_40_mg']...\n",
      "Transaction 3: ['DIA_07054', 'DIA_78959', 'DIA_V462', 'DIA_5715', 'DIA_2767', 'DIA_2761', 'DIA_496', 'DIA_V08', 'DIA_3051', 'DIA_78791']...\n",
      "Transaction 4: ['DIA_45829', 'DIA_07044', 'DIA_7994', 'DIA_2761', 'DIA_78959', 'DIA_2767', 'DIA_3051', 'DIA_V08', 'DIA_V4986', 'DIA_V462']...\n",
      "Transaction 5: ['DIA_30500', 'PRO_8938']...\n",
      "Transaction 6: ['DIA_G3183', 'DIA_F0280', 'DIA_R441', 'DIA_R296', 'DIA_E785', 'DIA_Z8546', 'PRE_Senna_8.6_mg', 'PRE_Sodium Chloride 0.9%  Flush_3-10_mL', 'PRE_Heparin_5000_UNIT', 'PRE_Polyethylene Glycol_17_g']...\n",
      "Transaction 7: ['DIA_R4182', 'DIA_G20', 'DIA_F0280', 'DIA_R609', 'DIA_E785', 'DIA_Z8546']...\n",
      "Transaction 8: ['DIA_5283', 'DIA_52109']...\n",
      "Transaction 9: ['DIA_R1310', 'DIA_R0989', 'DIA_K31819', 'DIA_K219', 'DIA_K449', 'DIA_F419', 'DIA_I341', 'DIA_M810', 'DIA_Z87891', 'PRE_Heparin_5000_UNIT']...\n",
      "Transaction 10: ['DIA_S72012A', 'DIA_W010XXA', 'DIA_Y92480', 'DIA_K219', 'DIA_E7800', 'DIA_I341', 'DIA_G43909', 'DIA_Z87891', 'DIA_Z87442', 'DIA_F419']...\n"
     ]
    }
   ],
   "source": [
    "# Generate transactions\n",
    "transactions = []\n",
    "for _, row in transactions_df.iterrows():\n",
    "    transaction = []\n",
    "    \n",
    "    # Helper function to safely add items\n",
    "    def add_items(items, prefix=''):\n",
    "        if isinstance(items, list):\n",
    "            transaction.extend([f\"{prefix}{item}\" for item in items if pd.notna(item) and str(item).strip()])\n",
    "    \n",
    "    # Diagnoses\n",
    "    if 'diagnoses' in row:\n",
    "        add_items(row['diagnoses'], 'DIA_')\n",
    "    \n",
    "    # Procedures\n",
    "    if 'procedures' in row:\n",
    "        add_items(row['procedures'], 'PRO_')\n",
    "    \n",
    "    # Lab results\n",
    "    if 'labs' in row:\n",
    "        add_items(row['labs'], 'LAB_')\n",
    "    \n",
    "    # Gender\n",
    "    if 'gender' in row and pd.notna(row['gender']):\n",
    "        transaction.append(f\"Gender_{row['gender']}\")\n",
    "    \n",
    "    # Race\n",
    "    if 'race' in row and pd.notna(row['race']):\n",
    "        transaction.append(f\"Race_{row['race'].replace(' ', '_')}\")\n",
    "    \n",
    "    # Symptoms\n",
    "    if 'Symptoms' in row:\n",
    "        add_items(row['Symptoms'])\n",
    "    \n",
    "    # Allergies\n",
    "    if 'allergies' in row:\n",
    "        add_items(row['allergies'])\n",
    "\n",
    "    # Prescriptions\n",
    "    if 'prescriptions' in row:\n",
    "        add_items(row['prescriptions'], 'PRE_')\n",
    "    \n",
    "    transactions.append(transaction)\n",
    "\n",
    "# Print sample transactions\n",
    "print(\"\\nSample transactions:\")\n",
    "for i, t in enumerate(transactions[:10], 1):\n",
    "    print(f\"Transaction {i}: {t[:10]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "c19a047e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   transaction_id  num_items  \\\n",
      "0               1         98   \n",
      "1               2         96   \n",
      "2               3         91   \n",
      "3               4         84   \n",
      "4               5         82   \n",
      "\n",
      "                                               items  \n",
      "0  DIA_T8131XA, DIA_R6521, DIA_J9601, DIA_N179, D...  \n",
      "1  DIA_8602, DIA_51881, DIA_42843, DIA_5070, DIA_...  \n",
      "2  DIA_2866, DIA_51881, DIA_5845, DIA_99594, DIA_...  \n",
      "3  DIA_J441, DIA_K7200, DIA_R579, DIA_J9602, DIA_...  \n",
      "4  DIA_0383, DIA_78552, DIA_5184, DIA_5845, DIA_5...  \n"
     ]
    }
   ],
   "source": [
    "# Sort transactions by number of items (descending or ascending as needed)\n",
    "sorted_transactions = sorted(transactions, key=len, reverse=True)  # Use reverse=False for ascending\n",
    "\n",
    "# Convert to DataFrame for CSV export\n",
    "df_sorted_transactions = pd.DataFrame({\n",
    "    'transaction_id': range(1, len(sorted_transactions) + 1),\n",
    "    'num_items': [len(t) for t in sorted_transactions],\n",
    "    'items': [', '.join(t) for t in sorted_transactions]\n",
    "})\n",
    "\n",
    "# Save to CSV\n",
    "# df_sorted_transactions.to_csv('sorted_transactions.csv', index=False)\n",
    "\n",
    "# Print a few to confirm\n",
    "print(df_sorted_transactions.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "6d1317a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Filter Rare Items First\n",
    "# from collections import Counter\n",
    "\n",
    "# # Count item frequencies across all transactions\n",
    "# item_counts = Counter(item for transaction in transactions for item in transaction)\n",
    "\n",
    "# # Keep only items that appear at least min_freq times\n",
    "# min_freq = 50  # Adjust based on your data size\n",
    "# frequent_items = {item for item, count in item_counts.items() if count >= min_freq}\n",
    "\n",
    "# # Filter transactions\n",
    "# filtered_transactions = [\n",
    "#     [item for item in txn if item in frequent_items] \n",
    "#     for txn in transactions\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "f1a17841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "544857 544857\n",
      "Transactions saved to transactions.csv\n"
     ]
    }
   ],
   "source": [
    "# Create a mask for rows with non-empty transactions\n",
    "mask = [len(t) > 0 for t in transactions]\n",
    "\n",
    "# Filter transactions and transactions_df together\n",
    "filtered_transactions = [t for t in transactions if len(t) > 0]\n",
    "filtered_df = transactions_df.loc[mask].reset_index(drop=True)\n",
    "\n",
    "# Now lengths should match\n",
    "print(len(filtered_transactions), filtered_df.shape[0])\n",
    "\n",
    "# Create the DataFrame\n",
    "transactions_df_out = pd.DataFrame({\n",
    "    'hadm_id': filtered_df['hadm_id'],\n",
    "    'transaction': filtered_transactions\n",
    "})\n",
    "\n",
    "# Save to CSV\n",
    "transactions_df_out.to_csv(\"transactions.csv\", index=False)\n",
    "print(\"Transactions saved to transactions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "ca4b0e61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-hot encoded shape: (544857, 8320)\n"
     ]
    }
   ],
   "source": [
    "# Extract the list of transactions from the DataFrame\n",
    "transactions_list = transactions_df_out['transaction'].tolist()\n",
    "\n",
    "# Initialize encoder and fit_transform the transactions\n",
    "encoder = TransactionEncoder()\n",
    "onehot = encoder.fit_transform(transactions_list)\n",
    "\n",
    "# Convert to DataFrame for easier analysis\n",
    "df_onehot = pd.DataFrame(onehot, columns=encoder.columns_)\n",
    "\n",
    "print(f\"One-hot encoded shape: {df_onehot.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "3791b79e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Most Frequent Items:\n",
      "DIA_4019      102362\n",
      "DIA_E785       84568\n",
      "DIA_I10        83773\n",
      "DIA_2724       67288\n",
      "DIA_Z87891     62803\n",
      "DIA_K219       56155\n",
      "DIA_53081      48624\n",
      "DIA_25000      43076\n",
      "DIA_F329       41876\n",
      "DIA_I2510      41548\n",
      "DIA_F419       38910\n",
      "DIA_42731      37063\n",
      "DIA_4280       36606\n",
      "DIA_311        36349\n",
      "DIA_41401      36077\n",
      "DIA_N179       35884\n",
      "DIA_Z20822     33113\n",
      "DIA_V1582      31704\n",
      "DIA_Z7901      30956\n",
      "DIA_5849       29135\n",
      "DIA_2449       28519\n",
      "DIA_E039       27999\n",
      "DIA_Z794       27640\n",
      "DIA_E119       26266\n",
      "DIA_3051       25878\n",
      "DIA_2859       24392\n",
      "DIA_F17210     24106\n",
      "DIA_G4733      23933\n",
      "DIA_40390      23831\n",
      "DIA_V5861      22634\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Get item frequencies from your one-hot encoded data\n",
    "item_frequencies = df_onehot.sum().sort_values(ascending=False)\n",
    "\n",
    "# Display top N frequent items\n",
    "print(\"Top Most Frequent Items:\")\n",
    "print(item_frequencies.head(30))\n",
    "\n",
    "# Convert to DataFrame and reset index to get items as a column\n",
    "freq_df = item_frequencies.reset_index()\n",
    "freq_df.columns = ['Item', 'Frequency']  # Name the columns\n",
    "\n",
    "# Save to CSV with both item names and frequencies\n",
    "freq_df.to_csv(\"frequent_items.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "6e2dcc6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing redundant columns\n",
    "# Define patterns to exclude\n",
    "# unwanted_patterns = [\n",
    "#     'no_known_allergies',\n",
    "#     'patient_recorded_as_having_no_known_allergies',\n",
    "#     'Allergy_*********per_pt_has_lots_of_allergies._daughter_will_bring_\\nlist***********_/_ampicillin_/_cortisone_/_nitrofurantoin_/',\n",
    "#     'Allergy____',\n",
    "#     '_____'  # For the \"____\" case\n",
    "# ]\n",
    "\n",
    "# # Filter transactions to remove these items\n",
    "# filtered_transactions = [\n",
    "#     [item for item in txn \n",
    "#      if not any(pattern in str(item).lower() for pattern in unwanted_patterns)]\n",
    "#     for txn in transactions\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "53a0a589",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample 1000 rows from the onehot dataframe\n",
    "#df_onehot_sample = df_onehot.sample(n=10000, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14925ef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(544857, 8320)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(df_onehot.shape)\n",
    "#frequent_itemsets = fpgrowth(df_onehot_sample, min_support=0.1, use_colnames=True)\n",
    "frequent_itemsets = fpgrowth(df_onehot, min_support=0.001, use_colnames=True)\n",
    "print(f\"Frequent itemsets shape: {frequent_itemsets.shape}\")\n",
    "print(\"Frequent Itemsets:\\n\", frequent_itemsets)\n",
    "frequent_itemsets.to_csv(\"frequent_itemsets.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b7dfd2",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[63], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#frequent_itemsets = fpgrowth(df_onehot_sample, min_support=0.1, use_colnames=True)\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m frequent_itemsets \u001b[38;5;241m=\u001b[39m \u001b[43mfpgrowth\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_onehot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_support\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0001\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_colnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFrequent itemsets shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfrequent_itemsets\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m rules \u001b[38;5;241m=\u001b[39m association_rules(frequent_itemsets, metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfidence\u001b[39m\u001b[38;5;124m\"\u001b[39m, min_threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)\n",
      "File \u001b[0;32m~/tt/.venv/lib/python3.10/site-packages/mlxtend/frequent_patterns/fpgrowth.py:95\u001b[0m, in \u001b[0;36mfpgrowth\u001b[0;34m(df, min_support, null_values, use_colnames, max_len, verbose)\u001b[0m\n\u001b[1;32m     92\u001b[0m minsup \u001b[38;5;241m=\u001b[39m math\u001b[38;5;241m.\u001b[39mceil(min_support \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(df\u001b[38;5;241m.\u001b[39mindex))  \u001b[38;5;66;03m# min support as count\u001b[39;00m\n\u001b[1;32m     93\u001b[0m generator \u001b[38;5;241m=\u001b[39m fpg_step(tree, minsup, colname_map, max_len, verbose)\n\u001b[0;32m---> 95\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfpc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_itemsets\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisabled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_support\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolname_map\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/tt/.venv/lib/python3.10/site-packages/mlxtend/frequent_patterns/fpcommon.py:93\u001b[0m, in \u001b[0;36mgenerate_itemsets\u001b[0;34m(generator, df, disabled, min_support, num_itemsets, colname_map)\u001b[0m\n\u001b[1;32m     91\u001b[0m item_dsbl \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(dec[i, :])\n\u001b[1;32m     92\u001b[0m \u001b[38;5;66;03m# select the i-th iset from original dataset\u001b[39;00m\n\u001b[0;32m---> 93\u001b[0m item_orig \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_dec\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;66;03m# check and keep count if there is a null value in iset of disabled\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mset\u001b[39m(item_dsbl):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.1)\n",
    "print(\"\\nAssociation Rules:\\n\", rules)\n",
    "rules.to_csv(\"association_rules.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d389a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
